{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title"},{"path":["abstract"],"id":"abstract","weight":1,"src":"abstract"}],"records":[{"i":0,"$":{"0":{"v":"Refactoring: Improving the Design of Existing Code","n":0.378},"1":{"v":"Almost every expert in Object-Oriented Development stresses the importance of iterative development. As you proceed with the iterative development, you need to add function to the existing code base. If you are really lucky that code base is structured just right to support the new function while still preserving its design integrity. Of course most of the time we are not lucky, the code does not quite fit what we want to do. You could just add the function on top of the code base. But soon this leads to applying patch upon patch making your system more complex than it needs to be. This complexity leads to bugs, and cripples your productivity.","n":0.094}}},{"i":1,"$":{"0":{"v":"Agile Estimating and Planning","n":0.5}}},{"i":2,"$":{"0":{"v":"Agile Project Management: Creating Innovative Products","n":0.408},"1":{"v":"Foreword by Israel Gat The Agile Software Development SeriesCockburn HighsmithSeries Editors Creating Innovative Products Software Development/Agile Best practices for managing projects in agile environmentsnow updated with new techniques for larger projects Today, the pace of project management moves faster. Project management needs to become more flexible and far more responsive to customers. Using Agile Project Management (APM), project managers can achieve all these goals without compromising value, quality, or business discipline. In Agile Project Management, Second Edition, renowned agile pioneer Jim Highsmith thoroughly updates his classic guide to APM, extending and refining it to support even the largest projects and organizations. Writing for project leaders, managers, and executives at all levels, Highsmith integrates the best project management, product management, and software development practices into an overall framework designed to support unprecedented speed and mobility. The many topics added in this new edition include incorporating agile values, scaling agile projects, release planning, portfolio governance, and enhancing organizational agility. Project and business leaders will especially appreciate Highsmiths new coverage of promoting agility through performance measurements based on value, quality, and constraints. This editions coverage includes: Understanding the agile revolutions impact on product development Recognizing when agile methods will work in project management, and when they wont Setting realistic business objectives for Agile Project Management Promoting agile values and principles across the organization Utilizing a proven Agile Enterprise Framework that encompasses governance, project and iteration management, and technical practices Optimizing all five stages of the agile project: Envision, Speculate, Explore, Adapt, and Close Organizational and product-related processes for scaling agile to the largest projects and teams Agile project governance solutions for executives and management The Agile Triangle: measuring performance in ways that encourage agility instead of discouraging it The changing role of the agile project leader Jim Highsmith is a founding member of the AgileAlliance, co-author of the Agile Manifesto, and director of the Agile Project Management Advisory Service for the Cutter Consortium. He consults with development organizations throughout the U.S., Europe, Canada, South Africa, Australia, Japan, India, and New Zealand on accelerating development in todays increasingly complex, uncertain environments. Highsmith is author of Adaptive Software Development, winner of the 2000 Jolt Award, and (with Alistair Cockburn) co-editor of The Agile Software Development Series. He has more than 25 years experience as an IT manager, product manager, project manager, consultant, and software developer.","n":0.051}}},{"i":3,"$":{"0":{"v":"Managing complex product development projects with design structure matrices and domain mapping matrices","n":0.277},"1":{"v":"Abstract   Complexity in product development (PD) projects can emanate from the product design, the development process, the development organization, the tools and technologies applied, the requirements to be met, and other domains. In each of these domains, complexity arises from the numerous elements and their multitude of relationships, such as between the components of the product being developed, between the activities to develop them, and among the people doing the activities. One approach to handing this complexity is to represent and analyze these domains’  design structures  or  architectures . The design structure matrix (DSM) has proved to be a very helpful tool for representing and analyzing the architecture of an individual system such as a product, process, or organization. Like many tools, the DSM has been applied in a variety of areas outside its original domain, as researchers and practitioners have sought to leverage its advantages. Along the way, however, its fundamental rules (such as being a square matrix) have been challenged. In this paper, we formalize an approach to using a domain mapping matrix (DMM) to compare two DSMs of different project domains. A DMM is a rectangular ( m  ×  n ) matrix relating two DSMs, where  m  is the size of DSM 1  and  n  is the size of DSM 2 . DMM analysis augments traditional DSM analyses. Our comparison of DSM and DMM approaches shows that DMM analysis offers several benefits. For example, it can help (1) capture the dynamics of PD, (2) show traceability of constraints across domains, (3) provide transparency between domains, (4) synchronize decisions across domains, (5) cross-verify domain models, (6) integrate a domain with the rest of a project or program, and (7) improve decision making among engineers and managers by providing a basis for communication and learning across domains.","n":0.058}}},{"i":4,"$":{"0":{"v":"Software aging","n":0.707}}},{"i":5,"$":{"0":{"v":"Discovering and representing systematic code changes","n":0.408},"1":{"v":"Software engineers often inspect program differences when reviewing others' code changes, when writing check-in comments, or when determining why a program behaves differently from expected behavior after modification. Program differencing tools that support these tasks are limited in their ability to group related code changes or to detect potential inconsistencies in those changes. To overcome these limitations and to complement existing approaches, we built Logical Structural Diff (LSdiff), a tool that infers systematic structural differences as logic rules. LSdiff notes anomalies from systematic changes as exceptions to the logic rules. We conducted a focus group study with professional software engineers in a large E-commerce company; we also compared LSdiff's results with textual differences and with structural differences without rules. Our evaluation suggests that LSdiff complements existing differencing tools by grouping code changes that form systematic change patterns regardless of their distribution throughout the code, and its ability to discover anomalies shows promise in detecting inconsistent changes.","n":0.08}}},{"i":6,"$":{"0":{"v":"Exploring the Duality between Product and Organizational Architectures: A Test of the Mirroring Hypothesis","n":0.267},"1":{"v":"A variety of academic studies argue that a relationship exists between the structure of an organization and the design of the products that this organization produces. Specifically, products tend to “mirror” the architectures of the organizations in which they are developed. This dynamic occurs because the organization's governance structures, problem solving routines and communication patterns constrain the space in which it searches for new solutions. Such a relationship is important, given that product architecture has been shown to be an important predictor of product performance, product variety, process flexibility and even the path of industry evolution.","n":0.102}}},{"i":7,"$":{"0":{"v":"The WyCash portfolio management system","n":0.447}}},{"i":8,"$":{"0":{"v":"Program evolution: processes of software change","n":0.408}}},{"i":9,"$":{"0":{"v":"An enterprise perspective on technical debt","n":0.408},"1":{"v":"Technical debt is a term that has been used to describe the increased cost of changing or maintaining a system due to expedient shortcuts taken during its development. Much of the research on technical debt has focused on decisions made by project architects and individual developers who choose to trade off short-term gain for a longer-term cost. However, in the context of enterprise software development, such a model may be too narrow. We explore the premise that technical debt within the enterprise should be viewed as a tool similar to financial leverage, allowing the organization to incur debt to pursue options that it couldn't otherwise afford. We test this premise by interviewing a set of experienced architects to understand how decisions to acquire technical debt are made within an enterprise, and to what extent the acquisition of technical debt provides leverage. We find that in many cases, the decision to acquire technical debt is not made by technical architects, but rather by non-technical stakeholders who cause the project to acquire new technical debt or discover existing technical debt that wasn't previously visible. We conclude with some preliminary observations and recommendations for organizations to better manage technical debt in the presence of some enterprise-scale circumstances.","n":0.07}}},{"i":10,"$":{"0":{"v":"Dynamic recovery of critical infrastructures: real-time temporal coordination","n":0.354},"1":{"v":"This paper takes a systems engineering approach to the problem of operations coordination among multiple infrastructures to minimise the impact of large disasters on human lives. Temporal coordination is essential to avoid bottlenecks in the simultaneous recovery of multiple infrastructures systems. A solution framework is presented in terms of multiple-delay difference equations which bring out the temporal interdependencies among infrastructures. The present work is part of an effort by the Government of Canada, through the Natural Sciences and Engineering Research Council (NSERC) and Public Safety and Emergency Preparedness Canada (PSEPC) to fund research to develop innovative ways to mitigate large disaster situations.","n":0.099}}},{"i":11,"$":{"0":{"v":"Building empirical support for automated code smell detection","n":0.354},"1":{"v":"Identifying refactoring opportunities in software systems is an important activity in today's agile development environments. The concept of code smells has been proposed to characterize different types of design shortcomings in code. Additionally, metric-based detection algorithms claim to identify the \"smelly\" components automatically. This paper presents results for an empirical study performed in a commercial environment. The study investigates the way professional software developers detect god class code smells, then compares these results to automatic classification. The results show that, even though the subjects perceive detecting god classes as an easy task, the agreement for the classification is low. Misplaced methods are a strong driver for letting subjects identify god classes as such. Earlier proposed metric-based detection approaches performed well compared to the human classification. These results lead to the conclusion that an automated metric-based pre-selection decreases the effort spent on manual code inspections. Automatic detection accompanied by a manual review increases the overall confidence in the results of metric-based classifiers.","n":0.079}}},{"i":12,"$":{"0":{"v":"Analytics for software development","n":0.5},"1":{"v":"Despite large volumes of data and many types of metrics, software projects continue to be difficult to predict and risky to conduct. In this paper we propose software analytics which holds out the promise of helping the managers of software projects turn their plentiful information resources, produced readily by current tools, into insights they can act on. We discuss how analytics works, why it's a good fit for software engineering, and the research problems that must be overcome in order to realize its promise.","n":0.109}}},{"i":13,"$":{"0":{"v":"Monitoring code quality and development activity by software maps","n":0.333},"1":{"v":"Software development projects are difficult to manage, in general, due to the friction between completing system features and, at the same time, obtaining a high degree of code quality to ensure maintainability of the system in the future. A major challenge of this optimization problem is that code quality is less visible to stakeholders in the development process, particularly, to the management. In this paper, we describe an approach for automated software analysis and monitoring of both quality-related code metrics and development activities by means of software maps. A software map represents an adaptive, hierarchical representation of software implementation artifacts such as source code files being organized in a modular hierarchy. The maps can express and combine information about software development, software quality, and system dynamics; they can systematically be specified, automatically generated, and organized by templates. The maps aim at supporting decision-making processes. For example, they facilitate to decide where in the code an increase of quality would be beneficial both for speeding up current development activities and for reducing risks of future maintenance problems. Due to their high degree of expressiveness and their instantaneous generation, the maps additionally serve as up-to-date information tools, bridging an essential information gap between management and development, improve awareness, and serve as early risk detection instrument. The software map concept and its tool implementation are evaluated by means of two case studies on large industrially developed software systems.","n":0.065}}},{"i":14,"$":{"0":{"v":"Contact and Friction of One- and Two-Dimensional Nanostructures","n":0.354},"1":{"v":"Because their thickness dimension is very small compared with other dimensions, the one-dimensional (1D) nanostructures (such as nanowire, nanotube, and nanobelt) and two-dimensional (2D) nanostructures (such as graphene) are highly prone to bend. Because of their large bending flexurality, the 1D and 2D nanostructures exhibit different contact behavior from those chunky ones. Without considering the flexurality effect, the analysis on the experimental data of 1D and 2D nanostructures can lead to different and even contradicting results/conclusions on their mechanical properties. One focus of this chapter is on what can go wrong in the indentation and three-point bending tests of 1D nanostructures if the flexurality effect is not accounted. At the same time, the 1D and 2D nanostructures also exhibit abnormal friction behavior. The assumptions of the classical contact are reviewed, and their possible deficiencies and difficulties of being used to analyze the contact and friction of 1D/2D nanostructures are also discussed.","n":0.081}}},{"i":15,"$":{"0":{"v":"Achieving Agility through Architecture Visibility","n":0.447},"1":{"v":"L.L.Bean is a large retail organization whose development processes must be agile in order to allow rapid enhancement and maintenance of its technology infrastructure. Over the past decade L.L.Bean's software code-base had become brittle and difficult to evolve. An effort was launched to identify and develop new approaches to software development that would enable ongoing agility to support the ever-increasing demands of a successful business. This paper recounts L.L.Bean's effort in restructuring its code-base and adoption of process improvements that support an architecture-based agile approach to development, governance, and maintenance. Unlike traditional refactoring, this effort was guided by an architectural blueprint that was created in a Dependency Structure Matrix where the refactoring was first prototyped before being applied to the actual code base.","n":0.09}}},{"i":16,"$":{"0":{"v":"Analysis and Management of Architectural Dependencies in Iterative Release Planning","n":0.316},"1":{"v":"Within any incremental development paradigm, there exists a tension between the desire to deliver value to the customer early and the desire to reduce cost by avoiding architectural refactoring in subsequent releases. What is lacking, however, is quantifiable guidance that highlights the potential benefits and risks of choosing one or the other of these alternatives or a blend of both strategies. In this paper, we assert that the ability to quantify architecture quality with measurable criteria provides engineering guidance for iterative release planning. We demonstrate the use of propagation cost as a proxy for architectural health with dependency analysis of design structure and domain mapping matrices as a quantifiable basis for iteration planning.","n":0.094}}},{"i":17,"$":{"0":{"v":"A conceptual model of disasters encompassing multiple stakeholder domains","n":0.333},"1":{"v":"Understanding the interdependencies of critical infrastructures (power, transport, communication, etc.) is essential in emergency preparedness and response in the face of disasters. Unfortunately, many factors (e.g., the unwillingness to disclose or share critical data) prohibited the complete development of such an understanding. As an alternative solution, this paper presents a conceptual model – an ontology – of disasters affecting critical infrastructures. We bring humans into the loop and distinguish between the physical and social interdependencies between infrastructures, where the social layer deals with communication and coordination among the representatives (either humans or intelligent agents) of the various critical infrastructures. We validated our conceptual model with the people responsible for disaster management from several different critical infrastructures and through a case study. We also derived tools from the model to provide decision support. We expect that this conceptual model can later be used by people as a common language to communicate, analyse and simulate their interdependencies without having to disclose all critical and confidential data.","n":0.078}}},{"i":18,"$":{"0":{"v":"From retrospect to prospect: Assessing modularity and stability from software architecture","n":0.302},"1":{"v":"Architecture-level decisions, directly influenced by environmental factors, are crucial to preserve modularity and stability throughout software development life-cycle. Tradeoffs of modularization alternatives, such as aspect-oriented vs. object-oriented decompositions, thus need to be assessed from architecture models instead of source code. In this paper, we present a suite of architecture-level metrics, taking external factors that drive software changes into consideration and measuring how well an architecture produces independently substitutable modules. We formalize these metrics using logical models to automate quantitative stability and modularity assessment. We evaluate the metrics using eight aspect-oriented and object-oriented releases of a software product-line architecture, driven by a series of heterogeneous changes. By contrasting with an implementation-level analysis, we observe that these metrics can effectively reveal which modularization alternative generates more stable, modular design from high-level models.","n":0.088}}},{"i":19,"$":{"0":{"v":"A canonical data model for simulator interoperation in a collaborative system for disaster response simulation","n":0.258},"1":{"v":"The Disaster Response Network Enabled Platform (DRNEP) is a system that integrates a set of independently developed infrastructure and disaster simulators. This paper describes some of the architectural choices that we made for DRNEP. The overall system uses a master-slave pattern, with one master simulator orchestrating all of the others, based on a central system clock. As the various simulators are developed by different organizations, they each have their own data models, with data elements not matching one for one, or with different representations, or not useful for collaboration. To integrate them in DRNEP, and to avoid developing n2 distinct translators, we devised a single common data model, akin to the mediator pattern, and we therefore need only one data translator per simulator. Developing this common data model poses many challenges: on one hand it must contain the right abstractions to communicate with a variety of existing and future simulators, in particular the topology of their underlying models, yet reduce the overall complexity of the system, and also minimize the likelihood of too many drastic changes when the system will evolve. We used principles from system theory to develop this common data model, and will be used with 2 simulators connected to UBC's Infrastructure Interdependency Simulator (I2Sim) serving as master.","n":0.069}}},{"i":20,"$":{"0":{"v":"Managing Software Debt: Building for Inevitable Change","n":0.378},"1":{"v":"Shipping imperfect software is like going into debt. When you incur debt, the illusion of doing things faster can lead to exponential growth in the cost of maintaining software. Software debt takes five major forms: technical, quality, configuration management, design, and platform experience. In todays rush to market, software debt is inevitable. And thats okayif youre careful about the debt you incur, and if you quickly pay it back. In Managing Software Debt, leading Agile expert Chris Sterling shows how understanding software debt can help you move products to market faster, with a realistic plan for refactoring them based on experience. Writing for all Agile software professionals, Sterling explains why youre going into software debt whether you know it or notand why the interest on that debt can bring projects to a standstill. Next, he thoroughly explains each form of software debt, showing how to plan for it intelligently and repay it successfully. Youll learn why accepting software debt is not the same as deliberate sloppiness, and youll learn how to use the software debt concept to systematically improve architectural agility. Coverage includes Managing tensions between speed and perfection and recognizing that youll inevitably ship some not quite right codePlanning to minimize interest payments by paying debts quicklyBuilding architectures that respond to change and help enterprises run more smoothlyIncorporating emergent architecture concepts into daily activities, using Agile collaboration and refactoring techniquesDelivering code and other software internals that reduce the friction of future changeUsing early, automated testing to move past the break/fix mentalityScripting and streamlining both deployment and rollbackImplementing team configuration patterns and knowledge sharing approaches that make software debt easier to repayClearing away technical impediments in existing architecturesUsing the YAGNI (you aint gonna need it) approach to strip away unnecessary complexity Using this books techniques, senior software leadership can deliver more business value; managers can organize and support development teams more effectively; and teams and team members can improve their performance throughout the development lifecycle.","n":0.055}}},{"i":21,"$":{"0":{"v":"Software evolution in agile development: a case study","n":0.354},"1":{"v":"The agile development method (ADM) is characterized by continuous feedback and change, and a software system developed using ADMevolves continuously through short iterations. Empirical studies on evolution of software following agile development method have been sparse. Most studies on software evolution have been performed on systems built using traditional (waterfall) development methods or using the open source development approach. This paper summarizes our study on the evolution of an enterprise software system following ADM. We evaluated key characteristics of evolution in the light of Lehman's laws of software evolution dealing with continuous change and growth, self-regulation and conservation, increasing complexity and declining quality. Our study indicates that most laws of evolution are followed by the system. We also present our observations on agile practices such as collective code ownership, test driven development and collaboration when the team is distributed.","n":0.085}}},{"i":22,"$":{"0":{"v":"Design Evolution of an Open Source Project Using an Improved Modularity Metric","n":0.289},"1":{"v":"Modularity of an open source software code base has been associated with community growth, incentives for voluntary contribution, and a reduction in free riding. As a theoretical construct, it links open source software to other domains of research, including organization theory, the economics of industry structure, and new product development; however, measuring the modularity of an open source software design has proven difficult, especially for large and complex systems. Building on previous work on Design Structure Matrices (DSMs), this paper describes two contributions towards a method for examining the evolving modularity of large-scale software systems: (1) an algorithm and new modularity metric for comparing code bases of different size; and (2) evolution analysis of Apache Tomcat to illustrate the insights gained from this approach. Over a ten-year period, the modularity of Tomcat continually increased, except in three instances: with each major change to the architecture or implementation, modularity first declined, then increased in the subsequent version to fully compensate for the decline.","n":0.079}}},{"i":23,"$":{"0":{"v":"Paying down design debt with strategic refactoring","n":0.378},"1":{"v":"Our studies indicate that strategic refactoring using design patterns is the most effective way to repair decaying code for object-oriented (OO) systems. However, applying a pattern-based approach to legacy system repair or even post-design pattern injection is often difficult and, in some cases if misapplied, detrimental","n":0.147}}},{"i":24,"$":{"0":{"v":"The Anatomy of a Large-Scale Hypertextual Web Search Engine.","n":0.333}}},{"i":25,"$":{"0":{"v":"Content analysis : an introduction to its methodology","n":0.354},"1":{"v":"Krippendorff views content analysis (one of the most important techniques in communication research) in historical perspective, in contrast to other techniques and in terms of what it can and cannot do.","n":0.18}}},{"i":26,"$":{"0":{"v":"A systematic mapping study on technical debt and its management","n":0.316},"1":{"v":"A systematic mapping study on technical debt and its management.Ninety-four papers are finally selected for data extraction and analysis.Technical debt was classified into 10 types.Eight activities of and 29 tools for technical debt management were identified.Code-related technical debt has gained the most attention. ContextTechnical debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefit but may hurt the long-term health of a software system. ObjectiveThis work aims at collecting studies on TD and TD management (TDM), and making a classification and thematic analysis on these studies, to obtain a comprehensive understanding on the TD concept and an overview on the current state of research on TDM. MethodA systematic mapping study was performed to identify and analyze research on TD and its management, covering publications between 1992 and 2013. ResultsNinety-four studies were finally selected. TD was classified into 10 types, 8 TDM activities were identified, and 29 tools for TDM were collected. ConclusionsThe term \"debt\" has been used in different ways by different people, which leads to ambiguous interpretation of the term. Code-related TD and its management have gained the most attention. There is a need for more empirical studies with high-quality evidence on the whole TDM process and on the application of specific TDM approaches in industrial settings. Moreover, dedicated TDM tools are needed for managing various types of TD in the whole TDM process.","n":0.066}}},{"i":27,"$":{"0":{"v":"The Qualitas Corpus: A Curated Collection of Java Code for Empirical Studies","n":0.289},"1":{"v":"In order to increase our ability to use measurement to support software development practise we need to do more analysis of code. However, empirical studies of code are expensive and their results are difficult to compare. We describe the Qualitas Corpus, a large curated collection of open source Java systems. The corpus reduces the cost of performing large empirical studies of code and supports comparison of measurements of the same artifacts. We discuss its design, organisation, and issues associated with its development.","n":0.11}}},{"i":28,"$":{"0":{"v":"An exploration of technical debt","n":0.447},"1":{"v":"Context: Whilst technical debt is considered to be detrimental to the long term success of software development, it appears to be poorly understood in academic literature. The absence of a clear definition and model for technical debt exacerbates the challenge of its identification and adequate management, thus preventing the realisation of technical debt's utility as a conceptual and technical communication device. Objective: To make a critical examination of technical debt and consolidate understanding of the nature of technical debt and its implications for software development. Method: An exploratory case study technique that involves multivocal literature review, supplemented by interviews with software practitioners and academics to establish the boundaries of the technical debt phenomenon. Result: A key outcome of this research is the creation of a theoretical framework that provides a holistic view of technical debt comprising a set of technical debts dimensions, attributes, precedents and outcomes, as well as the phenomenon itself and a taxonomy that describes and encompasses different forms of the technical debt phenomenon. Conclusion: The proposed framework provides a useful approach to understanding the overall phenomenon of technical debt for practical purposes. Future research should incorporate empirical studies to validate heuristics and techniques that will assist practitioners in their management of technical debt.","n":0.07}}},{"i":29,"$":{"0":{"v":"Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit.","n":0.289},"1":{"v":"A previously described coefficient of agreement for nominal scales, kappa, treats all disagreements equally. A generalization to weighted kappa (Kw) is presented. The Kw provides for the incorpation of ratio-scaled degrees of disagreement (or agreement) to each of the cells of the k * k table of joi","n":0.144}}},{"i":30,"$":{"0":{"v":"Measure it? Manage it? Ignore it? software practitioners and technical debt","n":0.302},"1":{"v":"The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches.","n":0.078}}},{"i":31,"$":{"0":{"v":"Hotspot Patterns: The Formal Definition and Automatic Detection of Architecture Smells","n":0.302},"1":{"v":"In this paper, we propose and empirically validate a suite of hotspot patterns: recurring architecture problems that occur in most complex systems and incur high maintenance costs. In particular, we introduce two novel hotspot patterns, Unstable Interface and Implicit Cross-module Dependency. These patterns are defined based on Baldwin and Clark's design rule theory, and detected by the combination of history and architecture information. Through our tool-supported evaluations, we show that these patterns not only identify the most error-prone and change-prone files, they also pinpoint specific architecture problems that may be the root causes of bug-proneness and change-proneness. Significantly, we show that 1) these structure-history integrated patterns contribute more to error- and change-proneness than other hotspot patterns, and 2) the more hotspot patterns a file is involved in, the more error- and change-prone it is. Finally, we report on an industrial case study to demonstrate the practicality of these hotspot patterns. The architect and developers confirmed that our hotspot detector discovered the majority of the architecture problems causing maintenance pain, and they have started to improve the system's maintainability by refactoring and fixing the identified architecture issues.","n":0.073}}},{"i":32,"$":{"0":{"v":"Assessing technical debt by identifying design flaws in software systems","n":0.316},"1":{"v":"Tough time-to-market constraints and unanticipated integration or evolution issues lead to design tradeoffs that usually cause flaws in the structure of a software system. Thus, maintenance costs grow significantly. The impact of these design decisions, which provide short-term benefits at the expense of the system's design integrity, is usually referred to as technical debt. In this paper, I propose a novel framework for assessing technical debt using a technique for detecting design flaws, i.e., specific violations of well-established design principles and rules. To make the framework comprehensive and balanced, it is built on top of a set of metrics-based detection rules for well-known design flaws that cover all of the major aspects of design such as coupling, complexity, and encapsulation. I demonstrate the effectiveness of the framework by assessing the evolution of technical debt symptoms over a total of 63 releases of two popular Eclipse® projects. The case study shows how the framework can detect debt symptoms and past refactoring actions. The experiment also reveals that in the absence of such a framework, restructuring actions are not always coherent and systematic, not even when performed by very experienced developers.","n":0.073}}},{"i":33,"$":{"0":{"v":"Identifying Architectural Bad Smells","n":0.5},"1":{"v":"Certain design fragments in software architectures can have a negative impact on system maintainability. In this paper, we introduce the concept of architectural \"bad smells,\" which are frequently recurring software designs that can have non-obvious and significant detrimental effects on system lifecycle properties. We define architectural smells and differentiate them from related concepts, such as architectural antipatterns and code smells. We also describe four representative architectural smells we encountered in the context of reverse-engineering eighteen grid technologies and refactoring one large industrial system.","n":0.11}}},{"i":34,"$":{"0":{"v":"The SQALE method for evaluating technical debt","n":0.378},"1":{"v":"This paper presents the SQALE (Software Quality Assessment Based on Lifecycle Expectations) method. We describe its Quality Model and Analysis Model which is used to estimate the Quality and the Technical Debt of an application source code. We provide recommendations and guidelines for using the SQALE indicators in order to analyse the structure and the impact of the Technical Debt.","n":0.129}}},{"i":35,"$":{"0":{"v":"Using metrics to evaluate software system maintainability","n":0.378},"1":{"v":"Software metrics have been much criticized in the last few years, sometimes justly but more often unjustly, because critics misunderstand the intent behind the technology. Software complexity metrics, for example, rarely measure the \"inherent complexity\" embedded in software systems, but they do a very good job of comparing the relative complexity of one portion of a system with another. In essence, they are good modeling tools. Whether they are also good measuring tools depends on how consistently and appropriately they are applied. >","n":0.11}}},{"i":36,"$":{"0":{"v":"Arcan: A Tool for Architectural Smells Detection","n":0.378},"1":{"v":"Code smells are sub-optimal coding circumstances such as blob classes or spaghetti code - they have received much attention and tooling in recent software engineering research. Higher-up in the abstraction level, architectural smells are problems or sub-optimal architectural patterns or other design-level characteristics. These have received significantly less attention even though they are usually considered more critical than code smells, and harder to detect, remove, and refactor. This paper describes an open-source tool called Arcan developed for the detection of architectural smells through an evaluation of several different architecture dependency issues. The detection techniques inside Arcan exploit graph database technology, allowing for high scalability in smells detection and better management of large amounts of dependencies of multiple kinds. In the scope of this paper, we focus on the evaluation of Arcan results carried out with real-life software developers to check if the architectural smells detected by Arcan are really perceived as problems and to get an overall usefulness evaluation of the tool.","n":0.079}}},{"i":37,"$":{"0":{"v":"Automatic Detection of Instability Architectural Smells","n":0.408},"1":{"v":"Code smells represent well known symptoms of problems at code level, and architectural smells can be seen as their counterpart at architecture level. If identified in a system, they are usually considered more critical than code smells, for their effect on maintainability issues. In this paper, we introduce a tool for the detection of architectural smells that could have an impact on the stability of a system. The detection techniques are based on the analysis of dependency graphs extracted from compiled Java projects and stored in a graph database. The results combine the information gathered from dependency and instability metrics to identify flaws hidden in the software architecture. We also propose some filters trying to avoid possible false positives.","n":0.092}}},{"i":38,"$":{"0":{"v":"Qualitas.class corpus: a compiled version of the qualitas corpus","n":0.333},"1":{"v":"This paper documents a compiled version of the Qualitas Corpus named Qualitas.class Corpus. We provide compiled Java projects for the 111 systems included in the corpus. We also gathered a large amount of metrics data (such as measurements from complexity, coupling, and CK metrics) about the systems. By making Qualitas.class Corpus public, our goal is to assist researchers by removing the compilation effort when conducting empirical studies.","n":0.122}}},{"i":39,"$":{"0":{"v":"Architectural debt management in value-oriented architecting","n":0.408},"1":{"v":"Architectural technical debt (ATD) may be incurred when making architecture decisions. In most cases, ATD is not effectively managed in the architecting process: It is not made explicit, and architecture decision making does not consider the ATD incurred by the different design options. This chapter proposes a conceptual model of ATD and an architectural technical debt management process applying this ATD conceptual model in order to facilitate decision making in a value-oriented perspective of architecting. We also demonstrate how ATD management can be employed in architectural synthesis and evaluation in a case study. The contribution of this work provides a controllable and predictable balance between the value and cost of architecture design in the long term.","n":0.093}}},{"i":40,"$":{"0":{"v":"Technical Debt Indexes Provided by Tools: A Preliminary Discussion","n":0.333},"1":{"v":"In software maintenance and evolution, it is important to assess both code and architecture in order to identify issues to be solved to improve software quality. Different tools provide some kind of index giving us an overall evaluation of a project to be used when managing its technical debt. In this paper, we outline how the indexes, that we call in general Technical Debt Indexes, provided by five different tools are computed. We describe their principal features and differences, what aspects they are missing, and we outline if (and how) the indexes take into account architectural problems that could have a major impact on the architectural debt. We show that the indexes rely on different information sources and measure different quantities.","n":0.091}}},{"i":41,"$":{"0":{"v":"Making Smart Moves to Untangle Programs","n":0.408},"1":{"v":"We present a novel algorithm to improve the design of programs by removing circular dependencies between packages without completely collapsing the package structure. This is achieved by moving classes between packages. The algorithm is based on a scoring function that is used to select the classes to be moved. The algorithm is validated against several open source case studies. The results show that our algorithm improves the program structure and removes inter-package cycles.","n":0.117}}},{"i":42,"$":{"0":{"v":"A study on the role of software architecture in the evolution and quality of software","n":0.258},"1":{"v":"Conventional wisdom suggests that a software system's architecture has a significant impact on its evolution. Prior research has studied the evolution of software using the information of how its files have changed together in their revision history. No prior study, however, has investigated the impact of architecture on the evolution of software from its change history. This is mainly because most open-source software systems do not document their architectures. We have overcome this challenge using several architecture recovery techniques. We used the recovered models to examine if co-changes spanning multiple architecture modules are more likely to introduce bugs than co-changes that are within modules. The results show that the co-changes that cross architectural module boundaries are more correlated with defects than co-changes within modules, implying that, to improve accuracy, bug predictors should also take the software architecture of the system into consideration.","n":0.084}}},{"i":43,"$":{"0":{"v":"Circular dependencies and change-proneness: An empirical study","n":0.378},"1":{"v":"Advice that circular dependencies between programming artefacts should be avoided goes back to the earliest work on software design, and is well-established and rarely questioned. However, empirical studies have shown that real-world (Java) programs are riddled with circular dependencies between artefacts on different levels of abstraction and aggregation. It has been suggested that additional heuristics could be used to distinguish between bad and harmless cycles, for instances by relating them to the hierarchical structure of the packages within a program, or to violations of additional design principles.","n":0.107}}},{"i":44,"$":{"0":{"v":"Stop the software architecture erosion: building better software systems","n":0.333},"1":{"v":"In lots of software projects unfortunately an architectural erosion happens over time. Modules which were independent, become connected, plug-ins finally depend on each other, and in general the architecture gets violated more and more. In this paper we will discuss how to avoid such architecture- and design-erosion and how an already eroded system can be fixed again. We will look at three different level of static analysis and examine architectural analysis in detail. Also typical use cases for architectural analysis are examined, followed by a collection of requirements for powerful tool support. The eclipse platform serves as case study and we look if, and how far architectural erosion happened there and if it can be fixed. Finally we discuss pros and cons of architectural analysis and conclude with an out view.","n":0.087}}},{"i":45,"$":{"0":{"v":"Mining version histories to guide software changes","n":0.378},"1":{"v":"We apply data mining to version histories in order to guide programmers along related changes: \"Programmers who changed these functions also changed. . . \". Given a set of existing changes, such rules (a) suggest and predict likely further changes, (b) show up item coupling that is indetectable by program analysis, and (c) prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict 26% of further files to be changed - and 15% of the precise functions or variables. The topmost three suggestions contain a correct location with a likelihood of 64%.","n":0.101}}},{"i":46,"$":{"0":{"v":"Managing technical debt in software-reliant systems","n":0.408},"1":{"v":"Delivering increasingly complex software-reliant systems demands better ways to manage the long-term effects of short-term expedients. The technical debt metaphor is gaining significant traction in the agile development community as a way to understand and communicate such issues. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a \"debt\": on which \"interest\" has to be paid and which the \"principal\" should be repaid at some point for the long-term health of the project. We argue that the software engineering research community has an opportunity to study and improve this concept. We can offer software engineers a foundation for managing such trade-offs based on models of their economic impacts. Therefore, we propose managing technical debt as a part of the future research agenda for the software engineering field.","n":0.081}}},{"i":47,"$":{"0":{"v":"Design Rules: The Power of Modularity Volume 1","n":0.354},"1":{"v":"From the Publisher:\r\nWe live in a dynamic economic and commerical world, surrounded by objects of remarkable complexity and power. In many industries, changes in products and technologies have brought with them new kinds of firms and forms of organization. We are discovering news ways of structuring work, of bringing buyers and sellers together, and of creating and using market information. Although our fast-moving economy often seems to be outside of our influence or control, human beings create the things that create the market forces. Devices, software programs, production processes, contracts, firms, and markets are all the fruit of purposeful action: they are designed.\r\nUsing the computer industry as an example, Carliss Y. Baldwin and Kim B. Clark develop a powerful theory of design and industrial evolution. They argue that the industry has experienced previously unimaginable levels of innovation and growth because it embraced the concept of modularity, building complex products from smaller subsystems that can be designed independently yet function together as a whole. Modularity freed designers to experiment with different approaches, as long as they obeyed the established design rules. Drawing upon the literatures of industrial organization, real options, and computer architecture, the authors provide insight into the forces of change that drive today's economy.","n":0.07}}},{"i":48,"$":{"0":{"v":"Design rule spaces: a new form of architecture insight","n":0.333},"1":{"v":"In this paper, we investigate software architecture as a set of overlapping design rule spaces, formed by one or more structural or evolutionary relationships and clustered using our design rule hierarchy algorithm. Considering evolutionary coupling as a special type of relationship, we investigated (1) whether design rule spaces can reveal structural relations among error-prone files; (2) whether design rule spaces can reveal structural problems contributing to error-proneness.We studied three large-scale open source projects and found that error-prone files can be captured by just a few design rule sub-spaces. Supported by our tool, Titan, we are able to flexibly visualize design rule spaces formed by different types of relationships, including evolutionary dependencies. This way, we are not only able to visualize which error-prone files belong to which design rule spaces, but also to visualize the structural problems that give insight into why these files are error prone. Design rule spaces provide valuable direction on which parts of the architecture are problematic, and on why, when, and how to refactor.","n":0.077}}},{"i":49,"$":{"0":{"v":"Playing Detective: Reconstructing Software Architecture from Available Evidence","n":0.354},"1":{"v":"Because a system‘s software architecture strongly influences its quality attributes such as modifiability, performance, and security, it is important to analyze and reason about that architecture. However, architectural documentation frequently does not exist, and when it does, it is often “out of sync” with the implemented system. In addition, it is rare that software development begins with a clean slates systems are almost always constrained by existing legacy code. As a consequence, we need to be able to extract information from existing system implementations and utilize this information for architectural reasoning. This paper presents Dali, an open, lightweight workbench that aids an analyst in extracting, manipulating, and interpreting architectural information. By assisting in the reconstruction of architectures from extracted information, Dali helps an analyst redocument architectures, discover the relationship between “as-implemented” and “as-designed” architectures, analyze architectural quality attributes and plan for architectural change.","n":0.084}}},{"i":50,"$":{"0":{"v":"Detecting and quantifying different types of self-admitted technical Debt","n":0.333},"1":{"v":"Technical Debt is a term that has been used to express non-optimal solutions during the development of software projects. These non optimal solutions are often shortcuts that allow the project to move faster in the short term, at the cost of increased maintenance in the future. To help alleviate the impact of technical debt, a number of studies focused on the detection of technical debt. More recently, our work shown that one possible source to detect technical debt is using source code comments, also referred to as self-admitted technical debt. However, what types of technical debt can be detected using source code comments remains as an open question. Therefore, in this paper we examine code comments to determine the different types of technical debt. First, we propose four simple filtering heuristics to eliminate comments that are not likely to contain technical debt. Second, we read through more than 33K comments, and we find that self-admitted technical debt can be classified into five main types - design debt, defect debt, documentation debt, requirement debt and test debt. The most common type of self-admitted technical debt is design debt, making up between 42% to 84% of the classified comments. Lastly, we make the classified dataset of more than 33K comments publicly available for the community as a way to encourage future research and the evolution of the technical debt landscape.","n":0.066}}},{"i":51,"$":{"0":{"v":"Integrated impact analysis for managing software changes","n":0.378},"1":{"v":"The paper presents an adaptive approach to perform impact analysis from a given change request to source code. Given a textual change request (e.g., a bug report), a single snapshot (release) of source code, indexed using Latent Semantic Indexing, is used to estimate the impact set. Should additional contextual information be available, the approach configures the best-fit combination to produce an improved impact set. Contextual information includes the execution trace and an initial source code entity verified for change. Combinations of information retrieval, dynamic analysis, and data mining of past source code commits are considered. The research hypothesis is that these combinations help counter the precision or recall deficit of individual techniques and improve the overall accuracy. The tandem operation of the three techniques sets it apart from other related solutions. Automation along with the effective utilization of two key sources of developer knowledge, which are often overlooked in impact analysis at the change request level, is achieved.     To validate our approach, we conducted an empirical evaluation on four open source software systems. A benchmark consisting of a number of maintenance issues, such as feature requests and bug fixes, and their associated source code changes was established by manual examination of these systems and their change history. Our results indicate that there are combinations formed from the augmented developer contextual information that show statistically significant improvement over stand-alone approaches.","n":0.066}}},{"i":52,"$":{"0":{"v":"Towards an Ontology of Terms on Technical Debt","n":0.354},"1":{"v":"Technical debt is a term that has been used to describe the increased cost of changing or maintaining a system due to shortcuts taken during its development. As technical debt is a recent research area, its different types and their indicators are not organized yet. Therefore, this paper proposes an ontology of terms on technical debt in order to organize a common vocabulary for the area. The organized concepts derived from the results of a systematic literature mapping. The proposed ontology was evaluated in two steps. In the first one, some ontology design quality criteria were used. For the second one, a specialist in the area performed an initial evaluation. This work contributes to evolve the Technical Debt Landscape through the organization of the different types of technical debt and their indicators. We consider this an important contribution for both researchers and practitioners because this information was spread out in the literature hindering their use in research and development activities.","n":0.079}}},{"i":53,"$":{"0":{"v":"The power of modularity","n":0.5},"1":{"v":"We live in a dynamic economic and commerical world, surrounded by objects of remarkable complexity and power. In many industries, changes in products and technologies have brought with them new kinds of firms and forms of organization. We are discovering news ways of structuring work, of bringing buyers and sellers together, and of creating and using market information. Although our fast-moving economy often seems to be outside of our influence or control, human beings create the things that create the market forces. Devices, software programs, production processes, contracts, firms, and markets are all the fruit of purposeful action: they are designed.Using the computer industry as an example, Carliss Y. Baldwin and Kim B. Clark develop a powerful theory of design and industrial evolution. They argue that the industry has experienced previously unimaginable levels of innovation and growth because it embraced the concept of modularity, building complex products from smaller subsystems that can be designed independently yet function together as a whole. Modularity freed designers to experiment with different approaches, as long as they obeyed the established design rules. Drawing upon the literatures of industrial organization, real options, and computer architecture, the authors provide insight into the forces of change that drive today's economy.","n":0.07}}},{"i":54,"$":{"0":{"v":"Manufacturing execution systems","n":0.577},"1":{"v":"We describe manufacturing execution systems (MESs) as a vision for software development.We identify gaps between the MES vision and current software development practices.To narrow the gaps we prototype a Modularity Debt Management Decision Support System.We provide four case studies, realizing MES vision and illustrating its benefits. Software development suffers from a lack of predictability with respect to cost, time, and quality. Predictability is one of the major concerns addressed by modern manufacturing execution systems (MESs). A MES does not actually execute the manufacturing (e.g., controlling equipment and producing goods), but rather collects, analyzes, integrates, and presents the data generated in industrial production so that employees have better insights into processes and can react quickly, leading to predictable manufacturing processes. In this paper, we introduce the principles and functional areas of a MES. We then analyze the gaps between MES-vision-driven software development and current practices. These gaps include: (1) lack of a unified data collection infrastructure, (2) lack of integrated people data, (3) lack of common conceptual frameworks driving improvement loops from development data, and (4) lack of support for projection and simulation. Finally, we illustrate the feasibility of leveraging MES principles to manage software development, using a Modularity Debt Management Decision Support System prototype we developed. In this prototype we demonstrate that information integration in MES-vision-driven systems enables new types of analyses, not previously available, for software development decision support. We conclude with suggestions for moving current software development practices closer to the MES vision.","n":0.064}}},{"i":55,"$":{"0":{"v":"Blending Conceptual and Evolutionary Couplings to Support Change Impact Analysis in Source Code","n":0.277},"1":{"v":"The paper presents an approach that combines conceptual and evolutionary techniques to support change impact analysis in source code. Information Retrieval (IR) is used to derive conceptual couplings from the source code in a single version (release) of a software system. Evolutionary couplings are mined from source code commits. The premise is that such combined methods provide improvements to the accuracy of impact sets. A rigorous empirical assessment on the changes of the open source systems Apache httpd, ArgoUML, iBatis, and KOffice is also reported. The results show that a combination of these two techniques, across several cut points, provides statistically significant improvements in accuracy over either of the two techniques used independently. Improvements in recall values of up to 20% over the conceptual technique in KOffice and up to 45% over the evolutionary technique in iBatis were reported.","n":0.085}}},{"i":56,"$":{"0":{"v":"The Danger of Architectural Technical Debt: Contagious Debt and Vicious Circles","n":0.302},"1":{"v":"A known problem in large software companies is to balance the prioritization of short-term with long-term viability. Specifically, architecture violations (Architecture Technical Debt) taken to deliver fast might hinder future feature development. However, some technical debt requires more interest to be paid than other. We have investigated which Technical Debt items generate more effort and how this effort is manifested during software development. We conducted a multiple-case embedded case study comprehending 7 sites at 5 large international software companies. We found that some Technical Debt items are contagious, causing other parts of the system to be contaminated with the same problem, which may lead to non-linear growth of interest. We also identify another socio-technical phenomenon, for which a combination of weak awareness of debt, time pressure and refactoring creates Vicious Circles of events during the development. Such phenomena need to be identified and stopped before the development is led to a crisis point. Finally, this paper presents a taxonomy of the most dangerous items identified during the qualitative investigation and a model of their effects that can be used for prioritization, for further investigation and as a quality model for extracting more precise and context-specific metrics.","n":0.071}}},{"i":57,"$":{"0":{"v":"Technical debt at the crossroads of research and practice: report on the fifth international workshop on managing technical debt","n":0.229},"1":{"v":"Increasingly, software developers and managers use the metaphor of technical debt to communicate key trade-offs related to release and quality issues. We report here on the Fifth International Workshop on Managing Technical Debt, collocated with the Seventh International Symposium on Empirical Software Engineering and Measurement (ESEM 2013). The workshop participants reiterated the usefulness of the metaphor, shared emerging practices used in software development organizations, and emphasized the need for more research and better means for sharing emerging practices and results.","n":0.112}}},{"i":58,"$":{"0":{"v":"Technical Debt: Showing the Way for Better Transfer of Empirical Results","n":0.302},"1":{"v":"In this chapter, we discuss recent progress and opportunities in empirical software engineering by focusing on a particular technology, Technical Debt (TD), which ties together many recent developments in the field. Recent advances in TD research are providing empiricists the chance to make more sophisticated recommendations that have observable impact on practice.","n":0.139}}},{"i":59,"$":{"0":{"v":"Mapping architectural decay instances to dependency models","n":0.378},"1":{"v":"The architectures of software systems tend to drift or erode as they are maintained and evolved. These systems often develop architectural decay instances, which are instances of design decisions that negatively impact a system's lifecycle properties and are the analog to code-level decay instances that are potential targets for refactoring. While code-level decay instances are based on source-level constructs, architectural decay instances are based on higher levels of abstractions, such as components and connectors, and related concepts, such as concerns. Unlike code-level decay instances, architectural decay usually has more significant consequences. Not being able to detect or address architectural decay in time incurs architecture debt that may result in a higher penalty in terms of quality and maintainability (interest) over time. To facilitate architecture debt detection, in this paper, we demonstrate the possibility of transforming architectural models and concerns into an extended augmented constraint network (EACN), which can uniformly model the constraints among design decisions and environmental conditions. From an ACN, a pairwise-dependency relation (PWDR) can be derived, which, in turn, can be used to automatically and uniformly detect architectural decay instances.","n":0.074}}},{"i":60,"$":{"0":{"v":"Visualising architectural dependencies","n":0.577},"1":{"v":"Visibility of technical debt is critical. A lack thereof can lead to significant problems without adequate visibility as part of the system level decision-making processes [2]. Current approaches for analysing and monitoring architecture related debt are based on dependency analysis to detect code level violations of the software architecture [2,3,6]. However, heterogeneous environments with several systems constructed using COTS, and/or several programming languages may not offer sufficient code visibility. Other limiting factors include legal contracts, Intellectual Property Rights, and just very large systems. Secondly, the complexity of a software dependency is often greater than simple structural dependencies, including; multi-dimensional properties (as argued by [10]); behavioural dependencies [5,9]; and 'implicit' dependencies (i.e., dependency inter-relatedness [11]). This paper proposes a simple modelling approach for visualising dependency relationships as an extension of the current approaches, while supporting complex dependencies. The model can be built using existing dependency analysis and general architectural knowledge; thus is better suited for heterogeneous environments. We demonstrate the proposed modelling using an exemplar, and two field case studies.","n":0.077}}},{"i":61,"$":{"0":{"v":"Technical Debt: The Ultimate Antipattern - The Biggest Costs May Be hidden, Widespread, and Long Term","n":0.25},"1":{"v":"Software projects run the gamut from simple to complex, difficult to impossible and everything in between. Software project managers and their development teams must cope with and adapt to unforeseeable changes in nearly every aspect of the project as originally envisioned, scheduled and planned. In spite of all this turmoil and chaos systems get built, they work and at a later time are seen as having been created via a variety of imprudent development practices now collectively referred to as technical debt. This paper examines the hidden cost of expediency by probing what taking shortcuts does to productivity, morale and turnover on the project identifying debt that goes much deeper than technical.","n":0.094}}},{"i":62,"$":{"0":{"v":"Design pattern decay: the case for class grime","n":0.354},"1":{"v":"Context: We investigate class grime, a form of design pattern decay, wherein classes of the pattern realization have extraneous attributes or methods, which obfuscate the intended design of a pattern. Goal: To expand the taxonomy of class grime using properties of class cohesion. Using this expanded taxonomy we explore the effect that forms of class grime have on pattern realization understandability. Method: A pilot study utilizing a formal experiment to explore the effects of class grime on design pattern understandability. The experiments used simulated injection of 8 types of class grime into design pattern realizations randomly selected from 16 design pattern types from a set of 6541 realizations from 520 distinct software systems. Results: We found that for each of the 8 identified class grime forms, understandability was negatively affected. Conclusion: This work serves as early communication of research for the validation of the extended taxonomy as well as the method of grime injection used in the experiment.","n":0.08}}},{"i":63,"$":{"0":{"v":"Evaluating the Impact of Software Evolution on Software Clustering","n":0.333},"1":{"v":"The evolution of a software project is a rich data source for analyzing and improving the software development process. Recently, several research groups have tried to cluster source code artifacts based on information about how the code of a software system evolves. The results of these evolutionary approaches seem promising, but a direct comparison to traditional software clustering approaches based on structural code dependencies is still missing. To fill this gap, we conducted several clustering experiments with an approved software clustering tool comparing and combining the evolutionary and the structural approach. These experiments show that the evolutionary approach could produce meaningful clustering results. But still the traditional approach provides better results because of a more reliable data density of the structural data. Finally, the combination of both approaches is able to improve the overall clustering quality.","n":0.086}}},{"i":64,"$":{"0":{"v":"Using concept analysis to detect co-change patterns","n":0.378},"1":{"v":"Software systems need to change over time to cope with new requirements, and due to design decisions, the changes happen to crosscut the system's structure. Understanding how changes appear in the system can reveal hidden dependencies between different entities of the system. We propose the usage of concept analysis to identify groups of entities that change in the same way and in the same time. We apply our approach at different levels of abstraction (i.e., method, class, package) and we detect fine grained changes (i.e., statements were added in a class, but no method was added there). Concept analysis is a technique that identifies entities that have the same properties, but it requires manual inspection due to the large number of candidates it detects. We propose a heuristic that dramatically eliminate the false positives. We apply our approach on two case studies and we show how we can identify hidden dependencies and detect bad smells.","n":0.08}}},{"i":65,"$":{"0":{"v":"Studying the effect of co-change dispersion on software quality","n":0.333},"1":{"v":"Software change history plays an important role in measuring software quality and predicting defects. Co-change metrics such as number of files changed together has been used as a predictor of bugs. In this study, we further investigate the impact of specific characteristics of co-change dispersion on software quality. Using statistical regression models we show that co-changes that include files from different subsystems result in more bugs than co-changes that include files only from the same subsystem. This can be used to improve bug prediction models based on co-changes.","n":0.107}}},{"i":66,"$":{"0":{"v":"Software Architecture in Practice","n":0.5},"1":{"v":"From the Book:\r\n\r\nOur goals for the first edition were threefold. First, we wanted to show through authentic case studies actual examples of software architectures solving real-world problems. Second, we wanted to establish and show the strong connection between an architecture and an organization's business goals. And third, we wanted to explain the importance of software architecture in achieving the quality goals for a system. \r\n\r\nOur goals for this second edition are the same, but the passage of time since the writing of the first edition has brought new developments in the field and new understanding of the important underpinnings of software architecture. We reflect the new developments with new case studies and the new understanding both through new chapters and through additions to and elaboration of the existing chapters.\r\n\r\nArchitecture analysis, design, reconstruction, and documentation have all had major developments since the first edition. Architecture analysis has developed into a mature field with industrial-strength methods. This is reflected by a new chapter about the architecture tradeoff analysis method (ATAM). The ATAM has been adopted by industrial organizations as a technique for evaluating their software architectures.\r\n\r\nArchitecture design has also had major developments since the first edition. The capturing of quality requirements, the achievement of those requirements through small-scale and large-scale architectural approaches (tactics and patterns, respectively), and a design method that reflects knowledge of how to achieve qualities are all captured in various chapters. Three new chapters treat understanding quality requirements, achieving qualities, and theattribute driven design (ADD) method, respectively.\r\n\r\nArchitecture reconstruction or reverse engineering is an essential activity for capturing undocumented architectures. It can be used as a portion of a design project, an analysis project, or to provide input into a decision process to determine what to use as a basis for reconstructing an existing system. In the first edition, we briefly mentioned a tool set (Dali) and its uses in the re-engineering context; in in this edition the topic merits its own chapter.\r\n\r\nDocumenting software architectures is another topic that has matured considerably in the recent past. When the first edition was published, the Unified Modeling Language (UML) was just arriving on the scene. Now it is firmly entrenched, a reality reflected by all-new diagrams. But more important, an understanding of what kind of information to capture about an architecture, beyond what notation to use, has emerged. A new chapter covers architecture documentation.\r\n\r\nThe understanding of the application of software architecture to enable organizations to efficiently produce a variety of systems based on a single architecture is summarized in a totally rewritten chapter on software product lines. The chapter reinforces the link between architecture and an organization's business goals, as product lines, based around a software architecture, can enable order-of-magnitude improvements in cost, quality, and time to market. \r\n\r\nIn addition to the architectural developments, the technology for constructing distributed and Web-based systems has become prominent in today's economy. We reflect this trend by updating the World Wide Web chapter, by using Web-based examples for the ATAM chapter and the chapter on building systems from components, by replacing the CORBA case study with one on Enterprise JavaBeans (EJB), and by introducing a case study on a wireless EJB system designed to support wearable computers for maintenance technicians.\r\n\r\nFinally, we have added a chapter that looks more closely at the financial aspects of architectures. There we introduce a method--the CBAM--for basing architectural decisions on economic criteria, in addition to the technical criteria that we had focused on previously.\r\n\r\nAs in the first edition, we use the architecture business cycle as a unifying motif and all of the case studies are described in terms of the quality goals that motivated the system design and how the architecture for the system achieves those quality goals.\r\n\r\nIn this edition, as in the first, we were very aware that our primary audience is practitioners, so we focus on presenting material that has been found useful in many industrial applications, as well as what we expect practice to be in the near future.\r\n\r\nWe hope that you enjoy reading it at least as much as we enjoyed writing it.\r\n\r\n\r\n0321154959P12162002","n":0.038}}},{"i":67,"$":{"0":{"v":"The 4+1 View Model of architecture","n":0.408},"1":{"v":"The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them. The logical view describes the design's object model when an object-oriented design method is used. To design an application that is very data driven, you can use an alternative approach to develop some other form of logical view, such as an entity-relationship diagram. The process view describes the design's concurrency and synchronization aspects. The physical view describes the mapping of the software onto the hardware and reflects its distributed aspect. The development view describes the software's static organization in its development environment. >","n":0.089}}},{"i":68,"$":{"0":{"v":"Visualization of test information to assist fault localization","n":0.354},"1":{"v":"One of the most expensive and time-consuming components of the debugging process is locating the errors or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. This paper presents a new technique that uses visualization to assist with these tasks. The technique uses color to visually map the participation of each program statement in the outcome of the execution of the program with a test suite, consisting of both passed and failed test cases. Based on this visual mapping, a user can inspect the statements in the program, identify statements involved in failures, and locate potentially faulty statements. The paper also describes a prototype tool that implements our technique along with a set of empirical studies that use the tool for evaluation of the technique. The empirical studies show that, for the subject we studied, the technique can be effective in helping a user locate faults in a program.","n":0.079}}},{"i":69,"$":{"0":{"v":"Mining metrics to predict component failures","n":0.408},"1":{"v":"What is it that makes software fail? In an empirical study of the post-release defect history of five Microsoft software systems, we found that failure-prone software entities are statistically correlated with code complexity measures. However, there is no single set of complexity metrics that could act as a universally best defect predictor. Using principal component analysis on the code metrics, we built regression models that accurately predict the likelihood of post-release defects for new entities. The approach can easily be generalized to arbitrary projects; in particular, predictors obtained from one project can also be significant for new, similar projects.","n":0.101}}},{"i":70,"$":{"0":{"v":"Predicting the location and number of faults in large software systems","n":0.302},"1":{"v":"Advance knowledge of which files in the next release of a large software system are most likely to contain the largest numbers of faults can be a very valuable asset. To accomplish this, a negative binomial regression model has been developed and used to predict the expected number of faults in each file of the next release of a system. The predictions are based on the code of the file in the current release, and fault and modification history of the file from previous releases. The model has been applied to two large industrial systems, one with a history of 17 consecutive quarterly releases over 4 years, and the other with nine releases over 2 years. The predictions were quite accurate: for each release of the two systems, the 20 percent of the files with the highest predicted number of faults contained between 71 percent and 92 percent of the faults that were actually detected, with the overall average being 83 percent. The same model was also used to predict which files of the first system were likely to have the highest fault densities (faults per KLOC). In this case, the 20 percent of the files with the highest predicted fault densities contained an average of 62 percent of the system's detected faults. However, the identified files contained a much smaller percentage of the code mass than the files selected to maximize the numbers of faults. The model was also used to make predictions from a much smaller input set that only contained fault data from integration testing and later. The prediction was again very accurate, identifying files that contained from 71 percent to 93 percent of the faults, with the average being 84 percent. Finally, a highly simplified version of the predictor selected files containing, on average, 73 percent and 74 percent of the faults for the two systems.","n":0.057}}},{"i":71,"$":{"0":{"v":"Predicting fault incidence using software change history","n":0.378},"1":{"v":"This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar module, the older module will have roughly a third fewer faults. Our most successful model measures the fault potential of a module as the sum of contributions from all of the times the module has been changed, with large, recent changes receiving the most weight.","n":0.07}}},{"i":72,"$":{"0":{"v":"Design Rules, Volume 1, The Power of Modularity","n":0.354},"1":{"v":"We live in a dynamic economic and commercial world, surrounded by objects of remarkable complexity and power. In many industries, changes in products and technologies have brought with them new kinds of firms and forms of organization. We are discovering news ways of structuring work, of bringing buyers and sellers together, and of creating and using market information. Although our fast-moving economy often seems to be outside of our influence or control, human beings create the things that create the market forces. Devices, software programs, production processes, contracts, firms, and market are all the fruit of purposeful action: they are designed. Using the computer industry as an example, Carliss Y. Baldwin and Kim B. Clark develop a powerful theory of design and industrial evolution. They argue that the industry has experienced previously unimaginable levels of innovation and growth because it embraced the concept of modularity, building complex products from smaller subsystems that can be designed independently yet function together as a whole. Modularity freed designers to experiment with different approaches, as long as they obeyed the established design rules. Drawing upon the literatures of industrial organization, real options, and computer architecture, the authors provide insight into the forces of change that drive today's economy.","n":0.07}}},{"i":73,"$":{"0":{"v":"Identification of Move Method Refactoring Opportunities","n":0.408},"1":{"v":"Placement of attributes/methods within classes in an object-oriented system is usually guided by conceptual criteria and aided by appropriate metrics. Moving state and behavior between classes can help reduce coupling and increase cohesion, but it is nontrivial to identify where such refactorings should be applied. In this paper, we propose a methodology for the identification of Move Method refactoring opportunities that constitute a way for solving many common feature envy bad smells. An algorithm that employs the notion of distance between system entities (attributes/methods) and classes extracts a list of behavior-preserving refactorings based on the examination of a set of preconditions. In practice, a software system may exhibit such problems in many different places. Therefore, our approach measures the effect of all refactoring suggestions based on a novel entity placement metric that quantifies how well entities have been placed in system classes. The proposed methodology can be regarded as a semi-automatic approach since the designer will eventually decide whether a suggested refactoring should be applied or not based on conceptual or other design quality criteria. The evaluation of the proposed approach has been performed considering qualitative, metric, conceptual, and efficiency aspects of the suggested refactorings in a number of open-source projects.","n":0.071}}},{"i":74,"$":{"0":{"v":"SOBER: statistical model-based bug localization","n":0.447},"1":{"v":"Automated localization of software bugs is one of the essential issues in debugging aids. Previous studies indicated that the evaluation history of program predicates may disclose important clues about underlying bugs. In this paper, we propose a new statistical model-based approach, called SOBER, which localizes software bugs without any prior knowledge of program semantics. Unlike existing statistical debugging approaches that select predicates correlated with program failures, SOBER models evaluation patterns of predicates in both correct and incorrect runs respectively and regards a predicate as bug-relevant if its evaluation pattern in incorrect runs differs significantly from that in correct ones. SOBER features a principled quantification of the pattern difference that measures the bug-relevance of program predicates.We systematically evaluated our approach under the same setting as previous studies. The result demonstrated the power of our approach in bug localization: SOBER can help programmers locate 68 out of 130 bugs in the Siemens suite when programmers are expected to examine no more than 10% of the code, whereas the best previously reported is 52 out of 130. Moreover, with the assistance of SOBER, we found two bugs in bc 1.06 (an arbitrary precision calculator on UNIX/Linux), one of which has never been reported before.","n":0.071}}},{"i":75,"$":{"0":{"v":"Detection of logical coupling based on product release history","n":0.333},"1":{"v":"Code-based metrics such as coupling and cohesion are used to measure a system's structural complexity. But dealing with large systems-those consisting of several millions of lines-at the code level faces many problems. An alternative approach is to concentrate on the system's building blocks such as programs or modules as the unit of examination. We present an approach that uses information in a release history of a system to uncover logical dependencies and change patterns among modules. We have developed the approach by working with 20 releases of a large Telecommunications Switching System. We use release information such as version numbers of programs, modules, and subsystems together with change reports to discover common change behavior (i.e. change patterns) of modules. Our approach identifies logical coupling among modules in such a way that potential structural shortcomings can be identified and further examined, pointing to restructuring or reengineering opportunities.","n":0.083}}},{"i":76,"$":{"0":{"v":"SAAM: a method for analyzing the properties of software architectures","n":0.316},"1":{"v":"While software architecture has become an increasingly important research topic in recent years, insufficient attention has been paid to methods for evaluation of these architectures. Evaluating architectures is difficult for two main reasons. First, there is no common language used to describe different architectures. Second, there is no clear way of understanding an architecture with respect to an organization's life cycle concerns -software quality concerns such as maintainability portability, modularity, reusability, and so forth. We address these shortcomings by describing three perspectives by which we can understand the description of a software architecture and then proposing a five-step method for analyzing software architectures called SAAM (Software Architecture Analysis Method). We illustrate the method by analyzing three separate user interface architectures with respect to the quality of modifiability. >","n":0.088}}},{"i":77,"$":{"0":{"v":"Software Dependencies, Work Dependencies, and Their Impact on Failures","n":0.333},"1":{"v":"Prior research has shown that customer-reported software faults are often the result of violated dependencies that are not recognized by developers implementing software. Many types of dependencies and corresponding measures have been proposed to help address this problem. The objective of this research is to compare the relative performance of several of these dependency measures as they relate to customer-reported defects. Our analysis is based on data collected from two projects from two independent companies. Combined, our data set encompasses eight years of development activity involving 154 developers. The principal contribution of this study is the examination of the relative impact that syntactic, logical, and work dependencies have on the failure proneness of a software system. While all dependencies increase the fault proneness, the logical dependencies explained most of the variance in fault proneness, while workflow dependencies had more impact than syntactic dependencies. These results suggest that practices such as rearchitecting, guided by the network structure of logical dependencies, hold promise for reducing defects.","n":0.078}}},{"i":78,"$":{"0":{"v":"Quantifying the costs and benefits of architectural decisions","n":0.354},"1":{"v":"The benefits of a software system are assessable only relative to the business goals the system has been developed to serve. In turn, these benefits result from interactions between the system's functionality and its quality attributes (such as performance, reliabilty and security). Its quality attributes are, in most cases, dictated by its architectural design decisions. Therefore, we argue in this paper that the software architecture is the crucial artifact to study in making design tradeoffs and in performing cost-benefit analyses. A substantial part of such an analysis is in determining the level of uncertainty with which we estimate both costs and benefits. In this paper we offer an architecture-centric approach to the economic modeling of software design decision making called CBAM (Cost Benefit Analysis Method), in which costs and benefits are traded off with system quality attributes. We present the CBAM, the early results from applying this method in a large-scale case study, and discuss the application of more sophisticated economic models to software decision making.","n":0.078}}},{"i":79,"$":{"0":{"v":"Architecture reviews: practice and experience","n":0.447},"1":{"v":"Architecture reviews have evolved over the past decade to become a critical part of our continuing efforts to improve the state of affairs. We use them to identify project problems before they become costly to fix and to provide timely information to upper management so that they can make better-informed decisions. It provides the foundation for reuse, using commercially available software, and getting to the marketplace fast. The reviews also help identify best practices to projects and socialize such practices across the organization, thereby improving the organization's quality and operations.","n":0.105}}},{"i":80,"$":{"0":{"v":"Experience with performing architecture tradeoff analysis","n":0.408},"1":{"v":"Software architectures, like complex designs in any field, embody tradeoffs made by the designers. However, these tradeoffs are not always made explicitly by the designers and they may not understand the impacts of their decisions. This paper describes the use of a scenario-based and model-based analysis technique for software architectures-called ATAM-that not only analyzes a software architecture with respect to multiple quality attributes, but explicitly considers the tradeoffs inherent in the design. This is a method aimed at illuminating risks in the architecture through the identification of attribute trends, rather than at precise characterizations of measurable quality attribute values. In this paper, the operationalization of ATAM is illustrated via a specific example in which we analyzed a U.S. Army system for battlefield management.","n":0.09}}},{"i":81,"$":{"0":{"v":"Locating faulty code using failure-inducing chops","n":0.408},"1":{"v":"Software debugging is the process of locating and correcting faulty code. Prior techniques to locate faulty code either use program analysis techniques such as backward dynamic program slicing or exclusively use delta debugging to analyze the state changes during program execution. In this paper, we present a new approach that integrates the potential of delta debugging algorithm with the benefit of forward and backward dynamic program slicing to narrow down the search for faulty code. Our approach is to use delta debugging algorithm to identify a minimal failure-inducing input, use this input to compute a forward dynamic slice and then intersect the statements in this forward dynamic slice with the statements in the backward dynamic slice of the erroneous output to compute a failure-inducing chop. We implemented our technique and conducted experiments with faulty versions of several programs from the Siemens suite to evaluate our technique. Our experiments show that failure-inducing chops can greatly reduce the size of search space compared to the dynamic slices without significantly compromising the capability to locate the faulty code. We also applied our technique to several programs with known memory related bugs such as buffer overflow bugs. The failure-inducing chop in several of these cases contained only 2 to 4 statements which included the code causing memory corruption.","n":0.068}}},{"i":82,"$":{"0":{"v":"Decision-making techniques for software architecture design: A comparative survey","n":0.333},"1":{"v":"The architecture of a software-intensive system can be defined as the set of relevant design decisions that affect the qualities of the overall system functionality; therefore, architectural decisions are eventually crucial to the success of a software project. The software engineering literature describes several techniques to choose among architectural alternatives, but it gives no clear guidance on which technique is more suitable than another, and in which circumstances. As such, there is no systematic way for software engineers to choose among decision-making techniques for resolving tradeoffs in architecture design. In this article, we provide a comparison of existing decision-making techniques, aimed to guide architects in their selection. The results show that there is no “best” decision-making technique; however, some techniques are more susceptible to specific difficulties. Hence architects should choose a decision-making technique based on the difficulties that they wish to avoid. This article represents a first attempt to reason on meta-decision-making, that is, the issue of deciding how to decide.","n":0.079}}},{"i":83,"$":{"0":{"v":"A domain analysis to specify design defects and generate detection algorithms","n":0.302},"1":{"v":"Quality experts often need to identify in software systems design defects, which are recurring design problems, that hinder development and maintenance. Consequently, several defect detection approaches and tools have been proposed in the literature. However, we are not aware of any approach that defines and reifies the process of generating detection algorithms from the existing textual descriptions of defects. In this paper, we introduce an approach to automate the generation of detection algorithms from specifications written using a domain-specific language. The domain-specific is defined from a thorough domain analysis. We specify several design defects, generate automatically detection algorithms using templates, and validate the generated detection algorithms in terms of precision and recall on Xerces v2.7.0, an open-source object-oriented system.","n":0.092}}},{"i":84,"$":{"0":{"v":"Design Rule Hierarchies and Parallelism in Software Development Tasks","n":0.333},"1":{"v":"As software projects continue to grow in scale, being able to maximize the work that developers can carry out in parallel as a set of concurrent development tasks, without incurring excessive coordination overhead, becomes increasingly important. Prevailing design models, however, are not explicitly conceived to suggest how development tasks on the software modules they describe can be effectively parallelized. In this paper, we present a design rule hierarchy based on the assumption relations among design decisions. Software modules located within the same layer of the hierarchy suggest independent, hence parallelizable, tasks. Dependencies between layers or within a module suggest the need for coordination during concurrent work. We evaluate our approach by investigating the source code and mailing list of Apache Ant. We observe that technical communication between developers working on different modules within the same hierarchy layer, as predicted, is significantly less than communication between developers working across layers.","n":0.082}}},{"i":85,"$":{"0":{"v":"Titan: a toolset that connects software architecture with quality analysis","n":0.316},"1":{"v":"In this tool demo, we will illustrate our tool---Titan---that supports a new architecture model: design rule spaces (DRSpaces). We will show how Titan can capture both architecture and evolutionary structure and help to bridge the gap between architecture and defect prediction. We will demo how to use our toolset to capture hundreds of buggy files into just a few architecturally related groups, and to reveal architecture issues that contribute to the error-proneness and change-proneness of these groups. Our tool has been used to analyze dozens of large-scale industrial projects, and has demonstrated its ability to provide valuable direction on which parts of the architecture are problematic, and on why, when, and how to refactor. The video demo of Titan can be found at https://art.cs.drexel.edu/~lx52/titan.mp4","n":0.09}}},{"i":86,"$":{"0":{"v":"Analyzing error-prone system structure","n":0.5},"1":{"v":"Using measures of data interaction called data bindings, the authors quantify ratios of coupling and strength in software systems and use the ratios to identify error-prone system structures. A 148000 source line system from a prediction environment was selected for empirical analysis. Software error data were collected from high-level system design through system testing and from field operation of the system. The authors use a set of five tools to calculate the data bindings automatically and use a clustering technique to determine a hierarchical description of each of the system's 77 subsystems. A nonparametric analysis of variance model is used to characterize subsystems and individual routines that had either many or few errors or high or low error correction effort. The empirical results support the effectiveness of the data bindings clustering approach for localizing error-prone system structure. >","n":0.085}}},{"i":87,"$":{"0":{"v":"Modularity Analysis of Logical Design Models","n":0.408},"1":{"v":"Traditional design representations are inadequate for generalized reasoning about modularity in design and its technical and economic implications. We have developed an architectural modeling and analysis approach, and automated tool support, for improved reasoning in these terms. However, the complexity of constraint satisfaction limited the size of models that we could analyze. The contribution of this paper is a more scalable approach. We exploit the dominance relations in our models to guide a divide-and-conquer algorithm, which we have implemented it in our Simon tool. We evaluate its performance in case studies. The approach reduced the time needed to analyze small but representative models from hours to seconds. This work appears to make our modeling and analysis approach practical for research on the evolvability and economic properties of software design architectures","n":0.088}}},{"i":88,"$":{"0":{"v":"Refactoring support based on code clone analysis","n":0.378},"1":{"v":"Software maintenance is the most expensive activity in software development. Many software companies spent a large amount of cost to maintain the existing software systems. In perfective maintenance, refactoring has often been applied to the software to improve the understandability and complexity. One of the targets of refactoring is code clone. A code clone is a code fragment in a source code that is identical or similar to another. In an actual software development process, code clones are introduced because of various reasons such as reusing code by 'copy-and-paste' and so on. Code clones are one of the factors that make software maintenance difficult. In this paper, we propose a method which removes code clones from object oriented software by using existing refactoring patterns, especially Extract Method and Pull Up Method. Then, we have implemented a refactoring supporting tool based on the proposed method. Finally, we have applied the tool to an open source program and actually perform refactoring.","n":0.079}}},{"i":89,"$":{"0":{"v":"Integrating the Architecture Tradeoff Analysis Method (ATAM) with the Cost Benefit Analysis Method (CBAM)","n":0.267},"1":{"v":"Abstract : The Architecture Tradeoff Analysis Initiative at the Carnegie Mellon Software Engineering Institute (SEl) has developed a number of architecture-centric methods currently in use including the SEISM Architecture Tradeoff Analysis Method (ATAM), the SEl Quality Attribute Workshop (QAW), the SEl Cost Benefit Analysis Method (CBAM), SEl Active Reviews for Intermediate Designs (ARID), and the SE Attribute-Driven Design (ADD) method. Building on our success in developing and piloting a collection of software architecture methods, we're now focusing on integrating them, and building the bridges between them and the processes and architecture efforts outside the SEl, all the while continuing to refine existing methods and models. This technical note reports on a proposal to integrate the SE ATAM and SEl CBAM. The ATAM provides software architects with a framework for understanding the technical tradeoffs and risks they face as they make design decisions, but it does not provide any guidance for understanding economic tradeoffs. The CBAM helps software architects consider the return on investment of any architectural decision and provides guidance on the economic tradeoffs involved. The CBAM takes the architectural decision analysis done during the ATAM arid helps make it part of a strategic roadmap for software design and evolution by associating priorities, costs, and benefits with architectural decisions.","n":0.069}}},{"i":90,"$":{"0":{"v":"Leveraging design rules to improve software architecture recovery","n":0.354},"1":{"v":"In order to recover software architecture, various clustering techniques have been created to automatically partition a software system into meaningful subsystems. While these techniques have demonstrated their effectiveness, we observe that a key feature within most software systems has not been fully exploited: most well-designed systems follow strong architectural design rules that split the overall system into modules. These design rules are often manifested as special program constructs, such as shared data structures or abstract interfaces, which should not belong to any of the subordinate modules. We contribute a new perspective of architecture recovery based on this rationale, which enables the combination of design-rule-based clustering with other clustering techniques, as well as enabling the splitting of a large system into subsystems. We evaluated our approach both quantitatively and qualitatively, using both open source and real industrial software projects.","n":0.085}}},{"i":91,"$":{"0":{"v":"The Patterns Handbook: Techniques, Strategies, And Applications","n":0.378}}},{"i":92,"$":{"0":{"v":"Improving the Efficiency of Dependency Analysis in Logical Decision Models","n":0.316},"1":{"v":"To address the problem that existing software dependency extraction methods do not work on higher-level software artifacts, do not express decisions explicitly, and do not reveal implicit or indirect dependencies, our recent work explored the possibility of formally defining and automatically deriving a pairwise dependence relation from an augmented constraint networks (ACN) that models the assumption relation among design decisions. The current approach is difficult to scale, requiring constraint solving and solution enumeration. We observe that the assumption relation among design decisions for most software systems can be abstractly modeled using a special form of ACN. For these more restrictive, but highly representative models, we present an O(n^3) algorithm to derive the dependency relation without solving the constraints. We evaluate our approach by computing design structure matrices for existing ACNs that model multiple versions of heterogenous real software designs, often reducing the running time from hours to seconds.","n":0.082}}},{"i":93,"$":{"0":{"v":"Patterns and antipatterns","n":0.577}}},{"i":94,"$":{"0":{"v":"The Structure and Function of Complex Networks","n":0.378},"1":{"v":"Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.","n":0.115}}},{"i":95,"$":{"0":{"v":"Discovering Statistics Using Ibm Spss Statistics","n":0.408},"1":{"v":"Unrivalled in the way it makes the teaching of statistics compelling and accessible to even the most anxious of students, the only statistics textbook you and your students will ever need just got better! Andy Field's comprehensive and bestselling Discovering Statistics Using SPSS 4th Edition takes students from introductory statistical concepts through very advanced concepts, incorporating SPSS throughout. The Fourth Edition focuses on providing essential content updates, better accessibility to key features, more instructor resources, and more content specific to select disciplines. It also incorporates powerful new digital developments on the textbook's companion website(visit sagepub.com for more information). WebAssign The Fourth Edition will be available on WebAssign, allowing instructors to produce and manage assignments with their studnets online using a grade book that allows them to track and monitor students' progress. Students receive unlimited practice using a combination of approximately 2000 multiple choice and algorithmic questions. WebAssign provided students with instant feedback and links directly to the accompanying eBook section where the concept was covered, allowing students to find the correct solution. SAGE MobileStudy SAGE MobileStudy allows students equipped with smartphones and tablets to access select material, such as Cramming Sam's Study Tips, anywhere they receive mobile service. With QR codes included throughout the text, it's easy for students to get right to the section they need to study, allowing them to continue their study from virtually anywhere, even when they are away from thier printed copy of the text. Click here to preview the MobileStudy site (available late spring 2013). Education and Sport Sciences instructor support materials with enhanced ones for Psychology, Business and Management and the Health sciences make the book even more relevant to a wider range of subjects across the social sciences and where statistics is taught to a cross-disciplinary audience. Major Updates to the 4th Edition Fully compatible with recent SPSS releases up to and including version 20.0 Exciting new characters, including statistical cult leader Oditi, who provides students access to interesting and helpful video clips to illustrate statistical and SPSS concepts, and Confusious, who helps students clarify confusing quantitative terminology New discipline specific support matierlas have been added for Education, Sports Sciences, Psychology, Business & Management, and Health Sciences, making the book even more relevant to a wider range of subjects across the Social, Behavioral, and Health Sciences is taught to an interdisciplinary audience. An enhanced Companion Website (available late spring 2013) offers a wealth of material that can be used in conjunction with the textbook, including: PowerPoints Testbanks Answers to the Smart Alex tasks at the end of each chapter Datafiles for testing problems in SPSS Flashcards of key concepts Self-assessment multiple-choice questions Online videos of key statistical and SPSS procedures","n":0.047}}},{"i":96,"$":{"0":{"v":"Guidelines for conducting and reporting case study research in software engineering","n":0.302},"1":{"v":"Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.","n":0.087}}},{"i":97,"$":{"0":{"v":"Technical Debt: From Metaphor to Theory and Practice","n":0.354},"1":{"v":"The metaphor of technical debt in software development was introduced two decades ago to explain to nontechnical stakeholders the need for what we call now \"refactoring.\" As the term is being used to describe a wide range of phenomena, this paper proposes an organization of the technical debt landscape, and introduces the papers on technical debt contained in the issue.","n":0.129}}},{"i":98,"$":{"0":{"v":"Measuring and Monitoring Technical Debt","n":0.447},"1":{"v":"Abstract   Technical debt is a metaphor for immature, incomplete, or inadequate artifacts in the software development lifecycle that cause higher costs and lower quality in the long run. These artifacts remaining in a system affect subsequent development and maintenance activities, and so can be seen as a type of debt that the system developers owe the system. Incurring technical debt may speed up software development in the short run, but such benefit is achieved at the cost of extra work in the future, as if paying interest on the debt. In this sense, the technical debt metaphor characterizes the relationship between the short-term benefits of delaying certain software maintenance tasks or doing them quickly and less carefully, and the long-term cost of those delays. However, managing technical debt is more complicated than managing financial debt because of the uncertainty involved. In this chapter, the authors review the main issues associated with technical debt, and propose a technical debt management framework and a research plan for validation. The objective of our research agenda is to develop and validate a comprehensive technical debt theory that formalizes the relationship between the cost and benefit sides of the concept. Further, we propose to use the theory to propose mechanisms (processes and tools) for measuring and managing technical debt in software product maintenance. The theory and management mechanisms are intended ultimately to contribute to the improved quality of software and facilitate decision making in software maintenance.","n":0.064}}},{"i":99,"$":{"0":{"v":"Software modeling and measurement: the Goal/Question/Metric paradigm","n":0.378}}},{"i":100,"$":{"0":{"v":"Modularization Metrics: Assessing Package Organization in Legacy Large Object-Oriented Software","n":0.316},"1":{"v":"There exist many large object-oriented software systems consisting of several thousands of classes that are organized into several hundreds of packages. In such software systems, classes cannot be considered as units for software modularization. In such context, packages are not simply classes containers, but they also play the role of modules: a package should focus to provide well identified services to the rest of the software system. Therefore, understanding and assessing package organization is primordial for software maintenance tasks. Although there exist a lot of works proposing metrics for the quality of a single class and/or the quality of inter-class relationships, there exist few works dealing with some aspects for the quality of package organization and relationship. We believe that additional investigations are required for assessing package modularity aspects. The goal of this paper is to provide a complementary set of metrics that assess some modularity principles for packages in large legacy object-oriented software: Information-Hiding, Changeability and Reusability principles. Our metrics are defined with respect to object-oriented dependencies that are caused by inheritance and method call. We validate our metrics theoretically through a careful study of the mathematical properties of each metric.","n":0.072}}},{"i":101,"$":{"0":{"v":"Second international workshop on managing technical debt (MTD 2011)","n":0.333},"1":{"v":"The technical debt metaphor is gaining significant traction in the software development community as a way to understand and communicate issues of intrinsic quality, value, and cost. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a \"debt\": on which \"interest\" has to be paid and which should be repaid at some point for the long-term health of the project. Little is known about technical debt, beyond feelings and opinions. The software engineering research community has an opportunity to study this phenomenon and improve the way it is handled. We can offer software engineers a foundation for managing such trade-offs based on models of their economic impacts. The goal of this second workshop is to discuss managing technical debt as a part of the research agenda for the software engineering field.","n":0.08}}},{"i":102,"$":{"0":{"v":"4th international workshop on managing technical debt (MTD 2013)","n":0.333},"1":{"v":"Although now 20 years old, only recently has the concept of technical debt gained some momentum and credibility in the software engineering community. The goal of this fourth workshop on managing technical debt is to engage researchers and practitioners in exchanging ideas on viable research directions and on how to put the concept to actual use, beyond its usage as a rhetorical instrument to discuss the fate and ailments of software development projects. The workshop participants presented and discussed approaches to detect, analyze, visualize, and manage technical debt, in its various forms, on large software-intensive system developments.","n":0.102}}},{"i":103,"$":{"0":{"v":"Programs, life cycles, and laws of software evolution","n":0.354},"1":{"v":"By classifying programs according to their relationship to the environment in which they are executed, the paper identifies the sources of evolutionary pressure on computer applications and programs and shows why this results in a process of never ending maintenance activity. The resultant life cycle processes are then briefly discussed. The paper then introduces laws of Program Evolution that have been formulated following quantitative studies of the evolution of a number of different systems. Finally an example is provided of the application of Evolution Dynamics models to program release planning.","n":0.105}}},{"i":104,"$":{"0":{"v":"Code Quality: The Open Source Perspective","n":0.408}}},{"i":105,"$":{"0":{"v":"Quantitative evaluation of software quality","n":0.447},"1":{"v":"The study reported in this paper establishes a conceptual framework and some key initial results in the analysis of the characteristics of software quality. Its main results and conclusions are:   • Explicit attention to characteristics of software quality can lead to significant savings in software life-cycle costs.   • The current software state-of-the-art imposes specific limitations on our ability to automatically and quantitatively evaluate the quality of software.   • A definitive hierarchy of well-defined, well-differentiated characteristics of software quality is developed. Its higher-level structure reflects the actual uses to which software quality evaluation would be put; its lower-level characteristics are closely correlated with actual software metric evaluations which can be performed.   • A large number of software quality-evaluation metrics have been defined, classified, and evaluated with respect to their potential benefits, quantifiability, and ease of automation.   •Particular software life-cycle activities have been identified which have significant leverage on software quality.   Most importantly, we believe that the study reported in this paper provides for the first time a clear, well-defined framework for assessing the often slippery issues associated with software quality, via the consistent and mutually supportive sets of definitions, distinctions, guidelines, and experiences cited. This framework is certainly not complete, but it has been brought to a point sufficient to serve as a viable basis for future refinements and extensions.","n":0.068}}},{"i":106,"$":{"0":{"v":"Code Quality: The Open Source Perspective (Effective Software Development Series)","n":0.316},"1":{"v":"Page 26: How can I avoid off-by-one errors? Page 143: Are Trojan Horse attacks for real? Page 158: Where should I look when my application can't handle its workload? Page 256: How can I detect memory leaks? Page 309: How do I target my application to international markets? Page 394: How should I name my code's identifiers? Page 441: How can I find and improve the code coverage of my tests? Diomidis Spinellis' first book, Code Reading, showed programmers how to understand and modify key functional properties of software. Code Quality focuses on non-functional properties, demonstrating how to meet such critical requirements as reliability, security, portability, and maintainability, as well as efficiency in time and space.Spinellis draws on hundreds of examples from open source projects--such as the Apache web and application servers, the BSD Unix systems, and the HSQLDB Java database--to illustrate concepts and techniques that every professional software developer will be able to appreciate and apply immediately.Complete files for the open source code illustrated in this book are available on the Code Reading CD-ROM and online at: http://www.spinellis.gr/codequality/","n":0.075}}},{"i":107,"$":{"0":{"v":"Measuring the Psychological Complexity of Software Maintenance Tasks with the Halstead and McCabe Metrics","n":0.267},"1":{"v":"Three software complexity measures (Halstead's E, McCabe's u(G), and the length as measured by number of statements) were compared to programmer performance on two software maintenance tasks. In an experiment on understanding, length and u(G) correlated with the percent of statements correctly recalled. In an experiment on modification, most significant correlations were obtained with metrics computed on modified rather than unmodified code. All three metrics correlated with both the accuracy of the modification and the time to completion. Relationships in both experiments occurred primarily in unstructured rather than structured code, and in code with no comments. The metrics were also most predictive of performance for less experienced programmers. Thus, these metrics appear to assess psychological complexity primarily where programming practices do not provide assistance in understanding the code.","n":0.088}}},{"i":108,"$":{"0":{"v":"Third time charm: Stronger prediction of programmer performance by software complexity metrics","n":0.289},"1":{"v":"This experiment is the third in a series investigating characteristics of software which are related to its psychological complexity. A major focus of this research has been to validate the use of software complexity metrics for predicting programmer performance. In this experiment we improved experimental procedures which produced only modest results in the previous two studies. The experimental task required 54 experienced Fortran programmers to locate a single bug in each of three programs. Performance was measured by the time to locate and successfully correct the bug. Much stronger results were obtained than in earlier studies. Halstead's E proved to be the best predictor of performance, followed by McCabe's v (G) and the number of lines of code.","n":0.092}}},{"i":109,"$":{"0":{"v":"A Design Science Research Methodology for Information Systems Research","n":0.333},"1":{"v":"The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.","n":0.066}}},{"i":110,"$":{"0":{"v":"Using technical debt data in decision making: potential decision approaches","n":0.316},"1":{"v":"The management of technical debt ultimately requires decision making -- about incurring, paying off, or deferring technical debt instances. This position paper discusses several existing approaches to complex decision making, and suggests that exploring their applicability to technical debt decision making would be a worthwhile subject for further research.","n":0.143}}},{"i":111,"$":{"0":{"v":"Investigating Architectural Technical Debt accumulation and refactoring over time","n":0.333},"1":{"v":"Display Omitted We provide a taxonomy of the causes for Architectural Technical Debt accumulation.Crisis model: shows the increasing accumulation of Architectural Technical Debt.Phases model: shows when Architectural Technical Debt is accumulated and refactored.Refactoring strategies: best and worst case scenarios with respect to crises.We conduct an empirical evaluation of the factors and models. ContextA known problem in large software companies is to balance the prioritization of short-term with long-term feature delivery speed. Specifically, Architecture Technical Debt is regarded as sub-optimal architectural solutions taken to deliver fast that might hinder future feature development, which, in turn, would hinder agility. ObjectiveThis paper aims at improving software management by shedding light on the current factors responsible for the accumulation of Architectural Technical Debt and to understand how it evolves over time. MethodWe conducted an exploratory multiple-case embedded case study in 7 sites at 5 large companies. We evaluated the results with additional cross-company interviews and an in-depth, company-specific case study in which we initially evaluate factors and models. ResultsWe compiled a taxonomy of the factors and their influence in the accumulation of Architectural Technical Debt, and we provide two qualitative models of how the debt is accumulated and refactored over time in the studied companies. We also list a set of exploratory propositions on possible refactoring strategies that can be useful as insights for practitioners and as hypotheses for further research. ConclusionSeveral factors cause constant and unavoidable accumulation of Architecture Technical Debt, which leads to development crises. Refactorings are often overlooked in prioritization and they are often triggered by development crises, in a reactive fashion. Some of the factors are manageable, while others are external to the companies. ATD needs to be made visible, in order to postpone the crises according to the strategic goals of the companies. There is a need for practices and automated tools to proactively manage ATD.","n":0.057}}},{"i":112,"$":{"0":{"v":"Comparison of scenario-based software architecture evaluation methods","n":0.378},"1":{"v":"Software engineering community has proposed several methods to evaluate software architectures with respect to desired quality attributes such as maintainability, performance, and so on. There is, however, little effort on systematically comparing such methods to discover similarities and differences between existing approaches. In this paper, we compare four well known scenario-based SA evaluation methods using an evaluation framework. The framework considers each method from the point of view of method context, stakeholders, structure, and reliability. The comparison reveals that most of the studied methods are structurally similar but there are a number of differences among their activities and techniques. Therefore, some methods overlap, which guides us to identify five common activities that can form a generic process model.","n":0.092}}},{"i":113,"$":{"0":{"v":"Controversy Corner: What do software architects really do?","n":0.354},"1":{"v":"To be successful, a software architect-or a software architecture team, collectively-must strike a delicate balance between an external focus-both outwards: Listening to customers, users, watching technology, developing a long-term vision, and inwards: driving the development teams-and an internal, reflective focus: spending time to make the right design choices, validating them, and documenting them. Teams that stray too far away from this metastable equilibrium fall into some traps that we describe as antipatterns of software architecture teams.","n":0.115}}},{"i":114,"$":{"0":{"v":"A cost-benefit framework for making architectural decisions in a business context","n":0.302},"1":{"v":"In any IT-intensive organization, it is useful to have a model to associate a value with software and system architecture decisions. More generally, any effort---a project undertaken by a team---needs to have an associated value to offset its labor and capital costs. Unfortunately, it is extremely difficult to precisely evaluate the benefit of \"architecture projects\"---those that aim to improve one or more quality attributes of a system via a structural transformation without (generally) changing its behavior. We often resort to anecdotal and informal \"hand-waving\" arguments of risk reduction or increased developer productivity. These arguments are typically unsatisfying to the management of organizations accustomed to decision-making based on concrete metrics. This paper will discuss research done to address this long-standing dilemma. Specifically, we will present a model derived from analyzing actual projects undertaken at Vistaprint Corporation. The model presented is derived from an analysis of effort tracked against modifications to specific software components before and after a significant architectural transformation to the subsystem housing those components. In this paper, we will discuss the development, implementation, and iteration of the model and the results that we have obtained.","n":0.073}}},{"i":115,"$":{"0":{"v":"Role of Architects in Agile Organizations","n":0.408},"1":{"v":"Agile software development is broadly adopted in industry and works well for small-scale projects. In the context of large-scale development, however, there is a need for additional structure in the form of roles and practices, especially in the area of software architecture. In this chapter, we introduce the CAFFEA framework that defines a model for architecture governance. The framework defines three roles, i.e., chief architect, governance architect, and team architect, as well as a set of practices and responsibilities assigned to these roles. The CAFFEA framework has been developed and validated in close collaboration with several companies.","n":0.102}}},{"i":116,"$":{"0":{"v":"Identification and management of technical debt","n":0.408},"1":{"v":"ContextThe technical debt metaphor describes the effect of immature artifacts on software maintenance that bring a short-term benefit to the project in terms of increased productivity and lower cost, but that may have to be paid off with interest later. Much research has been performed to propose mechanisms to identify debt and decide the most appropriate moment to pay it off. It is important to investigate the current state of the art in order to provide both researchers and practitioners with information that enables further research activities as well as technical debt management in practice. ObjectiveThis paper has the following goals: to characterize the types of technical debt, identify indicators that can be used to find technical debt, identify management strategies, understand the maturity level of each proposal, and identify what visualization techniques have been proposed to support technical debt identification and management activities. MethodA systematic mapping study was performed based on a set of three research questions. In total, 100 studies, dated from 2010 to 2014, were evaluated. ResultsWe proposed an initial taxonomy of technical debt types, created a list of indicators that have been proposed to identify technical debt, identified the existing management strategies, and analyzed the current state of art on technical debt, identifying topics where new research efforts can be invested. ConclusionThe results of this mapping study can help to identify points that still require further investigation in technical debt research.","n":0.065}}},{"i":117,"$":{"0":{"v":"Detecting software modularity violations","n":0.5},"1":{"v":"This paper presents Clio, an approach that detects modularity violations, which can cause software defects, modularity decay, or expensive refactorings. Clio computes the discrepancies between how components should change together based on the modular structure, and how components actually change together as revealed in version history. We evaluated Clio using 15 releases of Hadoop Common and 10 releases of Eclipse JDT. The results show that hundreds of violations identified using Clio were indeed recognized as design problems or refactored by the developers in later versions. The identified violations exhibit multiple symptoms of poor design, some of which are not easily detectable using existing approaches.","n":0.098}}},{"i":118,"$":{"0":{"v":"Identification and analysis of the elements required to manage technical debt by means of a systematic mapping study","n":0.236},"1":{"v":"Identification and classification of the elements to manage technical debt (TD).The classification resulted in a framework including the stakeholders interests.The industrial relevance of current support to elements was assessed.The framework is a basis for building TD decision-making models.It was found out that TD management decisions are context-dependent. Technical debt, a metaphor for the long-term consequences of weak software development, must be managed to keep it under control. The main goal of this article is to identify and analyze the elements required to manage technical debt. The research method used to identify the elements is a systematic mapping, including a synthesis step to synthesize the elements definitions. Our perspective differs from previous literature reviews because it focused on the elements required to manage technical debt and not on the phenomenon of technical debt or the activities used in performing technical debt management. Additionally, the rigor and relevance for industry of the current techniques used to manage technical debt are studied. The elements were classified into three groups (basic decision-making factors, cost estimation techniques, practices and techniques for decision-making) and mapped according three stakeholders' points of view (engineering, engineering management, and business-organizational management). The definitions, classification, and analysis of the elements provide a framework that can be deployed to help in the development of models that are adapted to the specific stakeholders' interests to assist the decision-making required in technical debt management and to assess existing models and methods. The analysis indicated that technical debt management is context dependent.","n":0.064}}},{"i":119,"$":{"0":{"v":"Practical considerations, challenges, and requirements of tool-support for managing technical debt","n":0.302},"1":{"v":"Developing a software product with a high level of quality that also meets budget and schedule is the main goal of any organization. This usually implies making tradeoffs among conflicting aspects like number of features to implement, user perceived quality, time-to-market, and the ability of the company to maintain and improve the system in a feasible way in the future (aka, managing Technical Debt (TD)). In this paper we present a fresh perspective on TD from a CMMI Maturity Level 5 company. Examples, practical considerations, and challenges in dealing with TD are presented along with ten requirements of a tool for managing TD.","n":0.099}}},{"i":120,"$":{"0":{"v":"An analysis of techniques and methods for technical debt management: a reflection from the architecture perspective","n":0.25},"1":{"v":"Technical debt is a metaphor referring to the consequences of weak software development. Managing technical debt is necessary in order to keep it under control, and several techniques have been developed with the goal of accomplishing this. However, available techniques have grown disperse and managers lack guidance. This paper covers this gap by providing a systematic mapping of available techniques and methods for technical debt management, covering architectural debt, and identifying existing gaps that prevent to manage technical debt efficiently.","n":0.112}}},{"i":121,"$":{"0":{"v":"Alloy: a lightweight object modelling notation","n":0.408},"1":{"v":"Alloy is a little language for describing structural properties. It offers a declaration syntax compatible with graphical object models, and a set-based formula syntax powerful enough to express complex constraints and yet amenable to a fully automatic semantic analysis. Its meaning is given by translation to an even smaller (formally defined) kernel. This paper presents the language in its entirety, and explains its motivation, contributions and deficiencies.","n":0.122}}},{"i":122,"$":{"0":{"v":"An Exploratory Study on Self-Admitted Technical Debt","n":0.378},"1":{"v":"Throughout a software development life cycle, developers knowingly commit code that is either incomplete, requires rework, produces errors, or is a temporary workaround. Such incomplete or temporary workarounds are commonly referred to as 'technical debt'. Our experience indicates that self-admitted technical debt is common in software projects and may negatively impact software maintenance, however, to date very little is known about them. Therefore, in this paper, we use source-code comments in four large open source software projects-Eclipse, Chromium OS, Apache HTTP Server, and ArgoUML to identify self-admitted technical debt. Using the identified technical debt, we study 1) the amount of self-admitted technical debt found in these projects, 2) why this self-admitted technical debt was introduced into the software projects and 3) how likely is the self-admitted technical debt to be removed after their introduction. We find that the amount of self-admitted technical debt exists in 2.4% -- 31% of the files. Furthermore, we find that developers with higher experience tend to introduce most of the self-admitted technical debt and that time pressures and complexity of the code do not correlate with the amount of self-admitted technical debt. Lastly, although self-admitted technical debt is meant to be addressed or removed in the future, only between 26.3% -- 63.5% of self-admitted technical debt gets removed from projects after introduction.","n":0.068}}},{"i":123,"$":{"0":{"v":"Sentiment Polarity Detection for Software Development","n":0.408},"1":{"v":"The role of sentiment analysis is increasingly emerging to study software developers' emotions by mining crowd-generated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers' communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexicon- and keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines.","n":0.082}}},{"i":124,"$":{"0":{"v":"Managing Technical Debt in Software Engineering (Dagstuhl Seminar 16162)","n":0.333},"1":{"v":"This report documents the program and outcomes of Dagstuhl Seminar 16162, “Managing Technical Debt in Software Engineering.” We summarize the goals and format of the seminar, results from the breakout groups, a definition for technical debt, a draft conceptual model, and a research road map that culminated from the discussions during the seminar. The report also includes the abstracts of the talks presented at the seminar and summaries of open discussions.","n":0.119}}},{"i":125,"$":{"0":{"v":"Tool Support for Architectural Decisions","n":0.447},"1":{"v":"In contrast to software architecture models, architectural decisions are often not explicitly documented, and therefore eventually lost. This contributes to major problems such as high-cost system evolution, stakeholders mis-communication, and limited reusability of core system assets. An approach is outlined that systematically and semi-automatically documents architectural decisions and allows them to be effectively shared by the stakeholders. A first attempt is presented that partially implements the approach by binding architectural decisions, models and the system implementation. The approach is demonstrated with an example demonstrating its usefulness with regards to some industrial use cases.","n":0.104}}},{"i":126,"$":{"0":{"v":"Managing architectural technical debt: A unified model and systematic literature review","n":0.302},"1":{"v":"Large Software Companies need to support the continuous and fast delivery of customer value in both the short and long term. However, this can be impeded if the evolution and maintenance of existing systems is hampered by what has been recently termed Technical Debt (TD). Specifically, Architectural TD has received increased attention in the last few years due to its significant impact on system success and, left unchecked, it can cause expensive repercussions. It is therefore important to understand the underlying factors of architectural TD. With this as background, there is a need for a descriptive model to illustrate and explain different architectural TD issues. The aim of this study is to synthesize and compile research efforts with the goal of creating new knowledge with a specific interest in the architectural TD field. The contribution of this paper is the presentation of a novel descriptive model, providing a comprehensive interpretation of the architectural TD phenomenon. This model categorizes the main characteristics of architectural TD and reveals their relations. The results show that, by using this model, different stakeholders could increase the system's success rate, and lower the rate of negative consequences, by raising awareness about architectural TD.","n":0.071}}},{"i":127,"$":{"0":{"v":"A Contextualized Vocabulary Model for identifying technical debt on code comments","n":0.302},"1":{"v":"The identification of technical debt (TD) is an important step to effectively manage it. In this context, a set of indicators has been used by automated approaches to identify TD items, but some debt may not be directly identified using only metrics collected from the source code. In this work we propose CVM-TD, a model to support the identification of technical debt through code comment analysis. We performed an exploratory study on two large open sources projects with the goal of characterizing the feasibility of the proposed model to support the detection of TD through code comments analysis. The results indicate that (1) developers use the dimensions considered by CVM-TD when writing code comments, (2) CVM-TD provides a vocabulary that may be used to detect TD items, and (3) the proposed model needs to be calibrated in order to reduce the difference between comments returned by the vocabulary and those that may indicate a TD item. Code comments analysis can be used to detect TD in software projects and CVM-TD may support the development team to perform this task.","n":0.075}}},{"i":128,"$":{"0":{"v":"An enhanced architectural knowledge metamodel linking architectural design decisions to other artifacts in the software engineering lifecycle","n":0.243},"1":{"v":"Software architects create and consume many interrelated artifacts during the architecting process. These artifacts may represent functional and nonfunctional requirements, architectural patterns, infrastructure topology units, code, and deployment descriptors as well as architecturally significant design decisions. Design decisions have to be linked to chunks of architecture description in order to achieve a fine-grained control when a design is modified. Moreover, it is imperative to identify quickly the key decisions affected by a runtime change that are critical for a system's mission. This paper extends previous work on architectural knowledge with a metamodel for architectural decision capturing and sharing to: (i) create and maintain fine-grained dependency links between the entities during decision identification, making, and enforcement, (ii) keep track of the evolution of the decisions, and (iii) support runtime decisions.","n":0.088}}},{"i":129,"$":{"0":{"v":"Detecting Technical Debt through Issue Trackers.","n":0.408}}},{"i":130,"$":{"0":{"v":"Architecture viewpoints for documenting architectural technical debt","n":0.378},"1":{"v":"Abstract   Technical debt (TD) has attracted an increasing interest from researchers and practitioners in the software engineering domain. Currently, most approaches to managing TD focus on dealing with TD at source code level, while few methods deal with TD at architecture level. If architectural technical debt (ATD) is not effectively managed in the architecting process, the knowledge about ATD is not made available to involved stakeholders and the impact of ATD is not considered during architecture decision-making. Thus, the system’s maintainability and evolvability can be intentionally or unintentionally compromised. As a result, architectures are costly to maintain and new features are difficult to introduce. To facilitate the management of ATD, it needs to be documented so that it becomes explicit to stakeholders. To this end, we propose a set of architecture viewpoints related to ATD (ATD viewpoints in short). Each viewpoint frames a number of concerns related to ATD. These ATD viewpoints together help to get a comprehensive understanding of ATD in a software system, thereby providing support for architecture decision-making. To evaluate the effectiveness of the ATD viewpoints in documenting ATD, we conducted a case study in a large telecommunications company. The results of this case study show that the documented ATD views can effectively facilitate the documentation of ATD. Specifically, the ATD viewpoints are relatively easy to understand; it takes an acceptable amount of effort to document ATD using the ATD viewpoints; and the documented ATD views are useful for stakeholders to understand the ATD in the software project.","n":0.063}}},{"i":131,"$":{"0":{"v":"Continuous Architectural Knowledge Integration: Making Heterogeneous Architectural Knowledge Available in Large-Scale Organizations","n":0.289},"1":{"v":"The timely discovery, sharing and integration of architectural knowledge (AK) have become critical aspects in enabling the software architects to make meaningful conceptual and technical design decisions and trade-offs. In large-scale organizations particular obstacles in making AK available to architects are a heterogeneous pool of internal and external knowledge sources, poor interoperability between AK management tools and limited support of computational AK reasoning. Therefore we introduce the Continuous Architectural Knowledge Integration (CAKI) approach that combines the continuous integration of internal and external AK sources together with enhanced semantic reasoning and personalization capabilities dedicated to large organizations. Preliminary evaluation results show that CAKI potentially reduces AK search effort by concurrently yielding more diverse and relevant results.","n":0.093}}},{"i":132,"$":{"0":{"v":"Architectural Technical Debt Identification: Moving Forward","n":0.408},"1":{"v":"Architectural technical debt is a metaphor used to describe sub-optimal architectural design and implementation choices that bring short-term benefits to the cost of the long-term gradual deterioration of the quality of software. Architectural technical debt is an active field of research. Nevertheless, how to accurately identify architectural technical debt is still an open question. Our research aims to fill this gap. We strive to: (i) consolidate the existing knowledge of architectural technical debt identification in practice, (ii) conceive novel identification approaches built upon the existing state of the art techniques and industrial needs, and (iii) provide empirical evidence of architectural technical debt phenomena and assess the viability of the conceived approaches.","n":0.095}}},{"i":133,"$":{"0":{"v":"C3: A METAMODEL FOR ARCHITECTURE DESCRIPTION LANGUAGE BASED ON FIRST-ORDER CONNECTOR TYPES","n":0.289},"1":{"v":"To provide hierarchical description from different software architectural viewpoints we need more than one abstraction hierarchy and connection mechanisms to support the interactions among components. Also,ion hierarchy and connection mechanisms to support the interactions among components. Also, these mechanisms will support the refinement and traceability of architectural elements through the different levels of each hierarchy. Current methods and tools provide poor support for the challenge posed by developing system using hierarchical description. This paper describes an architecture-centric approach allowing the user to describe the logical architecture view where a physical architecture view is generated automatically for all application instances of the logical architecture.","n":0.099}}},{"i":134,"$":{"0":{"v":"Towards Recovering Architectural Information from Images of Architectural Diagrams","n":0.333},"1":{"v":"The architecture of a software system is often described with diagrams embedded in the documentation. However, these diagrams are normally stored and shared as images, losing track of model-level archi- tectural information and refraining software engineers from working on the architectural model later on. In this context, tools able to extract architectural information from images can be of great help. In this arti- cle, we present a framework called IMEAV for processing architectural diagrams (based on specic viewtypes) and recovering information from them. We have instantiated our framework to analyze \\module views\" and evaluated this prototype with an image dataset. Results have been encouraging, showing a good accuracy for recognizing modules, relations and textual features.","n":0.093}}},{"i":135,"$":{"0":{"v":"Data Mining: Practical Machine Learning Tools and Techniques","n":0.354},"1":{"v":"Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization","n":0.068}}},{"i":136,"$":{"0":{"v":"Agile Software Development, Principles, Patterns, and Practices","n":0.378},"1":{"v":"From the Publisher:\r\nBest selling author and world-renowned software development expert Robert C. Martin shows how to solve the most challenging problems facing software developers, project managers, and software project leaders today. \r\n\r\nThis comprehensive, pragmatic tutorial on Agile Development and eXtreme programming, written by one of the founding father of Agile Development:\r\nTeaches software developers and project managers how to get projects done on time, and on budget using the power of Agile Development.\r\nUses real-world case studies to show how to of plan, test, refactor, and pair program using eXtreme programming.\r\nContains a wealth of reusable C++ and Java code.\r\nFocuses on solving customer oriented systems problems using UML and Design Patterns.\r\n\r\n\r\nRobert C. Martin is President of Object Mentor Inc. Martin and his team of software consultants use Object-Oriented Design, Patterns, UML, Agile Methodologies, and eXtreme Programming with worldwide clients. He is the author of the best-selling book Designing Object-Oriented C++ Applications Using the Booch Method (Prentice Hall, 1995), Chief Editor of, Pattern Languages of Program Design 3 (Addison Wesley, 1997), Editor of, More C++ Gems (Cambridge, 1999), and co-author of XP in Practice, with James Newkirk (Addison-Wesley, 2001). He was Editor in Chief of the C++ Report from 1996 to 1999. He is a featured speaker at international conferences and trade shows. \r\n\r\nAuthor Biography: \r\nROBERT C. MARTIN is President of Object Mentor Inc. Martin and his team of software consultants use Object-Oriented Design, Patterns, UML, Agile Methodologies, and eXtreme Programming with worldwide clients. He is the author of the best-selling book Designing Object-Oriented C++ Applications Using the Booch Method (Prentice Hall, 1995), Chief Editor of, Pattern Languages of Program Design 3 (Addison Wesley, 1997), Editor of, More C++ Gems (Cambridge, 1999), and co-author of XP in Practice, with James Newkirk (Addison-Wesley, 2001). He was Editor in Chief of the C++ Report from 1996 to 1999. He is a featured speaker at international conferences and trade shows.","n":0.057}}},{"i":137,"$":{"0":{"v":"Case Study Research in Software Engineering: Guidelines and Examples","n":0.333},"1":{"v":"Based on their own experiences of in-depth case studies of software projects in international corporations, in this bookthe authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering. This is the first software engineering specific book on thecase study research method.","n":0.143}}},{"i":138,"$":{"0":{"v":"Software Engineering: Principles and Practice","n":0.447},"1":{"v":"Software Engineering: Principles and Practice challenges the reader to appreciate the issues, design trade-offs and teamwork required for successful software development. This new edition has been brought fully up to date, with complete coverage of all aspects of the software lifecycle and a strong focus on all the skills needed to carry out software projects on time and within budget. Highlights of the third edition include: * Fully updated chapters on requirements engineering and software architecture. * New chapters on component-based software engineering, service orientation and global software development. * Extensive coverage of the human and social aspects of software development. * Balanced coverage of both traditional, heavyweight development and agile, lightweight development approaches such as Extreme Programming (XP). Written to support both introductory and advanced software engineering courses, this book is invaluable for everyone in software development and maintenance who wants an accessible account of the problems incurred in large-scale software development and the proposed solutions. A companion website with additional resources for students and instructors can be found at www.wileyeurope.com/college/van vliet","n":0.076}}},{"i":139,"$":{"0":{"v":"Introducing a Ripple Effect Measure: A Theoretical and Empirical Validation","n":0.316},"1":{"v":"Context: Change impact analysis investigates the negative consequence of system changes, i.e., the propagation of changes to other parts of the system (also known as the ripple effect). Identifying modules of the system that will be affected by the ripple effect is an important activity, before and after the application of any change. Goal: However, in the literature, there is only a limited set of studies that investigate the probability of a random change occurring in one class, to propagate to another. In this paper we discuss and evaluate the Ripple Effect Measure (in short REM), a metric that can be used to assess the aforementioned probability. Method: To evaluate the capacity of REM as an assessor of the prob-ability of a class to change due to the ripple effect, we: (a) mathematically validate it against established metric properties (e.g., non-negativity, monotonicity, etc.), proposed by Briand et al., and (b) empirically investigate its validity as an assessor of class proneness to the ripple effect, based on the 1061-1998 IEEE Standard on Software Measurement (e.g., correlation, predictive power, etc.). To apply the empirical validation process, we conducted a holistic multiple-case study on java open-source classes. Results: The results of REM validation (both mathematical and empirical) suggest that REM is a theoretically sound measure that is the most valid assessor of the probability of a class to change due to the ripple effect, compared to other existing metrics.","n":0.065}}},{"i":140,"$":{"0":{"v":"Software metrics fluctuation","n":0.577},"1":{"v":"ContextSoftware quality attributes are assessed by employing appropriate metrics. However, the choice of such metrics is not always obvious and is further complicated by the multitude of available metrics. To assist metrics selection, several properties have been proposed. However, although metrics are often used to assess successive software versions, there is no property that assesses their ability to capture structural changes along evolution. ObjectiveWe introduce a property, Software Metric Fluctuation (SMF), which quantifies the degree to which a metric score varies, due to changes occurring between successive system's versions. Regarding SMF, metrics can be characterized as sensitive (changes induce high variation on the metric score) or stable (changes induce low variation on the metric score). MethodSMF property has been evaluated by: (a) a case study on 20 OSS projects to assess the ability of SMF to differently characterize different metrics, and (b) a case study on 10 software engineers to assess SMF's usefulness in the metric selection process. ResultsThe results of the first case study suggest that different metrics that quantify the same quality attributes present differences in their fluctuation. We also provide evidence that an additional factor that is related to metrics' fluctuation is the function that is used for aggregating metric from the micro to the macro level. In addition, the outcome of the second case study suggested that SMF is capable of helping practitioners in metric selection, since: (a) different practitioners have different perception of metric fluctuation, and (b) this perception is less accurate than the systematic approach that SMF offers. ConclusionsSMF is a useful metric property that can improve the accuracy of metrics selection. Based on SMF, we can differentiate metrics, based on their degree of fluctuation. Such results can provide input to researchers and practitioners in their metric selection processes.","n":0.058}}},{"i":141,"$":{"0":{"v":"Assessing Change Proneness at the Architecture Level: An Empirical Validation","n":0.316},"1":{"v":"Change proneness is a characteristic of software artifacts that represents their probability to change in future. Change proneness can be assessed at different levels of granularity, ranging from classes to modules. Although change proneness can be successfully assessed at the source code level (i.e., methods and classes), it remains rather unexplored for architectures. Additionally, the methods that have been introduced at the source code level are not directly transferable to the architecture level. In this paper, we propose and empirically validate a method for assessing the change proneness of architectural modules. Assessing change proneness at the level of architectural modules requires information from two sources: (a) the history of changes in the module, as a proxy of how frequently the module itself undergoes changes; and (b) the dependencies with other modules that affect the probability of a change being propagated from one module to the other. To validate the proposed approach, we performed a case study on five open-source projects. Specifically, we compared the accuracy of the proposed approach to the use of software package metrics as assessors of modules change proneness, based on the 1061-1998 IEEE Standard. The results suggest that compared to examined metrics, the proposed method is a better assessor of change proneness. Therefore, we believe that the method and accompanying tool can effectively aid architects during software maintenance and evolution.","n":0.067}}},{"i":142,"$":{"0":{"v":"Cross versus Within-Company Cost Estimation Studies: A Systematic Review","n":0.333},"1":{"v":"The objective of this paper is to determine under what circumstances individual organizations would be able to rely on cross-company-based estimation models. We performed a systematic review of studies that compared predictions from cross-company models with predictions from within-company models based on analysis of project data. Ten papers compared cross-company and within-company estimation models; however, only seven presented independent results. Of those seven, three found that cross-company models were not significantly different from within-company models, and four found that cross-company models were significantly worse than within-company models. Experimental procedures used by the studies differed making it impossible to undertake formal meta-analysis of the results. The main trend distinguishing study results was that studies with small within-company data sets (i.e., $20 projects) that used leave-one-out cross validation all found that the within-company model was significantly different (better) from the cross-company model. The results of this review are inconclusive. It is clear that some organizations would be ill-served by cross-company models whereas others would benefit. Further studies are needed, but they must be independent (i.e., based on different data bases or at least different single company data sets) and should address specific hypotheses concerning the conditions that would favor cross-company or within-company models. In addition, experimenters need to standardize their experimental procedures to enable formal meta-analysis, and recommendations are made in Section 3.","n":0.067}}},{"i":143,"$":{"0":{"v":"Searching for build debt: experiences managing technical debt at Google","n":0.316},"1":{"v":"With a large and rapidly changing codebase, Google software engineers are constantly paying interest on various forms of technical debt. Google engineers also make efforts to pay down that debt, whether through special Fixit days, or via dedicated teams, variously known as janitors, cultivators, or demolition experts. We describe several related efforts to measure and pay down technical debt found in Google's BUILD files and associated dead code. We address debt found in dependency specifications, unbuildable targets, and unnecessary command line flags. These efforts often expose other forms of technical debt that must first be managed.","n":0.102}}},{"i":144,"$":{"0":{"v":"Bayesian analysis of empirical software engineering cost models","n":0.354},"1":{"v":"Many parametric software estimation models have evolved in the last two decades (L.H. Putnam and W. Myers, 1992; C. Jones, 1997; R.M. Park et al., 1992). Almost all of these parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. As discussed in the paper, the multiple regression approach imposes a few assumptions frequently violated by software engineering datasets. The paper illustrates the problems faced by the multiple regression approach during the calibration of one of the popular software engineering cost models, COCOMO II. It describes the use of a pragmatic 10 percent weighted average approach that was used for the first publicly available calibrated version (S. Chulani et al., 1998). It then moves on to show how a more sophisticated Bayesian approach can be used to alleviate some of the problems faced by multiple regression. It compares and contrasts the two empirical approaches, and concludes that the Bayesian approach was better and more robust than the multiple regression approach.","n":0.075}}},{"i":145,"$":{"0":{"v":"Architectural Tactics to Support Rapid and Agile Stability","n":0.354},"1":{"v":"Abstract : The essence of stability in software development is the ability to produce quality software with infrastructure that will meet long-term business goals. The essence of rapid and agile development is the ability to deliver capabilities quickly based on customer priorities. Stability often requires cross-functional analysis and infrastructure support that will build foundational technology for the capabilities to stand on, which takes time and resources. But today's organizations must attend to both agility and enduring design. This article presents three tactics that support rapid and agile stability: aligning feature-based development and system decomposition, creating an architectural runway, and using matrix teams.","n":0.099}}},{"i":146,"$":{"0":{"v":"On the limits of the technical debt metaphor: some guidance on going beyond","n":0.277},"1":{"v":"Over recent years the topic of technical debt has gained significant attention in the software engineering community. The area of technical debt research is somewhat peculiar within software engineering as it is built on a metaphor. This has certainly benefited the field as it helps to achieve a lot of attention and eases communication about the topic, however, it seems it is to some extent also sidetracking research work, if the metaphor is used beyond its range of applicability. In this paper, we focus on the limits of the metaphor and the problems that arise when over-extending its applicability. We do also aim at providing some additional insights by proposing certain ways of handling these restrictions.","n":0.093}}},{"i":147,"$":{"0":{"v":"Using Architecturally Significant Requirements for Guiding System Evolution","n":0.354},"1":{"v":"Rapidly changing technology is one of the key triggers of system evolution. Some examples are: physically relocating a data center, replacement of infrastructure such as migrating from an in-house broker to CORBA, moving to a new architectural approach such as migrating from client-server to a service-oriented architecture. At a high level, the goals of such an evolution are easy to describe. While the end goals of the evolution are typically captured and known, the key architecturally significant requirements that guide the actual evolution tasks are often unexplored. At best, they are tucked under maintainability and/or modifiability concerns. In this paper, we argue that eliciting and using architecturally significant requirements of an evolution has a potential to significantly improve the quality of the evolution effort. We focus on elicitation and representation techniques of architecturally significant evolution requirements, and demonstrate their use in analysis for evolution planning.","n":0.083}}},{"i":148,"$":{"0":{"v":"The Discovery of Grounded Theory: Strategies for Qualitative Research","n":0.333},"1":{"v":"Most writing on sociological method has been concerned with how accurate facts can be obtained and how theory can thereby be more rigorously tested. In The Discovery of Grounded Theory, Barney Glaser and Anselm Strauss address the equally Important enterprise of how the discovery of theory from data--systematically obtained and analyzed in social research--can be furthered. The discovery of theory from data--grounded theory--is a major task confronting sociology, for such a theory fits empirical situations, and is understandable to sociologists and laymen alike. Most important, it provides relevant predictions, explanations, interpretations, and applications. In Part I of the book, \"Generation Theory by Comparative Analysis,\" the authors present a strategy whereby sociologists can facilitate the discovery of grounded theory, both substantive and formal. This strategy involves the systematic choice and study of several comparison groups. In Part II, The Flexible Use of Data,\" the generation of theory from qualitative, especially documentary, and quantitative data Is considered. In Part III, \"Implications of Grounded Theory,\" Glaser and Strauss examine the credibility of grounded theory. The Discovery of Grounded Theory is directed toward improving social scientists' capacity for generating theory that will be relevant to their research. While aimed primarily at sociologists, it will be useful to anyone Interested In studying social phenomena--political, educational, economic, industrial-- especially If their studies are based on qualitative data.","n":0.067}}},{"i":149,"$":{"0":{"v":"Qualitative methods in empirical studies of software engineering","n":0.354},"1":{"v":"While empirical studies in software engineering are beginning to gain recognition in the research community, this subarea is also entering a new level of maturity by beginning to address the human aspects of software development. This added focus has added a new layer of complexity to an already challenging area of research. Along with new research questions, new research methods are needed to study nontechnical aspects of software engineering. In many other disciplines, qualitative research methods have been developed and are commonly used to handle the complexity of issues involving human behaviour. The paper presents several qualitative methods for data collection and analysis and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combined with quantitative methods. To illustrate this use of qualitative methods, examples from real software engineering studies are used throughout.","n":0.083}}},{"i":150,"$":{"0":{"v":"An Ontology of Architectural Design Decisions in Software-Intensive Systems","n":0.333},"1":{"v":"Architectural design decisions deserve to be first class entities in the process of developing complex software-intensive systems. Preserving the graphs of decisions and all their interdependencies will support the evolution and maintenance of such systems. In this paper we present a possible ontology of architectural design decisions, their attributes and relationships, for complex, software-intensive systems.","n":0.135}}},{"i":151,"$":{"0":{"v":"Architectural design decision: Existing models and tools","n":0.378},"1":{"v":"In the field of software architecture, there has been a paradigm shift from describing the outcome of architecting process mostly described by component and connector (know-what) to documenting architectural design decisions and their rationale (know-how) which leads to the production of an architecture. This paradigm shift results in emergence of various models and related tools for capturing, managing and sharing architectural design decisions and their rationale explicitly. This paper analyzes existing architectural design decisions models and provides a criteria-based comparison on tools that support these models. The major contribution of this paper is twofold: to show that all of these models have a consensus on capturing the essence of an architectural design decision; and to clarify the major difference among the tools and show what desired features are missing in these tools.","n":0.087}}},{"i":152,"$":{"0":{"v":"Organizing the technical debt landscape","n":0.447},"1":{"v":"To date, several methods and tools for detecting source code and design anomalies have been developed. While each method focuses on identifying certain classes of source code anomalies that potentially relate to technical debt (TD), the overlaps and gaps among these classes and TD have not been rigorously demonstrated. We propose to construct a seminal technical debt landscape as a way to visualize and organize research on the subject.","n":0.12}}},{"i":153,"$":{"0":{"v":"Generating precise dependencies for large software","n":0.408},"1":{"v":"Intra- and inter-module dependencies can be a significant source of technical debt in the long-term software development, especially for large software with millions of lines of code. This paper designs and implements a precise and scalable tool that extracts code dependencies and their utilization for large C/C++ software projects. The tool extracts both symbol-level and module-level dependencies of a software system and identifies potential underutilized and inconsistent dependencies. Such information points to potential refactoring opportunities and help developers perform large-scale refactoring tasks.","n":0.11}}},{"i":154,"$":{"0":{"v":"DCAR - Decision-Centric Architecture Reviews","n":0.447},"1":{"v":"Architecture evaluation is an important activity in the software engineering lifecycle that ensures that the architecture satisfies the stakeholders' expectations. Additionally, risks and issues can be uncovered before they cause tremendous costs later in the lifecycle. Unfortunately, architecture evaluation is not regularly practiced in industry. In this paper, we present DCAR (Decision-Centric Architecture Review), an architecture evaluation method that uses architecture decisions as first class entities. DCAR uncovers and evaluates the rationale behind the most important architecture decisions, considering the entire context, in which the decisions were made. Furthermore, it is lightweight and can be performed during or after the design was finalized. Experiences in large industrial projects have shown that full-scale DCAR evaluations, including reporting, can be conducted in less than five person-days, while producing satisfying results for the stakeholders.","n":0.087}}},{"i":155,"$":{"0":{"v":"Software Estimation: Demystifying the Black Art","n":0.408},"1":{"v":"A practical guide for software developers and development teams, this book features effective and understandable formulas, procedures, and heuristics to help organizations improve their project cost estimates.","n":0.192}}},{"i":156,"$":{"0":{"v":"Semantic annotation and search of cultural-heritage collections: The MultimediaN E-Culture demonstrator","n":0.302},"1":{"v":"In this article we describe a Semantic Web application for semantic annotation and search in large virtual collections of cultural-heritage objects, indexed with multiple vocabularies. During the annotation phase we harvest, enrich and align collection metadata and vocabularies. The semantic-search facilities support keyword-based queries of the graph (currently 20M triples), resulting in semantically grouped result clusters, all representing potential semantic matches of the original query. We show two sample search scenario's. The annotation and search software is open source and is already being used by third parties. All software is based on established Web standards, in particular HTML/XML, CSS, RDF/OWL, SPARQL and JavaScript.","n":0.099}}},{"i":157,"$":{"0":{"v":"Source-code queries with graph databases-with application to programming language usage and evolution","n":0.289},"1":{"v":"Program querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. Secondly there are program analysis tools in the style of Coverity which perform deeper program analysis searching for bugs as well as checking adherence to coding standards such as MISRA.The former class are typically implemented on top of relational or deductive databases and make ad-hoc trade-offs between scalability and the amount of source-code detail held-with consequent limitations on the expressiveness of queries. The latter class are more commercially driven and involve more ad-hoc queries over program representations, nonetheless similar pressures encourage user-visible domain-specific languages to specify analyses.We argue that a graph data model and associated query language provides a unifying conceptual model and gives efficient scalable implementation even when storing full source-code detail. It also supports overlays allowing a query DSL to pose queries at a mixture of syntax-tree, type, control-flow-graph or data-flow levels.We describe a prototype source-code query system built on top of Neo4j using its Cypher graph query language; experiments show it scales to multi-million-line programs while also storing full source-code detail. A scalable source-code querying system which stores full source-code detail.Queries on syntax, types and data flow via overlay relations (database views).Based on the graph data model, with prototype ('Wiggle') using Neo4j.","n":0.063}}},{"i":158,"$":{"0":{"v":"Toward a smart grid: power delivery for the 21st century","n":0.316},"1":{"v":"In this article, we present the security, agility, and robustness/survivability of a large-scale power delivery infrastructure that faces new threats and unanticipated conditions. By way of background, we present a brief overview of the past work on the challenges faced in online parameter estimation and real-time adaptive control of a damaged F-15 aircraft. This work, in part, provided the inspiration and laid the foundation in the 1990s for the flight testing of a fast parameter estimation/modeling and reconfigurable aircraft control system that allowed the F-15 to become self-healing in the face of damaged equipment.","n":0.103}}},{"i":159,"$":{"0":{"v":"Options, Futures, and Other Derivatives","n":0.447},"1":{"v":"Contents: Introduction. Futures Markets and the Use of Futures for Hedging. Forward and Futures Prices. Interest Rate Futures. Swaps. Options Markets. Properties of Stock Option Prices. Trading Strategies Involving Options. Introduction to Binomial Trees. Model of the Behavior of Stock Prices. The Black-Scholes Analysis. Options on Stock Indices, Currencies, and Futures Contracts. General Approach to Pricing Derivatives. The Management of Market Risk. Numerical Procedures. Interest Rate Derivatives and the Use of Black's Model. Interest Rate Derivatives and Models of the Yield Curve. Exotic Options. Alternatives to Black-Scholes for Option Pricing. Credit Risk and Regulatory Capital. Review of Key Concepts.","n":0.101}}},{"i":160,"$":{"0":{"v":"Design Rules: The Power of Modularity","n":0.408}}},{"i":161,"$":{"0":{"v":"Foundations for the study of software architecture","n":0.378},"1":{"v":"The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architecture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements --- that is, the constraints on the elements. The rationale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system requirements. We discuss the components of the model in the context of both architectures and architectural styles and present an extended example to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, summarizing our contributions, and relating our approach to other current work.","n":0.08}}},{"i":162,"$":{"0":{"v":"Option pricing: A simplified approach☆","n":0.447},"1":{"v":"This paper presents a simple discrete-time model for valuing options. The fundamental economic principles of option pricing by arbitrage methods are particularly clear in this setting. Its development requires only elementary mathematics, yet it contains as a special limiting case the celebrated Black-Scholes model, which has previously been derived only by much more difficult methods. The basic model readily lends itself to generalization in many ways. Moreover, by its very construction, it gives rise to a simple and efficient numerical procedure for valuing options for which premature exercise may be optimal.","n":0.105}}},{"i":163,"$":{"0":{"v":"Software Architecture: The Next Step","n":0.447},"1":{"v":"This position paper makes the following claims that, in our opinion, are worthwhile to discuss at the workshop. 1) The first phase of software architecture research, where the key concepts are components and connectors, has matured the technology to a level where industry adoption is wide-spread and few fundamental issues remain. 2) The traditional view on software architecture suffers from a number of key problems that cannot be solved without changing our perspective on the notion of software architecture. These problems include the lack of first-class representation of design decisions, the fact that these design decisions are cross-cutting and intertwined, that these problems lead to high maintenance cost, because of which design rules and constraints are easily violated and obsolete design decisions are not removed. 3) As a community, we need to take the next step and adopt the perspective that a software architecture is, fundamentally, a composition of architectural design decisions. These design decisions should be represented as first-class entities in the software architecture and it should, at least before system deployment, be possible to add, remove and change architectural design decisions against limited effort.","n":0.073}}},{"i":164,"$":{"0":{"v":"The structure and value of modularity in software design","n":0.333},"1":{"v":"The concept of information hiding modularity is a cornerstone of modern software design thought, but its formulation remains casual and its emphasis on changeability is imperfectly related to the goal of creating added value in a given context. We need better explanatory and prescriptive models of the nature and value of information hiding. We evaluate the potential of a new theory---developed to account for the influence of modularity on the evolution of the computer industry---to inform software design. The theory uses design structure matrices to model designs and real options techniques to value them. To test the potential utility of the theory for software we apply it to Parnas's KWIC designs. We contribute an extension to design structure matrices, and we show that the options results are consistent with Parnas's conclusions. Our results suggest that such a theory does have potential to help inform software design.","n":0.083}}},{"i":165,"$":{"0":{"v":"An analysis of modularity in aspect oriented design","n":0.354},"1":{"v":"We present an analysis of modularity in aspect oriented design using the theory of modular design developed by Baldwin and Clark [10]. We use the three major elements of that theory, namely: i) Design Structure Matrix (DSM), an analysis and modeling tool; ii) Modular Operators, units of variations for design evolution; and iii) Net Options Value (NOV), a quantitative approach to evaluate design. We study the design evolution of a Web Services application where we observe the effects of applying aspect oriented modularization.Based on our analysis we get to the following three main conclusions. First, on the structural part, it is possible to apply the DSM to aspect oriented modularizations in a straightforward manner, i.e. without modifications to DSMs basic model. This shows that aspects can, in fact, be treated as modules of design. Second, the evolution of a design into including aspect modules uses the modular operators proposed by Baldwin and Clark, with a variant of the Inversion operator. This variant captures taking redundant, scattered information hidden in modules and moving it down or keeping it at the same level in the design hierarchy. Third, when calculating and comparing NOVs of the different designs of our application, we obtained higher NOV for the design with aspects than for the design without aspects. This shows that, under this theory of modularity, certain aspect oriented modularizations can add value to the design.","n":0.066}}},{"i":166,"$":{"0":{"v":"Quality-Attribute Based Economic Valuation of Architectural Patterns","n":0.378},"1":{"v":"Architectural patterns generate value depending on the utility of the quality attributes that can be achieved from the application of those patterns. However, methods to rigorously evaluate the value-added of patterns do not exist. In this position paper, we make the case that architectural patterns carry economic value in the form of real options, providing designers with the right, but not the obligation, to take subsequent design actions in the future in the face of uncertainty. We summarize our observations in evaluating the relative value of patterns using real option valuation models on a model problem. We draw attention to how such economics-informed approaches can provide belter insights for the selection of situated design strategies.","n":0.093}}},{"i":167,"$":{"0":{"v":"Plastic Partial Components: A solution to support variability in architectural components","n":0.302},"1":{"v":"Software Product Line Engineering is becoming widely used due to the improvement it means when developing software products of the same family. The commonalities and variabilities of Software Product Lines (SPL) are identified during the Domain Engineering process and then, they are realized in the software architecture. Therefore, mechanisms to explicitly specify the commonalities and variabilities of SPLs at the architectural level are required. Most of the current mechanisms specify variations on the architecture by adding or removing architectural elements. However, it is also necessary to specify variations inside components. In this paper, we propose the notion of Plastic Partial Components to support internal variations. The specification of these components is performed using Invasive Software Composition techniques and without tangling the core and product architectures of the SPL. This contribution is illustrated through a SPL for developing domain-specific validation environments.","n":0.085}}},{"i":168,"$":{"0":{"v":"The economic impact of learning and flexibility on process decisions","n":0.316},"1":{"v":"The perception that software projects have unusually high failure rates has fueled the debate on software process. In the 1990s, the Standish Group estimated that the total economic toll of cancelled or overrun software projects could reach several tens of billions of dollars in terms of wasted effort and opportunity costs. Worse yet, even when delivered on time and on budget, software rarely has any significant salvage value when it fails to meet user needs. This article argues for an economic basis for rationalizing process selection decisions. It demonstrates how, under conditions of uncertainty, learning and flexibility affect such decisions. Uncertainty is a critical driver in process selection because it's ubiquitous in software development and it determines the degree of flexibility and learning needed to maximize economic value.","n":0.088}}},{"i":169,"$":{"0":{"v":"Evaluating architectural stability with real options theory","n":0.378},"1":{"v":"Architectural stability refers to the extent to which a software architecture is flexible enough to respond to changes in stakeholders' requirements and the environment. We contribute to a model that exploits options theory to evaluate architectural stability. We describe how we have derived the model: the analogy and assumptions made; its formulation and possible interpretations. We use a refactoring case study to empirically evaluate the model. The results show that the model can provide insights into architectural stability and investment decisions related to the evolution of software systems.","n":0.107}}},{"i":170,"$":{"0":{"v":"Flexible working architectures: agile architecting using PPCs","n":0.378},"1":{"v":"Software systems need software architectures to improve their scalability and maintenance. However, many agile practitioners claim that the upfront design of software architectures is an investment that does not pay off, since customers can rarely appreciate the value delivered by architectures. Furthermore, conventional architectural practices may be considered unacceptable from the Agile values and principles perspective. In this paper, the development of working architectures in agile iterations is presented as an attempt to solve the problem of designing software architectures in Agile. This contribution is based on the new concept of Plastic Partial Component (PPC). PPCs are highly malleable components that can be partially described, what increases the flexibility of architecture design. PPCs based architectures let reinforce some of the agile values and principles. Our experience of putting this contribution into practice is illustrated through the agile development of a Testing Framework for Biogas Plants.","n":0.083}}},{"i":171,"$":{"0":{"v":"Valuation of Software Initiatives Under Uncertainty: Concepts, Issues, and Techniques","n":0.316},"1":{"v":"State of the practice in software engineering economics often focuses exclusively on cost issues and technical considerations for decision making. Value-based software engineering (VBSE) expands the cost focus by also considering benefits, opportunities, and risks. Of central importance in this context is valuation, the process for determining the economic value of a product, service, or a process. Uncertainty is a major challenge in the valuation of software assets and projects. This chapter first introduces uncertainty along with other significant issues and concepts in valuation, and surveys the relevant literature. Then it discusses decision tree and options-based techniques to demonstrate how valuation can help with dynamic decision making under uncertainty in software development projects.","n":0.094}}},{"i":172,"$":{"0":{"v":"Evaluating Flexibility in Embedded Automotive Product Lines Using Real Options","n":0.316},"1":{"v":"Embedded automotive architectures and software need to support a large number of vehicle product lines over many years of production. This leads to a complexity and many uncertain factors when developing such systems and a need for support in the design process. An evaluation method using real options provides the opportunity to analyze the cost of designing for flexibility to cope with a future growth of a product line, based on the estimated value of the future functionality. In this paper real options is applied on a case within the automotive industry. To improve the usability an evaluation process is defined to aid engineers. This process provides a way of valuing system designs and enables the engineer to think about the future in a systematic manor. The value of a flexible design can thereby be quantified and the proposed process shows how it can be accepted by practitioners.","n":0.082}}},{"i":173,"$":{"0":{"v":"Architecture-Centric Methods and Agile Approaches","n":0.447},"1":{"v":"Agile practices have recently gained popularity among large number of companies as a mechanism for reducing cost and increasing ability to handle change in dynamic market conditions. Based on the principles of the Agile manifesto [1, 2], researchers and practitioners have proposed several software development approaches such as Extreme Programming, Scrum and Feature-Driven Development. These and other agile approaches have had significant impact on industrial software development practices. However, there is also a significant concern and perplexity about the role and importance of the issues related to a system’s software architecture, which is considered one of the most important initial design artefacts. It is argued that software architecture is an effective tool to cut development cost and time and to increase the quality of a system. Many practitioners of Agile approaches appear to view software architecture in the context of the plan-driven development paradigm [3]. For them, upfront design and evaluation of software architecture requires too much work, which may have very little value to the customers of a system. Hence, they perceive architectural work as part of high ceremony processes, which usually require large amount of documentation. We maintain that these two seemingly opposing views to software engineering can be integrated but it requires that experts from both fields work together to overcome evident challenges in bridging these two paradigms together. Indeed, software architecture researchers and practitioners appear to believe that sound architectural practices cannot be followed using Agile approaches. However, these two extreme views of Agile and architecture appear to neglect that many agile experts emphasises the importance of paying attention to good design and architecture early in the development process [4, 5]. Recently, there is growing recognition of the importance of paying more attention to architectural aspects in agile approaches [3, 6, 7]. We argue that there is a vital need for devising a research agenda for identifying and dealing with architecture-centric challenges in agile software development. Such research agenda should make it possible to guide the future research on integrating architecture-centric methods in agile approaches and give advice to the software industry on dealing with architecture related challenges. Some of the questions to stimulate discussion in the workshop are:","n":0.052}}},{"i":174,"$":{"0":{"v":"Genetic Algorithms + Data Structures = Evolution Programs","n":0.354},"1":{"v":"1 GAs: What Are They?.- 2 GAs: How Do They Work?.- 3 GAs: Why Do They Work?.- 4 GAs: Selected Topics.- 5 Binary or Float?.- 6 Fine Local Tuning.- 7 Handling Constraints.- 8 Evolution Strategies and Other Methods.- 9 The Transportation Problem.- 10 The Traveling Salesman Problem.- 11 Evolution Programs for Various Discrete Problems.- 12 Machine Learning.- 13 Evolutionary Programming and Genetic Programming.- 14 A Hierarchy of Evolution Programs.- 15 Evolution Programs and Heuristics.- 16 Conclusions.- Appendix A.- Appendix B.- Appendix C.- Appendix D.- References.","n":0.108}}},{"i":175,"$":{"0":{"v":"A practical guide for using statistical tests to assess randomized algorithms in software engineering","n":0.267},"1":{"v":"Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for.   This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering","n":0.067}}},{"i":176,"$":{"0":{"v":"A comparison of issues and advantages in agile and incremental development between state of the art and an industrial case","n":0.224},"1":{"v":"Recent empirical studies have been conducted identifying a number of issues and advantages of incremental and agile methods. However, the majority of studies focused on one model (Extreme Programming) and small projects. To draw more general conclusions we conduct a case study in large-scale development identifying issues and advantages, and compare the results with previous empirical studies on the topic. The principle results are that (1) the case study and literature agree on the benefits while new issues arise when using agile in large-scale and (2) an empirical research framework is needed to make agile studies comparable.","n":0.102}}},{"i":177,"$":{"0":{"v":"Exploring the costs of technical debt management --- a case study","n":0.302},"1":{"v":"Technical debt is a metaphor for delayed software maintenance tasks. Incurring technical debt may bring short-term benefits to a project, but such benefits are often achieved at the cost of extra work in future, analogous to paying interest on the debt. Currently technical debt is managed implicitly, if at all. However, on large systems, it is too easy to lose track of delayed tasks or to misunderstand their impact. Therefore, we have proposed a new approach to managing technical debt, which we believe to be helpful for software managers to make informed decisions. In this study we explored the costs of the new approach by tracking the technical debt management activities in an on-going software project. The results from the study provided insights into the impact of technical debt management on software projects. In particular, we found that there is a significant start-up cost when beginning to track and monitor technical debt, but the cost of ongoing management soon declines to very reasonable levels.","n":0.078}}},{"i":178,"$":{"0":{"v":"Refactoring-a Shot in the Dark?","n":0.447},"1":{"v":"A study performed semistructured interviews of 12 seasoned software architects and developers at nine Finnish companies. Its main goals were to find out how the practitioners viewed the role and importance of refactoring, and how and when they refactored. Another goal was to see whether shortened cycle times and, especially, continuous-deployment practices affected how and when refactoring was done. The results paint a multifaceted picture with some common patterns. The respondents considered refactoring to be valuable but had difficulty explaining and justifying it to management and customers. Refactoring often occurred in conjunction with the development of new features because it seemed to require a clear business need. The respondents didn't use measurements to quantify the need for or impact of refactoring. This article is part of a special issue on Refactoring.","n":0.087}}},{"i":179,"$":{"0":{"v":"Maximizing Product Value: Continuous Maintenance","n":0.447},"1":{"v":"A frequent software related claim is that the initial development costs are 30% and that 70% more is needed in maintenance. However we claim that in today’s software industry, software maintenance and the development of new features are intimately tangled, and it is impossible to separate them in a reliable fashion. We demonstrate this by showing how some modern software engineering approaches address maintenance and the development of new features, and we describe a concept of continuous maintenance to manage frequent changes both in software and business.","n":0.107}}},{"i":180,"$":{"0":{"v":"Tool Support for Planning Global Software Development Projects","n":0.354},"1":{"v":"Planning global software development (GSD) projects is a challenging task, as it involves balancing both technical and business related issues. While planning GSD projects, project managers face decision-making situations such as, choosing the right site to distribute work and finding an optimal work distribution considering both the cost and duration of the project. Finding an optimal solution for these decision-making situations is a difficult task without some kind of automated support, as there are many possible alternative work allocation solutions and each solution affects the cost and duration of project differently. To assist project managers in these situations, we propose a tool for planning GSD projects. The tool uses multi-objective genetic algorithms for finding optimal work allocation solutions in a trade off between cost and time. This article discusses the implementation of the tool and application of the tool using two scenarios.","n":0.084}}},{"i":181,"$":{"0":{"v":"Cytoscape 2.8","n":0.707},"1":{"v":"Summary: Cytoscape is a popular bioinformatics package for biological network visualization and data integration. Version 2.8 introduces two powerful new features—Custom Node Graphics and Attribute Equations—which can be used jointly to greatly enhance Cytoscape's data integration and visualization capabilities. Custom Node Graphics allow an image to be projected onto a node, including images generated dynamically or at remote locations. Attribute Equations provide Cytoscape with spreadsheet-like functionality in which the value of an attribute is computed dynamically as a function of other attributes and network properties.\r\n\r\nAvailability and implementation: Cytoscape is a desktop Java application released under the Library Gnu Public License (LGPL). Binary install bundles and source code for Cytoscape 2.8 are available for download from http://cytoscape.org.\r\n\r\nContact: [email protected]","n":0.092}}},{"i":182,"$":{"0":{"v":"An algorithm for drawing general undirected graphs","n":0.378}}},{"i":183,"$":{"0":{"v":"Evaluating and Mitigating Software Supply Chain Security Risks","n":0.354},"1":{"v":"Abstract : The Department of Defense (DoD) is concerned that security vulnerabilities could be inserted into software that has been developed outside of the DoD's supervision or control. This report presents an initial analysis of how to evaluate and mitigate the risk that such unauthorized insertions have been made. The analysis is structured in terms of actions that should be taken in each phase of the DoD acquisition life cycle","n":0.12}}},{"i":184,"$":{"0":{"v":"Software Supply Chain Risk Management: From Products to Systems of Systems","n":0.302},"1":{"v":"Abstract : Supply chains are usually thought of as manufacturing and delivering physical items, but there are also supply chains associated with the development and operation of a software system. Software supply chain research does not have decades of evidence to draw on, as with physical-item supply chains. Taking a systems perspective on software supply chain risks, this report considers current practices in software supply chain analysis and suggests some foundational practices. The product and supplier selection criteria for system development depend on how a product is used in a system. While many of the criteria for the selection of product suppliers and system development contractors are the same, there is also a significant difference between these kinds of acquisitions. Product development is completed in advance of an acquirer?s product and supplier assessment. There is no guarantee that current supplier development practices were used for a specific product. For custom system acquisitions, acquirers can and should actively monitor both contractor and product supply chain risks during development. This report suggests contractor and acquirer activities that support the management of supply chain risks.","n":0.074}}},{"i":185,"$":{"0":{"v":"Technical debt aggregation in ecosystems","n":0.447},"1":{"v":"The members of the ecosystem encompassing our organization are affected by our decisions just as we are affected by their decisions. If an organization takes on technical debt with respect to a specific asset, that decision will affect users of the asset either directly or indirectly. In this position paper we distinguish between incurring technical debt directly and experiencing the effects of technical debt indirectly. We illustrate why two separate concepts are needed for a complete theory and provide examples from ecosystem models we have created for several organizations. The result is a model that produces good explanations for posited scenarios.","n":0.1}}},{"i":186,"$":{"0":{"v":"It Takes an Ecosystem","n":0.5}}},{"i":187,"$":{"0":{"v":"Enhancing Digital Business Ecosystem trust and reputation with centrality measures","n":0.316},"1":{"v":"Digital Business Ecosystem (DBE) is a decentralised environment where very small enterprises (VSEs) and small to medium sized enterprises (SMEs) interoperate by establishing collaborations with each other. Collaborations play a major role in the development of DBEs where it is often difficult to select partners, as they are most likely strangers. Even though trust forms the basis for collaboration decisions, trust and reputation information may not be available for each participant. Recommendations from other participants are therefore necessary to help with the selection process. Given the nature of DBEs, social network centrality measures that can influence power and control in the network need to be considered for DBE trust and reputation. A number of social network centralities, which influence reputation in social graphs have been studied in the past. This paper investigates an unexploited centrality measure, betweenness centrality, as a metric to be considered for trust and reputation.","n":0.082}}},{"i":188,"$":{"0":{"v":"Exploring the Structure of Complex Software Designs: An Empirical Study of Open Source and Proprietary Code","n":0.25},"1":{"v":"This paper reports data from a study that seeks to characterize the differences in design structure between complex software products. We use design structure matrices (DSMs) to map dependencies between the elements of a design and define metrics that allow us to compare the structures of different designs. We use these metrics to compare the architectures of two software products---the Linux operating system and the Mozilla Web browser---that were developed via contrasting modes of organization: specifically, open source versus proprietary development. We then track the evolution of Mozilla, paying attention to a purposeful “redesign” effort undertaken with the intention of making the product more “modular.” We find significant differences in structure between Linux and the first version of Mozilla, suggesting that Linux had a more modular architecture. Yet we also find that the redesign of Mozilla resulted in an architecture that was significantly more modular than that of its predecessor and, indeed, than that of Linux. Our results, while exploratory, are consistent with a view that different modes of organization are associated with designs that possess different structures. However, they also suggest that purposeful managerial actions can have a significant impact in adapting a design's structure. This latter result is important given recent moves to release proprietary software into the public domain. These moves are likely to fail unless the product possesses an “architecture for participation.”","n":0.067}}},{"i":189,"$":{"0":{"v":"Decoupling level: a new metric for architectural maintenance complexity","n":0.333},"1":{"v":"Despite decades of research on software metrics, we still cannot reliably measure if one design is more maintainable than another. Software managers and architects need to understand whether their software architecture is \"good enough\", whether it is decaying over time and, if so, by how much. In this paper, we contribute a new architecture maintainability metric---Decoupling Level (DL)---derived from Baldwin and Clark's option theory. Instead of measuring how coupled an architecture is, we measure how well the software can be decoupled into small and independently replaceable modules. We measured the DL for 108 open source projects and 21 industrial projects, each of which has multiple releases. Our main result shows that the larger the DL, the better the architecture. By \"better\" we mean: the more likely bugs and changes can be localized and separated, and the more likely that developers can make changes independently. The DL metric also opens the possibility of quantifying canonical principles of single responsibility and separation of concerns, aiding cross-project comparison and architecture decay monitoring.","n":0.077}}},{"i":190,"$":{"0":{"v":"Measuring architecture quality by structure plus history analysis","n":0.354},"1":{"v":"This case study combines known software structure and revision history analysis techniques, in known and new ways, to predict bug-related change frequency, and uncover architecture-related risks in an agile industrial software development project. We applied a suite of structure and history measures and statistically analyzed the correlations between them. We detected architecture issues by identifying outliers in the distributions of measured values and investigating the architectural significance of the associated classes. We used a clustering method to identify sets of files that often change together without being structurally close together, investigating whether architecture issues were among the root causes. The development team confirmed that the identified clusters reflected significant architectural violations, unstable key interfaces, and important undocumented assumptions shared between modules. The combined structure diagrams and history data justified a refactoring proposal that was accepted by the project manager and implemented.","n":0.084}}},{"i":191,"$":{"0":{"v":"Keynote address - data abstraction and hierarchy","n":0.378}}},{"i":192,"$":{"0":{"v":"Experiences applying automated architecture analysis tool suites","n":0.378},"1":{"v":"In this paper, we report our experiences of applying three complementary automated software architecture analysis techniques, supported by a tool suite, called DV8, to 8 industrial projects within a large company. DV8 includes two state-of-the-art architecture-level maintainability metrics—Decoupling Level and Propagation Cost, an architecture flaw detection tool, and an architecture root detection tool. We collected development process data from the project teams as input to these tools, reported the results back to the practitioners, and followed up with telephone conferences and interviews. Our experiences revealed that the metrics scores, quantitative debt analysis, and architecture flaw visualization can effectively bridge the gap between management and development, help them decide if, when, and where to refactor. In particular, the metrics scores, compared against industrial benchmarks, faithfully reflected the practitioners' intuitions about the maintainability of their projects, and enabled them to better understand the maintainability relative to other projects internal to their company, and to other industrial products. The automatically detected architecture flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify the “hotspots” within the systems that are responsible for high maintenance costs. Except for the two smallest projects for which both architecture metrics indicated high maintainability, all other projects are planning or have already begun refactorings to address the problems detected by our analyses. We are working on further automating the tool chain, and transforming the analysis suite into deployable services accessible by all projects within the company.","n":0.065}}},{"i":193,"$":{"0":{"v":"A longitudinal study of identifying and paying down architecture debt","n":0.316},"1":{"v":"Architecture debt is a form of technical debt that derives from the gap between the intended and the actual architecture design. In this study we measured architecture debt in two ways: 1) in terms of system-wide coupling measures, and 2) in terms of the number and severity of architecture flaws. In recent research it was shown that the amount of architecture debt has a huge impact on software maintainability and evolution. Consequently, reducing debt is expected to make software less costly and more amenable to change. This paper reports on a longitudinal study of a healthcare communications product created by BrightSquid Secure Communications Corp. This young company is facing the typical trade-off problem of desiring responsiveness to change requests, but wanting to avoid the ever-increasing effort that the accumulation of quick-and-dirty changes eventually incurs. In the first stage of the study, we analyzed the status of the \"before\" system, which showed the impacts of change requests. This initial study motivated a more in-depth analysis of architecture debt. The results of this debt analysis were used in the second stage of the work to motivate a comprehensive refactoring of the software system. The third stage was a follow-on architecture debt analysis which quantified the improvements realized. Using this quantitative evidence, augmented by qualitative evidence gathered from in-depth interviews with BrightSquid's architects, we present lessons learned about the costs and benefits of paying down architecture debt in practice.","n":0.065}}},{"i":194,"$":{"0":{"v":"Software Architecture Measurement—Experiences from a Multinational Company","n":0.378},"1":{"v":"In this paper, we present our 4-year experience of creating, evolving, and validating an automated software architecture measurement system within Huawei. This system is centered around a comprehensive scale called the Standard Architecture Index (SAI), which is composed of a number of measures, each reflecting a recurring architecture problem. Development teams use this as a guide to figure out how to achieve a better score by addressing the underlying problems. The measurement practice thus motivates desired behaviors and outcomes. In this paper, we present our experience of creating and validating SAI 1.0 and 2.0, which has been adopted as the enterprise-wide standard, and our directions towards SAI 3.0. We will describe how we got the development teams to accept and apply SAI through pilot studies, constantly adjusting the formula based on feedback, and correlating SAI scores with productivity measures. Our experience shows that it is critical to guide development teams to focus on the underlying problems behind each measure within SAI, rather than on the score itself. It is also critical to introduce state-of-the-art technologies to the development teams. In doing so they can leverage these technologies to pinpoint and quantify architecture problems so that better SAI scores can be achieved, along with better quality and productivity.","n":0.07}}},{"i":195,"$":{"0":{"v":"The financial aspect of managing technical debt","n":0.378},"1":{"v":"ContextTechnical debt is a software engineering metaphor, referring to the eventual financial consequences of trade-offs between shrinking product time to market and poorly specifying, or implementing a software product, throughout all development phases. Based on its inter-disciplinary nature, i.e. software engineering and economics, research on managing technical debt should be balanced between software engineering and economic theories. ObjectiveThe aim of this study is to analyze research efforts on technical debt, by focusing on their financial aspect. Specifically, the analysis is carried out with respect to: (a) how financial aspects are defined in the context of technical debt and (b) how they relate to the underlying software engineering concepts. MethodIn order to achieve the abovementioned goals, we employed a standard method for SLRs and applied it on studies retrieved from seven general-scope digital libraries. In total we selected 69 studies relevant to the financial aspect of technical debt. ResultsThe most common financial terms that are used in technical debt research are principal and interest, whereas the financial approaches that have been more frequently applied for managing technical debt are real options, portfolio management, cost/benefit analysis and value-based analysis. However, the application of such approaches lacks consistency, i.e., the same approach is differently applied in different studies, and in some cases lacks a clear mapping between financial and software engineering concepts. ConclusionThe results are expected to prove beneficial for the communication between technical managers and project managers, in the sense that they will provide a common vocabulary, and will help in setting up quality-related goals, during software development. To achieve this we introduce: (a) a glossary of terms and (b) a classification scheme for financial approaches used for managing technical debt. Based on these, we have been able to underline interesting implications for researchers and practitioners.","n":0.058}}},{"i":196,"$":{"0":{"v":"How do software development teams manage technical debt? - An empirical study","n":0.289},"1":{"v":"Exploratory case study with empirical data from eight software development teams.Observation of various different strategies for technical debt management.Developed technical debt management framework. Technical debt (TD) is a metaphor for taking shortcuts or workarounds in technical decisions to gain short-term benefit in time-to-market and earlier software release. In this study, one large software development organization is investigated to gather empirical evidence related to the concept of technical debt management (TDM). We used the exploratory case study method to collect and analyze empirical data in the case organization by interviewing a total of 25 persons in eight software development teams. We were able to identify teams where the current strategy for TDM was only to fix TD when necessary, when it started to cause too much trouble for development. We also identified teams where the management had a systematic strategy to identify, measure and monitor TD during the development process. It seems that TDM can be associated with a similar maturity concept as software development in general. Development teams may raise their maturity by increasing their awareness and applying more advanced processes, techniques and tools in TDM. TDM is an essential part of sustainable software development, and companies have to find right approaches to deal with TD to produce healthy software that can be developed and maintained in the future.","n":0.068}}},{"i":197,"$":{"0":{"v":"Managing technical debt: an industrial case study","n":0.378},"1":{"v":"Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management's high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.","n":0.069}}},{"i":198,"$":{"0":{"v":"The Introduction of Technical Debt Tracking in Large Companies","n":0.333},"1":{"v":"Large software companies need to support continuous and fast delivery of customer value both in the short and long term. However, this can be hindered if both evolution and maintenance of existing systems are hampered by Technical Debt. Although a lot of theoretical work on Technical Debt has been recently produced, its practical management lacks empirical studies. In this paper we investigate the state of practice in several companies in order to understand how they start tracking Technical Debt. We combined different methodologies: we conducted a survey, involving 226 respondents from 15 organizations and a more in-depth multiple case-study in three organizations, where Technical Debt was tracked: we involved 13 interviews and 79 Technical Debt issues analysis. We found that the development time dedicated to manage Technical Debt is substantial (around 25% of the overall development) but not systematic: only a few participants methodically track Technical Debt. By studying the approaches in the companies participating in the case-study, we understood how companies start tracking Technical Debt and what are the initial benefits and challenges. Finally, we propose a Strategic Adoption Model based to define and adopt a dedicated process for tracking Technical Debt","n":0.072}}},{"i":199,"$":{"0":{"v":"Software fault prediction metrics","n":0.5},"1":{"v":"ContextSoftware metrics may be used in fault prediction models to improve software quality by predicting fault location. ObjectiveThis paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics' selection and performance. MethodThis systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties. ResultsObject-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer's (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics. ConclusionMore studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.","n":0.074}}},{"i":200,"$":{"0":{"v":"Software Metrics: A Rigorous and Practical Approach, Third Edition","n":0.333},"1":{"v":"A Framework for Managing, Measuring, and Predicting Attributes of Software Development Products and Processes Reflecting the immense progress in the development and use of software metrics in the past decades, Software Metrics: A Rigorous and Practical Approach, Third Edition provides an up-to-date, accessible, and comprehensive introduction to software metrics. Like its popular predecessors, this third edition discusses important issues, explains essential concepts, and offers new approaches for tackling long-standing problems. New to the Third Edition This edition contains new material relevant to object-oriented design, design patterns, model-driven development, and agile development processes. It includes a new chapter on causal models and Bayesian networks and their application to software engineering. This edition also incorporates recent references to the latest software metrics activities, including research results, industrial case studies, and standards. Suitable for a Range of Readers With numerous examples and exercises, this book continues to serve a wide audience. It can be used as a textbook for a software metrics and quality assurance course or as a useful supplement in any software engineering course. Practitioners will appreciate the important results that have previously only appeared in research-oriented publications. Researchers will welcome the material on new results as well as the extensive bibliography of measurement-related information. The book also gives software managers and developers practical guidelines for selecting metrics and planning their use in a measurement program.","n":0.067}}},{"i":201,"$":{"0":{"v":"A Balancing Act: What Software Practitioners Have to Say about Technical Debt","n":0.289},"1":{"v":"An interview study involving 35 practitioners from a variety of domains aimed to characterize technical debt at the ground level to find out how software practitioners perceive it. The study also aimed to understand the context in which technical debt occurs, including its causes, symptoms, and effects. In addition, the study focused on how practitioners currently deal with technical debt. This analysis paints a picture of a large, complex balancing act of various short- and long-term concerns. The Web Extra gives the interview questions used by Erin Lim, Nitin Taksande, and Carolyn Seaman.","n":0.104}}},{"i":202,"$":{"0":{"v":"A systematic review of software maintainability prediction and metrics","n":0.333},"1":{"v":"This paper presents the results of a systematic review conducted to collect evidence on software maintainability prediction and metrics. The study was targeted at the software quality attribute of maintainability as opposed to the process of software maintenance. The evidence was gathered from the selected studies against a set of meaningful and focused questions. 710 studies were initially retrieved; however of these only 15 studies were selected; their quality was assessed; data extraction was performed; and data was synthesized against the research questions. Our results suggest that there is little evidence on the effectiveness of software maintainability prediction techniques and models.","n":0.1}}},{"i":203,"$":{"0":{"v":"Hidden Structure: Using Network Methods to Map System Architecture","n":0.333},"1":{"v":"In this paper, we describe an operational methodology for characterizing the architecture of complex technical systems and demonstrate its application to a large sample of software releases. Our methodology is based upon directed network graphs, which allows us to identify all of the direct and indirect linkages between the components in a system. We use this approach to define three fundamental architectural patterns, which we label core–periphery, multi-core, and hierarchical. Applying our methodology to a sample of 1286 software releases from 17 applications, we find that the majority of releases possess a “core–periphery” structure. This architecture is characterized by a single dominant cyclic group of components (the “Core”) that is large relative to the system as a whole as well as to other cyclic groups in the system. We show that the size of the Core varies widely, even for systems that perform the same function. These differences appear to be associated with different models of development – open, distributed organizations develop systems with smaller Cores, while closed, co-located organizations develop systems with larger Cores. Our findings establish some “stylized facts” about the fine-grained structure of large, real-world technical systems, serving as a point of departure for future empirical work.","n":0.071}}},{"i":204,"$":{"0":{"v":"Empirical evidence on the link between object-oriented measures and external quality attributes: a systematic literature review","n":0.25},"1":{"v":"There is a plethora of studies investigating object-oriented measures and their link with external quality attributes, but usefulness of the measures may differ across empirical studies. This study aims to aggregate and identify useful object-oriented measures, specifically those obtainable from the source code of object-oriented systems that have gone through such empirical evaluation. By conducting a systematic literature review, 99 primary studies were identified and traced to four external quality attributes: reliability, maintainability, effectiveness and functionality. A vote-counting approach was used to investigate the link between object-oriented measures and the attributes, and to also assess the consistency of the relation reported across empirical studies. Most of the studies investigate links between object-oriented measures and proxies for reliability attributes, followed by proxies for maintainability. The least investigated attributes were: effectiveness and functionality. Measures from the CK their link with reliability and maintainability could be context dependent. There were too few studies traced to effectiveness and functionality attributes; thus a meaningful vote-counting analysis could not be conducted for these attributes. Thus, there is a need for diversification of quality attributes investigated in empirical studies. This would help with identifying useful measures during quality assessment initiatives, and not just for reliability and maintainability aspects.","n":0.071}}},{"i":205,"$":{"0":{"v":"Evaluating the Quality of Open Source Software","n":0.378},"1":{"v":"Traditionally, research on quality attributes was either kept under wraps within the organization that performed it, or carried out by outsiders using narrow, black-box techniques. The emergence of open source software has changed this picture allowing us to evaluate both software products and the processes that yield them. Thus, the software source code and the associated data stored in the version control system, the bug tracking databases, the mailing lists, and the wikis allow us to evaluate quality in a transparent way. Even better, the large number of (often competing) open source projects makes it possible to contrast the quality of comparable systems serving the same domain. Furthermore, by combining historical source code snapshots with significant events, such as bug discoveries and fixes, we can further dig into the causes and effects of problems. Here we present motivating examples, tools, and techniques that can be used to evaluate the quality of open source (and by extension also proprietary) software.","n":0.079}}},{"i":206,"$":{"0":{"v":"Calculation and optimization of thresholds for sets of software metrics","n":0.316},"1":{"v":"In this article, we present a novel algorithmic method for the calculation of thresholds for a metric set. To this aim, machine learning and data mining techniques are utilized. We define a data-driven methodology that can be used for efficiency optimization of existing metric sets, for the simplification of complex classification models, and for the calculation of thresholds for a metric set in an environment where no metric set yet exists. The methodology is independent of the metric set and therefore also independent of any language, paradigm or abstraction level. In four case studies performed on large-scale open-source software metric sets for C functions, C+?+, C# methods and Java classes are optimized and the methodology is validated.","n":0.092}}},{"i":207,"$":{"0":{"v":"GitHub Projects. Quality Analysis of Open-Source Software","n":0.378},"1":{"v":"Nowadays Open-Source Software is developed mostly by decentralized teams of developers cooperating on-line. GitHub portal is an online social network that supports development of software by virtual teams of programmers. Since there is no central mechanism that governs the process of team formation, it is interesting to investigate if there are any significant correlations between project quality and the characteristics of the team members. However, for such analysis to be possible, we need good metrics of a project quality. This paper develops two such metrics, first one reflecting project’s popularity, and the second one - the quality of support offered by team members to users. The first metric is based on the number of ‘stars’ a project is given by other GitHub members, the second is obtained using survival analysis techniques applied to issues reported on the project by its users. After developing the metrics we have gathered characteristics of several GitHub projects and analyzed their influence on the project quality using statistical regression techniques.","n":0.078}}},{"i":208,"$":{"0":{"v":"System design and the cost of architectural complexity","n":0.354},"1":{"v":"Thesis (Ph. D.)--Massachusetts Institute of Technology, Engineering Systems Division, 2013.","n":0.316}}},{"i":209,"$":{"0":{"v":"Software Product Quality Models","n":0.5},"1":{"v":"Both for software developers and managers it is crucial to have information about different aspects of the quality of their systems. This chapter gives a brief overview about the history of software product quality measurement, focusing on software maintainability, and the existing approaches and high-level models for characterizing software product quality. The most widely accepted and used practical maintainability models and the state-of-the-art works in the subject are introduced. These models play a very important role in software evolution by allowing to estimate future development costs, assess risks, or support management decisions. Based on objective aspects, the implementations of the most popular software maintainability models are compared and evaluated. The evaluation includes the Quality Index, SQALE, SQUALE, SIG, QUAMOCO, and Columbus Quality Model. The chapter presents the result of comparing the features and stability of the tools and the different models on a large number of open-source Java projects.","n":0.082}}},{"i":210,"$":{"0":{"v":"The application of software maintainability models in industrial software systems","n":0.316},"1":{"v":"Abstract   The impact of software maintainability has become one of the most important aspects of past, present, and future software systems. Tools and models that can measure software maintainability will play an increasingly important role in the software industry. This article reviews two early attempts at software maintainability assessment and describes five recently developed models. Two of these are then applied to industrial software systems, and the results are evaluated. Each of the two models are shown to be effective in evaluating industrial software systems at the component, subsystem, and system levels.","n":0.104}}},{"i":211,"$":{"0":{"v":"Deriving thresholds of software metrics to predict faults on open source software","n":0.289},"1":{"v":"We empirically examined if there are effective thresholds for software metrics.Open-source software systems were used as benchmarking datasets.The learner model was created using logistic regression and the Bender method.Experimental results revealed that some metrics have effective threshold values. Object-oriented metrics aim to exhibit the quality of source code and give insight to it quantitatively. Each metric assesses the code from a different aspect. There is a relationship between the quality level and the risk level of source code. The objective of this paper is to empirically examine whether or not there are effective threshold values for source code metrics. It is targeted to derive generalized thresholds that can be used in different software systems. The relationship between metric thresholds and fault-proneness was investigated empirically in this study by using ten open-source software systems. Three types of fault-proneness were defined for the software modules: non-fault-prone, more-than-one-fault-prone, and more-than-three-fault-prone. Two independent case studies were carried out to derive two different threshold values. A single set was created by merging ten datasets and was used as training data by the model. The learner model was created using logistic regression and the Bender method. Results revealed that some metrics have threshold effects. Seven metrics gave satisfactory results in the first case study. In the second case study, eleven metrics gave satisfactory results. This study makes contributions primarily for use by software developers and testers. Software developers can see classes or modules that require revising; this, consequently, contributes to an increment in quality for these modules and a decrement in their risk level. Testers can identify modules that need more testing effort and can prioritize modules according to their risk levels.","n":0.06}}},{"i":212,"$":{"0":{"v":"Software metrics for measuring the understandability of architectural structures: a systematic mapping study","n":0.277},"1":{"v":"The main idea of software architecture is to concentrate on the \"big picture\" of a software system. In the context of object-oriented software systems higher-level architectural structures or views above the level of classes are frequently used to capture the \"big picture\" of the system. One of the critical aspects of these higher-level views is understandability, as one of their main purposes is to enable designers to abstract away fine-grained details. In this article we present a systematic mapping study on software metrics related to the understandability concepts of such higher-level software structures with regard to their relations to the system implementation. In our systematic mapping study, we started from 3951 studies obtained using an electronic search in the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our inclusion/exclusion criteria as well as the snowballing technique we selected 268 studies for in-depth study. From those, we selected 25 studies that contain relevant metrics. We classify the identified studies and metrics with regard to the measured artefacts, attributes, quality characteristics, and representation model used for the metrics definitions. Additionally, we present the assessment of the maturity level of the identified studies. Overall, there is a lack of maturity in the studies. We discuss possible techniques how to mitigate the identified problems. From the academic point of view we believe that our study is a good starting point for future studies aiming at improving the existing works. From a practitioner's point of view, the results of our study can be used as a catalogue and an indication of the maturity of the existing research results.","n":0.061}}},{"i":213,"$":{"0":{"v":"An Empirical Study of the Impact of Two Antipatterns, Blob and Spaghetti Code, on Program Comprehension","n":0.25},"1":{"v":"Antipatterns are \"poor\" solutions to recurring design problems which are conjectured in the literature to make object-oriented systems harder to maintain. However, little quantitative evidence exists to support this conjecture. We performed an empirical study to investigate whether the occurrence of antipatterns does indeed affect the understandability of systems by developers during comprehension and maintenance tasks. We designed and conducted three experiments, with 24 subjects each, to collect data on the performance of developers on basic tasks related to program comprehension and assessed the impact of two antipatterns and of their combinations: Blob and Spaghetti Code. We measured the developers’ performance with: (1) the NASA task load index for their effort, (2) the time that they spent performing their tasks, and, (3) their percentages of correct answers. Collected data show that the occurrence of one antipattern does not significantly decrease developers’ performance while the combination of two antipatterns impedes significantly developers. We conclude that developers can cope with one antipattern but that combinations of antipatterns should be avoided possibly through detection and refactorings.","n":0.076}}},{"i":214,"$":{"0":{"v":"Software Engineering: A Practitioner's Approach","n":0.447},"1":{"v":"From the Publisher:\r\nWell-suited for both the student and the working professional,Software Engineering A Practitioner's Approach recognizes the dramatic growth in the field of software engineering and emphasizes new and important methods and tools used in the industry.","n":0.164}}},{"i":215,"$":{"0":{"v":"A survey of software refactoring","n":0.447},"1":{"v":"We provide an extensive overview of existing research in the field of software refactoring. This research is compared and discussed based on a number of different criteria: the refactoring activities that are supported, the specific techniques and formalisms that are used for supporting these activities, the types of software artifacts that are being refactored, the important issues that need to be taken into account when building refactoring tool support, and the effect of refactoring on the software process. A running example is used to explain and illustrate the main concepts.","n":0.105}}},{"i":216,"$":{"0":{"v":"Mining Version Histories for Detecting Code Smells","n":0.378},"1":{"v":"Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase change- and fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change over time. In this paper, we propose  H  istorical  I nformation for  S mell de T ection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system’s snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers’ perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.","n":0.066}}},{"i":217,"$":{"0":{"v":"Are all code smells harmful? A study of God Classes and Brain Classes in the evolution of three open source systems","n":0.218},"1":{"v":"Code smells are particular patterns in object-oriented systems that are perceived to lead to difficulties in the maintenance of such systems. It is held that to improve maintainability, code smells should be eliminated by refactoring. It is claimed that classes that are involved in certain code smells are liable to be changed more frequently and have more defects than other classes in the code. We investigated the extent to which this claim is true for God Classes and Brain Classes, with and without normalizing the effects with respect to the class size. We analyzed historical data from 7 to 10 years of the development of three open-source software systems. The results show that God and Brain Classes were changed more frequently and contained more defects than other kinds of class. However, when we normalized the measured effects with respect to size, then God and Brain Classes were less subject to change and had fewer defects than other classes. Hence, under the assumption that God and Brain Classes contain on average as much functionality per line of code as other classes, the presence of God and Brain Classes is not necessarily harmful; in fact, such classes may be an efficient way of organizing code.","n":0.07}}},{"i":218,"$":{"0":{"v":"Yesterday's Weather: guiding early reverse engineering efforts by summarizing the evolution of changes","n":0.277},"1":{"v":"Knowing where to start reverse engineering a large software system, when no information other than the system's source code itself is available, is a daunting task. Having the history of the code (i.e., the versions) could be of help if this would not imply analyzing a huge amount of data. We present an approach for identifying candidate classes for reverse engineering and reengineering efforts. Our solution is based on summarizing the changes in the evolution of object-oriented software systems by defining history measurements. Our approach, named Yesterday's Weather, is an analysis based on the retrospective empirical observation that classes which changed the most in the recent past also suffer important changes in the near future. We apply this approach on two case studies and show how we can obtain an overview of the evolution of a system and pinpoint its classes that might change in the next versions.","n":0.082}}},{"i":219,"$":{"0":{"v":"An approach to prioritize code smells for refactoring","n":0.354},"1":{"v":"Code smells are a popular mechanism to find structural design problems in software systems. Consequently, several tools have emerged to support the detection of code smells. However, the number of smells returned by current tools usually exceeds the amount of problems that the developer can deal with, particularly when the effort available for performing refactorings is limited. Moreover, not all the code smells are equally relevant to the goals of the system or its health. This article presents a semi-automated approach that helps developers focus on the most critical problems of the system. We have developed a tool that suggests a ranking of code smells, based on a combination of three criteria, namely: past component modifications, important modifiability scenarios for the system, and relevance of the kind of smell. These criteria are complementary and enable our approach to assess the smells from different perspectives. Our approach has been evaluated in two case-studies, and the results show that the suggested code smells are useful to developers.","n":0.078}}},{"i":220,"$":{"0":{"v":"Prioritizing code-smells correction tasks using chemical reaction optimization","n":0.354},"1":{"v":"The presence of code-smells increases significantly the cost of maintenance of systems and makes them difficult to change and evolve. To remove code-smells, refactoring operations are used to improve the design of a system by changing its internal structure without altering the external behavior. In large-scale systems, the number of code-smells to fix can be very large and not all of them can be fixed automatically. Thus, the prioritization of the list of code-smells is required based on different criteria such as the risk and importance of classes. However, most of the existing refactoring approaches treat the code-smells to fix with the same importance. In this paper, we propose an approach based on a chemical reaction optimization metaheuristic search to find the suitable refactoring solutions (i.e., sequence of refactoring operations) that maximize the number of fixed riskiest code-smells according to the maintainer's preferences/criteria. We evaluate our approach on five medium- and large-sized open-source systems and seven types of code-smells. Our experimental results show the effectiveness of our approach compared to other existing approaches and three different others metaheuristic searches.","n":0.075}}},{"i":221,"$":{"0":{"v":"Ranking Refactoring Suggestions Based on Historical Volatility","n":0.378},"1":{"v":"The widespread acceptance of refactorings as a simple yet effective approach to improve the design of object-oriented systems, has stimulated an effort to develop semi-automatic tools for detecting design flaws, with simultaneous suggestions for their removal. However, even in medium-sized projects the number of detected occurrences can be so large that the refactoring process becomes intractable for the designer. It is reasonable to expect that some of the suggested refactorings will have a significant effect on the improvement of maintainability while others might be less important. This implies that the suggested solutions can be ranked according to one or more criteria. In this paper we propose the exploitation of past source code versions in order to rank refactoring suggestions according to the number, proximity and extent of changes related with the corresponding code smells. The underlying philosophy is that code fragments which have been subject to maintenance tasks in the past, are more likely to undergo changes in a future version and thus refactorings involving the corresponding code should have a higher priority. To this end, historical volatility models drawn from the field of forecasting risk in financial markets, are investigated as measures expressing the urgency to resolve a given design problem. The approach has been integrated into an existing smell detection Eclipse plug-in, while the evaluation results focus on the forecast accuracy of the examined models.","n":0.066}}},{"i":222,"$":{"0":{"v":"On the relationship of code-anomaly agglomerations and architectural problems","n":0.333},"1":{"v":"Several projects have been discontinued in the history of the software industry due to the presence of software architecture problems. The identification of such problems in source code is often required in real project settings, but it is a time-consuming and challenging task. Some authors assume that architectural problems are reflected in source code through individual code anomalies. However, each architectural problem may be realized by multiple code anomalies, which are intertwined in several program elements. The relationships of these various code anomalies and their architecture problems’ counterparts are hard to reveal and characterize. To overcome this limitation, we are studying the architecture impact of a wide range of code-anomaly agglomerations. An agglomeration is a group of code anomalies that are explicitly related to each other in the implementation – e.g. two or more anomalies affecting the same class or method in the program. In our empirical study, we analyzed a total of 5418 code anomalies and 2229 agglomerations within 7 systems. In particular, our analysis focused in understanding (i) how agglomerations and architectural problems relate to each other, and (ii) how agglomerations can support the diagnosis of well-known architectural problems. We observed that most of the anomalous code elements related to architectural problems are members of one or more agglomerations. In addition, this study revealed that, for each agglomeration related to an architectural problem, an average of 2 to 4 anomalous code elements contribute to the architectural problem. Finally, the result of our study suggests that certain types of agglomerations are better indicators of architectural problems than others.","n":0.062}}},{"i":223,"$":{"0":{"v":"Challenges to and Solutions for Refactoring Adoption: An Industrial Perspective","n":0.316},"1":{"v":"Refactoring is a key approach for managing technical debt. In the past few years, refactoring techniques and tools have received considerable attention from researchers and tool vendors. However, several practical challenges must be overcome to facilitate the adoption of refactoring in industrial contexts. Results from a survey at the Siemens Corporate Development Center India highlight common challenges to refactoring adoption. The article also outlines ways to address these challenges and describes key initiatives the development center is planning and launching. This article is part of a special issue on Refactoring.","n":0.105}}},{"i":224,"$":{"0":{"v":"Prioritizing maintainability defects based on refactoring recommendations","n":0.378},"1":{"v":"As a measure of software quality, current static code analyses reveal thousands of quality defects on systems in brown-field development in practice. Currently, there exists no way to prioritize among a large number of quality defects and developers lack a structured approach to address the load of refactoring. Consequently, although static analyses are often used, they do not lead to actual quality improvement. Our approach recommends to remove quality defects, exemplary code clones and long methods, which are easy to refactor and, thus, provides developers a first starting point for quality improvement. With an empirical industrial Java case study, we evaluate the usefulness of the recommendation based on developers’ feedback. We further quantify which external factors influence the process of quality defect removal in industry software development.","n":0.089}}},{"i":225,"$":{"0":{"v":"Identifying refactoring sequences for improving software maintainability","n":0.378},"1":{"v":"Refactoring is a well-known technique that preserves software behaviors and improves its bad structures or bad smells. In most cases, more than one bad smell is found in a program. Consequently, developers frequently apply refactorings more than once. Applying appropriate refactoring sequences, an ordered list of refactorings, developers can remove bad smells as well as reduce improvement time and produce highly maintainable software. According to our 2011 survey, developers consider four main criteria to select an optimal refactoring sequence: 1) the number of removed bad smells, 2) maintainability, 3) the size of refactoring sequence and 4) the number of modified program elements. A refactoring sequence that satisfies these four criteria produces code without bad smells, with higher maintainability, using the least improvement effort and time, and providing more traceability. Some existing works suggest a list of refactorings without ordering, and others suggest refactoring sequences. However, these works do not consider the four criteria discussed earlier. Therefore, our research proposes an approach to identify an optimal refactoring sequence that meets these criteria. In addition, it is expected that the findings will reduce maintenance time and cost, increase maintainability and enhance software quality.","n":0.072}}},{"i":226,"$":{"0":{"v":"An expert system for determining candidate software classes for refactoring","n":0.316},"1":{"v":"In the lifetime of a software product, development costs are only the tip of the iceberg. Nearly 90% of the cost is maintenance due to error correction, adaptation and mainly enhancements. As Lehman and Belady Lehman, M. M., & Belady, L. A. (1985). Program evolution: Processes of software change. Academic Press Professional. state that software will become increasingly unstructured as it is changed. One way to overcome this problem is refactoring. Refactoring is an approach which reduces the software complexity by incrementally improving internal software quality. Our motivation in this research is to detect the classes that need to be rafactored by analyzing the code complexity. We propose a machine learning based model to predict classes to be refactored. We use Weighted Naive Bayes with InfoGain heuristic as the learner and we conducted experiments with metric data that we collected from the largest GSM operator in Turkey. Our results showed that we can predict 82% of the classes that need refactoring with 13% of manual inspection effort on the average.","n":0.077}}},{"i":227,"$":{"0":{"v":"Rank-based refactoring decision support: two studies","n":0.408},"1":{"v":"Refactoring can result in code with improved maintainability and is considered a preventive maintenance activity. Managers of large projects need ways to decide where to apply scarce resources when performing refactoring. There is a lack of tools for supporting such decisions. We introduce a rank-based software measure-driven refactoring decision support approach to assist managers. The approach uses various static measures to develop a weighted rank, ranking classes or packages that need refactoring. We undertook two case studies to examine the effectiveness of the approach. Specifically, we wanted to see if the decision support tool yielded results similar to those of human analysts/managers and in less time so that it can be used to augment human decision making. In the first study, we found that our approach identified classes as needing refactoring that were also identified by humans. In the second study, a hierarchical approach was used to identify packages that had actually been refactored in 15 releases of the open source project Tomcat. We examined the overlap between the tool's findings and the actual refactoring activities. The tool reached 100/86.7% recall on the package/class level. Though these studies were limited in size and scope, it appears that this approach is worthy of further examination.","n":0.07}}},{"i":228,"$":{"0":{"v":"Enhancing the Detection of Code Anomalies with Architecture-Sensitive Strategies","n":0.333},"1":{"v":"Research has shown that code anomalies may be related to problems in the architecture design. Without proper mechanisms to support the identification of architecturally-relevant code anomalies, software systems will degrade and might be discontinued as a consequence. Nowadays, metrics-based detection strategy is the most common mechanism to identify code anomalies. However, these strategies often fail to detect architecturally-relevant code anomalies. A key limitation is that they solely exploit measurable static properties of the source code. This paper proposes and evaluates a suite of architecture-sensitive detection strategies. These strategies exploit information related to how fully-modularized and widely-scoped architectural concerns are realized by the code elements. The accuracy of the proposed detection strategies is assessed in a sample of nearly 3500 architecturally-relevant code anomalies and 950 architectural problems distributed in 6 software systems. Our findings show that more than the 60% of code anomalies detected by the proposed strategies were related to architectural problems. Additionally, the proposed strategies identified on average 50% more architecturally-relevant code anomalies than those gathered with using conventional strategies.","n":0.076}}},{"i":229,"$":{"0":{"v":"A Metrics Suite for Object Oriented Design","n":0.378},"1":{"v":"Given the central role that software development plays in the delivery and application of information technology, managers are increasingly focusing on process improvement in the software development area. This demand has spurred the provision of a number of new and/or improved approaches to software development, with perhaps the most prominent being object-orientation (OO). In addition, the focus on process improvement has increased the demand for software measures, or metrics with which to manage the process. The need for such metrics is particularly acute when an organization is adopting a new technology for which established practices have yet to be developed. This research addresses these needs through the development and implementation of a new suite of metrics for OO design. Metrics developed in previous research, while contributing to the field's understanding of software development processes, have generally been subject to serious criticisms, including the lack of a theoretical base. Following Wand and Weber (1989), the theoretical base chosen for the metrics was the ontology of Bunge (1977). Six design metrics are developed, and then analytically evaluated against Weyuker's (1988) proposed set of measurement principles. An automated data collection tool was then developed and implemented to collect an empirical sample of these metrics at two field sites in order to demonstrate their feasibility and suggest ways in which managers may use these metrics for process improvement. >","n":0.067}}},{"i":230,"$":{"0":{"v":"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence","n":0.243},"1":{"v":"From the Publisher:\r\nGenetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.\r\nIn its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.\r\nInitially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements.\r\nJohn H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and isDirector of the University of Michigan/Santa Fe Institute Advanced Research Program.","n":0.063}}},{"i":231,"$":{"0":{"v":"The origins of order","n":0.5}}},{"i":232,"$":{"0":{"v":"Scale-Free Networks: A Decade and Beyond","n":0.408},"1":{"v":"For decades, we tacitly assumed that the components of such complex systems as the cell, the society, or the Internet are randomly wired together. In the past decade, an avalanche of research has shown that many real networks, independent of their age, function, and scope, converge to similar architectures, a universality that allowed researchers from different disciplines to embrace network theory as a common paradigm. The decade-old discovery of scale-free networks was one of those events that had helped catalyze the emergence of network science, a new research field with its distinct set of challenges and accomplishments.","n":0.102}}},{"i":233,"$":{"0":{"v":"The Role of Product Architecture in the Manufacturing Firm","n":0.333},"1":{"v":"Product architecture is the scheme by which the function of a product is allocated to physical components. This paper further defines product architecture, provides a typology of product architectures, and articulates the potential linkages between the architecture of the product and five areas of managerial importance: (1) product change; (2) product variety; (3) component standardization; (4) product performance; and (5) product development management. The paper is conceptual and foundational, synthesizing fragments from several different disciplines, including software engineering, design theory, operations management and product development management. The paper is intended to raise awareness of the far-reaching implications of the architecture of the product, to create a vocabulary for discussing and addressing the decisions and issues that are linked to product architecture, and to identify and discuss specific trade-offs associated with the choice of a product architecture.","n":0.086}}},{"i":234,"$":{"0":{"v":"Decoding the DNA of the Toyota Production System","n":0.354}}},{"i":235,"$":{"0":{"v":"The Architecture of Complexity","n":0.5},"1":{"v":"A number of proposals have been advanced in recent years for the development of “general systems theory” which, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial.","n":0.108}}},{"i":236,"$":{"0":{"v":"Does code decay? Assessing the evidence from change management data","n":0.316},"1":{"v":"A central feature of the evolution of large software systems is that change-which is necessary to add new functionality, accommodate new hardware, and repair faults-becomes increasingly difficult over time. We approach this phenomenon, which we term code decay, scientifically and statistically. We define code decay and propose a number of measurements (code decay indices) on software and on the organizations that produce it, that serve as symptoms, risk factors, and predictors of decay. Using an unusually rich data set (the fifteen-plus year change history of the millions of lines of software for a telephone switching system), we find mixed, but on the whole persuasive, statistical evidence of code decay, which is corroborated by developers of the code. Suggestive indications that perfective maintenance can retard code decay are also discussed.","n":0.088}}},{"i":237,"$":{"0":{"v":"A complexity measure","n":0.577}}},{"i":238,"$":{"0":{"v":"A Model-Based Method for Organizing Tasks in Product Development","n":0.333},"1":{"v":"This research is aimed at structuring complex design projects in order to develop better products more quickly. We use a matrix representation to capture both the sequence of and the technical relationships among the many design tasks to be performed. These relationships define the “technical structure” of a project, which is then analyzed in order to find alternative sequences and/or definitions of the tasks. Such improved design procedures offer opportunities to speed development progress by streamlining the inter-task coordination. After using this technique to model design processes in several organizations, we have developed a design management strategy which focuses attention on the essential information transfer requirements of a technical project.","n":0.095}}},{"i":239,"$":{"0":{"v":"Imitation of Complex Strategies","n":0.5},"1":{"v":"Researchers examining loosely coupled systems, knowledge management, and complementary practices in organizations have proposed, informally, that the complexity of a successful business strategy can deter imitation of the strategy. This paper explores this proposition rigorously. A simple model is developed that parametrizes the two aspects of strategic complexity: the number of elements in a strategy and the interactions among those elements. The model excludes conventional resource-based and game-theoretic barriers to imitation altogether. The model is used to show that complexity makes the search for an optimal strategy intractable in the technical sense of the word provided by the theory of NP-completeness. Consequently, would-be copycats must rely on search heuristics or on learning, not on algorithmic \"solutions,\" to match the performance of superior firms. However, complexity also undermines heuristics and learning. In the face of complexity, firms that follow simple hill-climbing heuristics are quickly snared on low \"local peaks,\" and firms that try to learn and mimic a high performer's entire strategy suffer large penalties from small errors. The model helps to explain why some winning strategies remain unmatched even though they are open to public scrutiny; why certain bundles of organizational practices diffuse slowly even though they lead to superior performance; and why some strategies yield superior returns even after many of their critical ingredients are adopted by competitors. The analysis also suggests roles for management science and managerial choice in a world of complex strategies.","n":0.065}}},{"i":240,"$":{"0":{"v":"On the criteria to be used in decomposing systems into modules","n":0.302},"1":{"v":"This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a \"modularization\" is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.","n":0.092}}},{"i":241,"$":{"0":{"v":"The Misalignment of Product Architecture and Organizational Structure in Complex Product Development","n":0.289},"1":{"v":"Product architecture knowledge is typically embedded in the communication patterns of established development organizations. While this enables the development of products using the existing architecture, it hinders the organization's ability to implement novel architectures, especially for complex products. Structured methods addressing this issue are lacking, as previous research has studied complex product development from two separate perspectives: product architecture and organizational structure. Our research integrates these viewpoints with a structured approach to study how design interfaces in the product architecture map onto communication patterns within the development organization. We investigate how organizational and system boundaries, design interface strength, indirect interactions, and system modularity impact the alignment of design interfaces and team interactions. We hypothesize and test how these factors explain the existence of the following cases: (1) known design interfaces not addressed by team interactions, and (2) observed team interactions not predicted by design interfaces. Our results offer important insights to managers dealing with interdependences across organizational and functional boundaries. In particular, we show how boundary effects moderate the impact of design interface strength and indirect team interactions, and are contingent on system modularity. The research uses data collected from a large commercial aircraft engine development process.","n":0.071}}},{"i":242,"$":{"0":{"v":"Elements of software science","n":0.5}}},{"i":243,"$":{"0":{"v":"Developing Products on Internet Time: The Anatomy of a Flexible Development Process","n":0.289},"1":{"v":"Uncertain and dynamic environments present fundamental challenges to managers of the new product development process. Between successive product generations, significant evolutions can occur in both the customer needs a product must address and the technologies it employs to satisfy these needs. Even within a single development project, firms must respond to new information, or risk developing a product that is obsolete the day it is launched. This paper examines the characteristics of an effective development process in one such environment-the Internet software industry. Using data on 29 completed development projects we show that in this industry, constructs that support a more flexible development process are associated with better-performing projects. This flexible process is characterized by the ability to generate and respond to new information for a longer proportion of a development cycle. The constructs that support such a process are greater investments in architectural design, earlier feedback on product performance from the market, and the use of a development team with greater amounts of \"generational\" experience. Our results suggest that investments in architectural design play a dual role in a flexible process: First, through the need to select an architecture that maximizes product performance and, second, through the need to select an architecture that facilitates development process flexibility. We provide examples from our fieldwork to support this view.","n":0.068}}},{"i":244,"$":{"0":{"v":"The design structure system: A method for managing the design of complex systems","n":0.277},"1":{"v":"Systems design involves the determination of interdependent variables. Thus the precedence ordering for the tasks of determining these variables involves circuits. Circuits require planning decisions about how to iterate and where to use estimates. Conventional planning techniques, such as critical path, do not deal with these problems. Techniques are shown which acknowledge these circuits in the design of systems. These techniques can be used to develop an effective engineering plan, showing where estimates are to be used, how design iterations and reviews are handled, and how information flows during the design work. This information flow can be used to determine the consequences of a change in any variable on the rest of the variables in the system, and thus which engineers must be informed and which documents must be changed. From this, a critical path schedule can be developed for implementing the change. This method is ideally suited to an automated design office where data, computer input and output, and communications are all handled through the use of computer terminals and data bases. However, these same techniques can also be effectively used in classical engineering environments.","n":0.073}}},{"i":245,"$":{"0":{"v":"Characterizing complex product architectures","n":0.5},"1":{"v":"Due to the large-scale nature of complex product architectures, it is necessary to develop some form of abstraction in order to be able to describe and grasp the structure of the product, facilitating product modularization. In this paper we develop three methods for describing product architectures: (a) the Dependency Structure Matrix (DSM), (b) Molecular Diagrams (MD), and (c) Visibility-Dependency (VD) signature diagrams. Each method has its own language (and abstraction), which can be used to qualitatively or quantitatively characterize any given architecture spanning the modular-integrated continuum. A consequence of abstraction is the loss of some detail. So, it is important to choose the correct method (and resolution) to characterize the architecture in order to retain the salient details. The proposed methods are suited for describing architectures of varying levels of complexity and detail. The three methods are demonstrated using a sequence of illustrative simple examples and a case-study analysis of a complex product architecture for an industrial gas turbine. © 2003 Wiley Periodicals, Inc. Syst Eng 7: 35–60, 2004","n":0.077}}},{"i":246,"$":{"0":{"v":"A Network Approach to Define Modularity of Components in Complex Products","n":0.302},"1":{"v":"Modularity has been defined at the product and system levels. However, little effort has gone into defining and quantifying modularity at the component level. We consider complex products as a network of components that share technical interfaces (or connections) in order to function as a whole and define component modularity based on the lack of connectivity among them. Building upon previous work in graph theory and social network analysis, we define three measures of component modularity based on the notion of centrality. Our measures consider how components share direct interfaces with adjacent components, how design interfaces may propagate to nonadjacent components in the product, and how components may act as bridges among other components through their interfaces. We calculate and interpret all three measures of component modularity by studying the product architecture of a large commercial aircraft engine. We illustrate the use of these measures to test the impact of modularity on component redesign. Our results show that the relationship between component modularity and component redesign depends on the type of interfaces connecting product components. We also discuss directions for future work. DOI: 10.1115/1.2771182","n":0.074}}},{"i":247,"$":{"0":{"v":"Patterned Interactions in Complex Systems: Implications for Exploration","n":0.354},"1":{"v":"Scholars who view organizational, social, and technological systems as sets of interdependent decisions have increasingly used simulation models from the biological and physical sciences to examine system behavior. These models shed light on an enduring managerial question: How much exploration is necessary to discover a good configuration of decisions? The models suggest that, as interactions across decisions intensify and local optima proliferate, broader exploration is required. The models typically assume, however, that the interactions among decisions are distributed randomly. Contrary to this assumption, recent empirical studies of real organizational, social, and technological systems show that interactions among decisions are highly patterned. Patterns such as centralization, small-world connections, power-law distributions, hierarchy, and preferential attachment are common. We embed such patterns into an NK simulation model and obtain dramatic results: Holding fixed the total number of interactions among decisions, a shift in the pattern of interaction can alter the number of local optima by more than an order of magnitude. Thus, the long-run value of broader exploration is significantly greater in the face of some interaction patterns than in the face of others. We develop simple, intuitive rules of thumb that allow a decision maker to examine two interaction patterns and determine which warrants greater investment in broad exploration. We also find that, holding fixed the interaction pattern, an increase in the number of interactions raises the number of local optima regardless of the pattern. This validates prior comparative static results with respect to the number of interactions, but highlights an important implicit assumption in earlier work---that the underlying interaction pattern remains constant as interactions become more numerous.","n":0.061}}},{"i":248,"$":{"0":{"v":"Managing product families: The case of the Sony Walkman","n":0.333},"1":{"v":"Abstract   Success in fast cycle industries (e.g. consumer electronics) can depend both on rapid model replacement and on model longevity. Although Sony was as fast as any of its chief competitors in getting new models to market, an important explanation for the wide variety of models offered by the firm is the greater longevity of its key models. This finding adds an important insight to the conventional literature on time-based competition which emphasizes rapid innovation exclusively. Sony's special understanding of the US market enabled it to respond more effectively to life style differences by locating industrial designers in its key markets. Sony's strong design capability and effective division of labor (engineers lead generational and incremental projects, industrial designers and market personnel lead topological projects) allow for parallel model development. Investment in manufacturing flexibility amortized over multiple models within the product family make the rapid model changeover possible.","n":0.082}}},{"i":249,"$":{"0":{"v":"Elements of software science (Operating and programming systems series)","n":0.333}}},{"i":250,"$":{"0":{"v":"THE INFLUENCE OF ARCHITECTURE IN ENGINEERING SYSTEMS","n":0.378},"1":{"v":"The field of Engineering Systems is distinguished from traditional engineering design in part by the issues it brings to the top. Engineering Systems focuses on abstractions like architecture and complexity, and defines system boundaries very broadly. It also seeks to apply these concepts to the process of creating systems. This paper summarizes the role and influence of architecture in complex engineering systems. Using the research literature and examples, this paper defines architecture, argues for its importance as a determinant of system behavior, and reviews its ability to help us understand and manage the design, operation, and behaviors of complex engineering systems. A. INTRODUCTION Typical engineering design education focuses on specific aspects of design, such as the technical behavior of a set of elements interconnected in a certain way. By contrast, Engineering Systems focuses on a number of abstract concepts first because they provide a general framework for guiding the development of many diverse kinds of systems, so that these systems will provide the desired functions in the desired ways. Among these abstract concepts is that of system architecture. In this paper, we explore this concept and provide a number of ways of appreciating system architecture’s importance in both the practical aspects of system design and in the intellectual aspects of understanding complex systems from a variety of viewpoints. The paper begins with a definition of architecture and its influence on functional behavior, extra desired properties like flexibility and reliability (collectively called “ilities”), complexity, and emergent behaviors. Architectures are not static but instead evolve over long periods as technologies mature. They also evolve during the normal course of designing an individual system. These evolutionary patterns are useful in understanding architecture’s importance. The paper next provides several examples of architectures and illustrates how architecture affects the way systems are designed, built, and operated. The examples include aircraft, automobiles, infrastructures, and living organisms. The importance of architecture is framed in three domains of importance: as a way to understand complex systems, to design them, to manage them, and to provide long-term rationality by means of standards. The abstract concepts of modularity and integrality are shown to be useful for categorizing systems and illustrating how architectural form can influence important system characteristics. Several contrasts are noted between relatively small, deliberately designed products and evolutionary, less-managed large infrastructures. Architecture’s ability to influence the functions and allied properties of systems is shown to extend to robustness, adaptability, flexibility, safety, and scalability. Examples from recent research are given to show how some of these properties might be measured using network models of particular architectures.","n":0.048}}},{"i":251,"$":{"0":{"v":"Software complexity and maintenance costs","n":0.447},"1":{"v":"While the link between the difficulty in understanding computer software and the cost of maintaining it is appealing, prior empirical evidence linking software complexity to software maintenance costs is relatively weak [21]. Many of the attempts to link software complexity to maintainability are based on experiments involving small pieces of code, or are based on analysis of software written by students. Such evidence is valuable, but several researchers have noted that such results must be applied cautiously to the large-scale commercial application systems that account for most software maintenance expenditures [13,17]","n":0.105}}},{"i":252,"$":{"0":{"v":"Modularization of a Large-Scale Business Application: A Case Study","n":0.333},"1":{"v":"In industries such as banking, retail, transportation, and telecommunications, large software systems support numerous work processes and develop over many years. Throughout their evolution, such systems are subject to repeated debugging and feature enhancements. Consequently, they gradually deviate from the intended architecture and deteriorate into unmanageable monoliths. To contend with this, practitioners often rewrite the entire application in a new technology or invest considerable time in documenting the code and training new engineers to work on it. However, for very large systems, such approaches are typically impossible to carry out. As an alternative, researchers have proposed several tools to automatically modularize software that's grown to be inadequate in both quality and scalability. This case study describes the modularization approach that one company adopted to reengineer a monolithic banking application beset with maintenance and complexity problems. In this case study, we describe the modularization approach we adopted to address this situation, as well as certain other benefits we unearthed as a result of this reengineering exercise.","n":0.078}}},{"i":253,"$":{"0":{"v":"The Impact of Component Modularity on Design Evolution: Evidence from the Software Industry","n":0.277},"1":{"v":"Much academic work asserts a relationship between the design of a complex system and the manner in which this system evolves over time. In particular, designs which are modular in nature are argued to be more \"evolvable,\" in that these designs facilitate making future adaptations, the nature of which do not have to be specified in advance. In essence, modularity creates \"option value\" with respect to new and improved designs, which is particularly important when a system must meet uncertain future demands. Despite the conceptual appeal of this research, empirical work exploring the relationship between modularity and evolution has had limited success. Three major challenges persist: first, it is difficult to measure modularity in a robust and repeatable fashion; second, modularity is a property of individual components, not systems as a whole, hence we must examine these dynamics at the microstructure level; and third, evolution is a temporal phenomenon, in that the conditions at time t affect the nature of the design at time t+1, hence exploring this phenomenon requires longitudinal data. In this paper, we tackle these challenges by analyzing the evolution of a successful commercial software product over its entire lifetime, comprising six major \"releases.\" In particular, we develop measures of modularity at the component level, and use these to predict patterns of evolution between successive versions of the design. We find that modularity has a strong and unambiguous impact on design evolution. Specifically, we show that i) tightly-coupled components are \"harder to kill,\" in that they have a greater likelihood of survival in subsequent versions of a design; ii) tightly-coupled components are \"harder to maintain,\" in that they experience more surprise changes to their dependency relationships that are not associated with new functionality; and iii) tightly-coupled components are \"harder to augment,\" in that the mix of new components added in each version is significantly more modular than the legacy design.","n":0.057}}},{"i":254,"$":{"0":{"v":"An empirical study of static call graph extractors","n":0.354},"1":{"v":"Informally, a call graph represents calls between entities in a given program. The call graphs that compilers compute to determine the applicability of an optimization must typically be conservative: a call may be omitted only if it can never occur in any execution of the program. Numerous software engineering tools also extract call graphs with the expectation that they will help software engineers increase their understanding of a program. The requirements placed on software engineeringtools that compute call graphs are typically more relaxed than for compilers. For example, some false negatives—calls that can in fact take place in some execution of the program, but which are omitted from the call graph—may be acceptable, depending on the understanding task at hand. In  this article, we empirically show a consequence of this spectrum of requirements by comparing the C call graphs extracted from three software systems (mapmaker, mosaic, and gcc) by nine tools (cflow, cawk, CIA, Field, GCT, Imagix, LSME, Mawk, and Rigiparse). A quantitative analysis of the call graphs extracted for each system shows considerable variation, a result that is counterintuitive to many experienced software engineers. A qualitative analysis of these results reveals a number of reasons for  this variation: differing treatments of macros, function pointers, input formats, etc. The fundamental problem is not that variances among the graphs extracted by different tools exist, but that software engineers have little sense of the dimensions of approximation in any particular call  graph. In this article, we describe and discuss the study, sketch a design space for static call graph extractors, and discuss the impact of our study on practitioners, tool developers, and researchers. Although this article considers only one kind of information, call graphs, many of the observations also apply to static extractors of other kinds of information, such as inheritance structures, file dependences, and references to global variables.","n":0.057}}},{"i":255,"$":{"0":{"v":"Complex Engineered Systems: Science Meets Technology","n":0.408},"1":{"v":"This book sheds light on the large-scale engineering systems that shape and guide our everyday lives. It does this by bringing together the latest research and practice defining the emerging field of Complex Engineered Systems. Understanding, designing, building and controlling such complex systems is going to be a central challenge for engineers in the coming decades. This book is a step toward addressing that challenge.","n":0.124}}},{"i":256,"$":{"0":{"v":"The Moderating Effects of Structure on Volatility and Complexity in Software Enhancement","n":0.289},"1":{"v":"The cost of enhancing software applications to accommodate new and evolving user requirements is significant. Many enhancement cost-reduction initiatives have focused on increasing software structure in applications. However, while software structure can decrease enhancement effort by localizing data processing, increased effort is also required to comprehend structure. Thus, it is not clear whether high levels of software structure are economically efficient in all situations. In this study, we develop a model of the relationship between software structure and software enhancement costs and errors. We introduce the notion of software structure as a moderator of the relationship between software volatility, total data complexity, and software enhancement outcomes. We posit that it is efficient to more highly structure the more volatile applications, because increased familiarity with the application structure through frequent enhancement enables localization of maintenance effort. For more complex applications, software structure is more beneficial than for less complex applications because it facilitates the comprehension process where it is most needed. Given the downstream enhancement benefits of structure for more volatile and complex applications, we expect that the optimal level of structure is higher for these applications. We empirically evaluate our model using data collected on the business applications of a major mass merchandiser and a large commercial bank. We find that structure moderates the relationship between complexity, volatility, and enhancement outcomes, such that higher levels of structure are more advantageous for the more complex and more volatile applications in terms of reduced enhancement costs and errors. We also find that more structure is designed in for volatile applications and for applications with higher levels of complexity. Finally, we identify application type as a significant factor in predicting which applications are more volatile and more complex at our research sites. That is, applications with induction-based algorithms such as those that support planning, forecasting, and management decision-making activities are more complex and more volatile than applications with rule-based algorithms that support operational and transaction-processing activities. Our results indicate that high investment in software quality practices such as structured design is not economically efficient in all situations. Our findings also suggest the importance of organizational mechanisms in promoting efficient design choices that lead to reduced enhancement costs and errors.","n":0.052}}},{"i":257,"$":{"0":{"v":"Determinants of software maintenance profiles: an empirical investigation","n":0.354},"1":{"v":"Software maintenance is a task that is difficult to manage effectively. In part, this is because software managers have very little knowledge about the types of maintenance work that are likely to occur. If managers could forecast changes to software systems, they could more effectively plan, allocate workforce and manage change requests. But, the ability to forecast software modifications depends on whether there are predictable patterns in maintenance work. We posit that there are patterns in maintenance work and that certain characteristics of software modules are associated with these patterns.\r\n\r\nWe examine modification profiles for 621 software modules in five different business systems of a commercial merchandiser. We find that only a small number of modules in these systems is likely to be modified frequently, and that certain maintenance patterns emerge. Modules frequently enhanced are in systems whose functionality is considered strategic. Modules frequently repaired have high software complexity, are large in size, and are relatively older. However, modules that have been code generated are less likely to be repaired. Older and larger modules are restructured and upgraded more frequently. Our results suggest that these characteristics of software modules are associated with predictable maintenance profiles. Such profile information can be used by software managers to predict and plan for maintenance more effectively. In addition, our results suggest the use of code generators as a means of reducing repair maintenance. © 1997 John Wiley & Sons, Ltd.","n":0.065}}},{"i":258,"$":{"0":{"v":"Quantitative models of cohesion and coupling in software","n":0.354},"1":{"v":"Abstract   Our project goal is to specify, implement, and verify quantitative models for measuring cohesion and coupling (C & C) in software modules. This article is our project interim report on the specification of the C & C quantitative models and preliminary verification effort. To quantify cohesion, we subdivided it into four categories and then quantified each category. Coupling is subdivided into four categories, followed by the quantification of each category. Although the C & C concepts are applicable to any procedural language such as FORTRAN, PASCAL, or Ada, we chose to apply the C & C formulas to Ada programs. We have hand-calculated C & C values for a number of programs, but here we report and discuss in detail only a typical result of our calculations obtained by applying the C & C formulas to two different implementations of an algorithm. We have found that the formulas are sensitive enough to distinguish between the two implementations, and the obtained quantitative values agree with the qualitative assessment of the implementations.","n":0.076}}},{"i":259,"$":{"0":{"v":"Complex Engineered Systems: Science Meets Technology (Understanding Complex Systems)","n":0.333}}},{"i":260,"$":{"0":{"v":"Linking cyclicality and product quality","n":0.447}}},{"i":261,"$":{"0":{"v":"Environmental Volatility, Development Decisions, and Software Volatility: A Longitudinal Analysis","n":0.316},"1":{"v":"Although product development research often focuses on activities prior to product launch, for long-lived, adaptable products like software, development can continue over the entire product life cycle. For managers of these products the challenges are to predict when and how much the products will change and to understand how their development decisions influence the timing and magnitude of future change activities. We develop a two-stage model that relates environmental volatility to product development decisions and product development decisions to software volatility. The model is evaluated using a data archive that captures changes over 20 years to a firms environment, its managers development choices, and its software products. In Stage 1 we find that higher environmental volatility leads to greater use of process technology and standard component designs but less team member rotation. Earlier development decisions strongly influence current development choices, especially for product design and process technology. In Stage 2 we find that increased use of standard component designs dampens future software volatility by decreasing the average rate and magnitude of change. Adding new team members increases product enhancements at a faster pace than more intense use of process technology but adds repairs at almost the same rate as enhancements.","n":0.071}}},{"i":262,"$":{"0":{"v":"A coordination perspective on software architecture: towards a design handbook for integrating software components","n":0.267},"1":{"v":"This thesis argues that many of the difficulties associated with building software applications by integrating existing components are related to a failure of current programming languages to recognize component interconnection as a separate design problem, orthogonal to the specification and implementation of a component's core function. It proposes SYNOPSIS, an architectural description language which supports two orthogonal abstractions: activities, for representing the functional pieces of an application, and dependencies, for describing their interconnection relationships. Coordination processes, defined as an attribute of dependencies, describe implementations of interconnection protocols. Executable systems can be generated from SYNOPSIS descriptions by successively replacing activities with more specialized versions and managing dependencies with coordination processes, until all elements of the description are specific enough for code generation to take place. Furthermore, it proposes a \"design handbook\", consisting of a vocabulary of common dependency types and a design space of associated coordination processes. The handbook is based on the insight that many software interconnection relationships can be described using a relatively narrow set of concepts orthogonal to the problem domain of most applications, such as resource flows, resource sharing, and timing dependencies. A prototype component-based application development tool called SYNTHESIS was developed. SYNTHESIS maintains a repository of increasingly specialized dependency types, based on the proposed design handbook. It assists the generation of executable applications by successive semi-automatic transformations of their SYNOPSIS descriptions. A set of four experiments is described. Each experiment consisted in specifying a test application as a SYNOPSIS diagram, associating application activities with components exhibiting various mismatches, and using SYNTHESIS to assemble these components into executable systems. SYNTHESIS was able to exploit its dependencies repository in order to resolve a wide range of interoperability and architectural mismatches and integrate independently developed components into the test applications, with minimal or no need for additional manually-written code. It was able to reuse a single SYNOPSIS architectural description in order to generate versions of a test application for two different execution environments. Finally, it was able to suggest various alternative architectures for integrating each component set into its corresponding application.","n":0.054}}},{"i":263,"$":{"0":{"v":"Error localization during software maintenance: generating hierarchical system descriptions from the source code alone","n":0.267},"1":{"v":"An empirical study is presented that investigates hierarchical software system descriptions that are based on measures of cohesion and coupling. The study evaluates the effectiveness of the hierarchical descriptions in identifying error-prone system structure. The measurement of cohesion and coupling is based on intrasystem interaction in terms of software data bindings. The measurement of error-proneness is based on software error data collected from high-level system design through system test; some error data from system operation are also included. The data bindings software analysis and supporting tools are described, followed by the data analysis, interpretations of the results, and some conclusions. >","n":0.1}}},{"i":264,"$":{"0":{"v":"A Practical Model for Measuring Maintainability","n":0.408},"1":{"v":"The amount of effort needed to maintain a software system is related to the technical quality of the source code of that system. The ISO 9126 model for software product quality recognizes maintainability as one of the 6 main characteristics of software product quality, with adaptability, changeability, stability, and testability as subcharacteristics of maintainability. Remarkably, ISO 9126 does not provide a consensual set of measures for estimating maintainability on the basis of a system's source code. On the other hand, the maintainability index has been proposed to calculate a single number that expresses the maintainability of a system. In this paper, we discuss several problems with the MI, and we identify a number of requirements to be fulfilled by a maintainability model to be usable in practice. We sketch a new maintainability model that alleviates most of these problems, and we discuss our experiences with using such as system for IT management consultancy activities.","n":0.081}}},{"i":265,"$":{"0":{"v":"A framework for developing measurement systems and its industrial evaluation","n":0.316},"1":{"v":"As in every engineering discipline, metrics play an important role in software development, with the difference that almost all software projects need the customization of metrics used. In other engineering disciplines, the notion of a measurement system (i.e. a tool used to collect, calculate, and report quantitative data) is well known and defined, whereas it is not as widely used in software engineering. In this paper we present a framework for developing custom measurement systems and its industrial evaluation in a software development unit within Ericsson. The results include the framework for designing measurement systems and its evaluation in real life projects at the company. The results show that with the help of ISO/IEC standards, measurement systems can be effectively used in software industry and that the presented framework improves the way of working with metrics. This paper contributes with the presentation of how automation of metrics collection and processing can be successfully introduced into a large organization and shows the benefits of it: increased efficiency of metrics collection, increased adoption of metrics in the organization, independence from individuals and standardized nomenclature for metrics in the organization.","n":0.073}}},{"i":266,"$":{"0":{"v":"Six strategies for generalizing software engineering theories","n":0.378},"1":{"v":"General theories of software engineering must balance between providing full understanding of a single case and providing partial understanding of many cases. In this paper we argue that for theories to be useful in practice, they should give sufficient understanding of a sufficiently large class of cases, without having to be universal or complete. We provide six strategies for developing such theories of the middle range.\r\n\r\nIn lab-to-lab strategies, theories of laboratory phenomena are developed and generalized to other laboratory phenomena. This is a characteristic strategy for basic science. In lab-to-field strategies, theories are developed of artifacts that first operate under idealized laboratory conditions, which are then scaled up until they can operate under uncontrolled field conditions. This is the characteristic strategy for the engineering sciences.\r\n\r\nIn case-based strategies, we generalize about components of real-world cases, that are supposed to exhibit less variation than the cases as a whole. In sample-based strategies, we generalize about the aggregate behavior of samples of cases, which can exhibit patterns not visible at the case level. We discuss three examples of sample-based strategies.\r\n\r\nThroughout the paper, we use examples of theories and generalization strategies from software engineering to illustrate our analysis. The paper concludes with a discussion of related work and implications for empirical software engineering research.","n":0.069}}},{"i":267,"$":{"0":{"v":"Reducing Friction in Software Development","n":0.447},"1":{"v":"Software is being produced so fast that its growth hinders its sustainability. Technical debt, which encompasses internal software quality, evolution and maintenance, reengineering, and economics, is growing such that its management is becoming the dominant driver of software engineering progress. It spans the software engineering life cycle, and its management capitalizes on recent advances in fields such as source code analysis, quality measurement, and project management. Managing technical debt will become an investment activity applying economic theories. It will effectively address the architecture level and will offer specific processes and tools employing data science and analytics to support decision making. It will also be an essential part of the software engineering curriculum. Getting ahead of the software quality and innovation curve will inevitably involve establishing technical-debt management as a core software engineering practice. This article is part of a special issue on the Future of Software Engineering.","n":0.082}}},{"i":268,"$":{"0":{"v":"Identifying risky areas of software code in Agile/Lean software development: An industrial experience report","n":0.267},"1":{"v":"Modern software development relies on incremental delivery to facilitate quick response to customers' requests. In this dynamic environment the continuous modifications of software code can cause risks for software developers; when developing a new feature increment, the added or modified code may contain fault-prone or difficult-to-maintain elements. The outcome of these risks can be defective software or decreased development velocity. This study presents a method to identify the risky areas and assess the risk when developing software code in Lean/Agile environment. We have conducted an action research project in two large companies, Ericsson AB and Volvo Group Truck Technology. During the study we have measured a set of code properties and investigated their influence on risk. The results show that the superposition of two metrics, complexity and revisions of a source code file, can effectively enable identification and assessment of the risk. We also illustrate how this kind of assessment can be successfully used by software developers to manage risks on a weekly basis as well as release-wise. A measurement system for systematic risk assessment has been introduced to two companies.","n":0.074}}},{"i":269,"$":{"0":{"v":"Standardization and modularization driven by minimizing overall process effort","n":0.333},"1":{"v":"Faster product development is a major goal for companies in competitive markets. Product platform architectures support planning for addressing diverse markets and fulfilling future market desires. Applying standardization or modularization on product platform components leverages current product design effort across future products. This work introduces a method-SMDP (standardization and modularization driven by process effort)-for focusing engineering effort when applying standardization or modularization on product platform components. SMDP calculates the total design effort from current to future generations of the platform following standardization or modularization of components. By comparing the total design cost of different simulations, we can direct the design team to standardization or modularization opportunities. The contribution of this work is in using an estimation of design effort as the basis for decision in contrast to commonly used static measures of components' interactions. Such a computational approach allows conducting sensitivity studies that address the subjective nature of various estimations needed for exercising SMDP. SMDP is illustrated in a product platform design of an external-drum plate-setter for the digital prepress printing market.","n":0.076}}},{"i":270,"$":{"0":{"v":"Object-Oriented Metrics in Practice","n":0.5}}},{"i":271,"$":{"0":{"v":"Do They Really Smell Bad? A Study on Developers' Perception of Bad Code Smells","n":0.267},"1":{"v":"In the last decade several catalogues have been defined to characterize bad code smells, i.e., symptoms of poor design and implementation choices. On top of such catalogues, researchers have defined methods and tools to automatically detect and/or remove bad smells. Nevertheless, there is an ongoing debate regarding the extent to which developers perceive bad smells as serious design problems. Indeed, there seems to be a gap between theory and practice, i.e., what is believed to be a problem (theory) and what is actually a problem (practice). This paper presents a study aimed at providing empirical evidence on how developers perceive bad smells. In this study, we showed to developers code entities -- belonging to three systems -- affected and not by bad smells, and we asked them to indicate whether the code contains a potential design problem, and if any, the nature and severity of the problem. The study involved both original developers from the three projects and outsiders, namely industrial developers and Master's students. The results provide insights on characteristics of bad smells not yet explored sufficiently. Also, our findings could guide future research on approaches for the detection and removal of bad smells.","n":0.072}}},{"i":272,"$":{"0":{"v":"Do developers care about code smells? An exploratory survey","n":0.333},"1":{"v":"Code smells are a well-known metaphor to describe symptoms of code decay or other issues with code quality which can lead to a variety of maintenance problems. Even though code smell detection and removal has been well-researched over the last decade, it remains open to debate whether or not code smells should be considered meaningful conceptualizations of code quality issues from the developer's perspective. To some extent, this question applies as well to the results provided by current code smell detection tools. Are code smells really important for developers? If they are not, is this due to the lack of relevance of the underlying concepts, due to the lack of awareness about code smells on the developers' side, or due to the lack of appropriate tools for code smell analysis or removal? In order to align and direct research efforts to address actual needs and problems of professional developers, we need to better understand the knowledge about, and interest in code smells, together with their perceived criticality. This paper reports on the results obtained from an exploratory survey involving 85 professional software developers.","n":0.074}}},{"i":273,"$":{"0":{"v":"Assessing the capability of code smells to explain maintenance problems: an empirical study combining quantitative and qualitative data","n":0.236},"1":{"v":"Code smells are indicators of deeper design problems that may cause difficulties in the evolution of a software system. This paper investigates the capability of twelve code smells to reflect actual maintenance problems. Four medium-sized systems with equivalent functionality but dissimilar design were examined for code smells. Three change requests were implemented on the systems by six software developers, each of them working for up to four weeks. During that period, we recorded problems faced by developers and the associated Java files on a daily basis. We developed a binary logistic regression model, with \"problematic file\" as the dependent variable. Twelve code smells, file size, and churn constituted the independent variables. We found that violation of the Interface Segregation Principle (a.k.a. ISP violation) displayed the strongest connection with maintenance problems. Analysis of the nature of the problems, as reported by the developers in daily interviews and think-aloud sessions, strengthened our view about the relevance of this code smell. We observed, for example, that severe instances of problems relating to change propagation were associated with ISP violation. Based on our results, we recommend that code with ISP violation should be considered potentially problematic and be prioritized for refactoring.","n":0.071}}},{"i":274,"$":{"0":{"v":"Supporting the identification of architecturally-relevant code anomalies","n":0.378},"1":{"v":"Code anomalies are likely to be critical to the systems' maintainability when they are related to architectural problems. Many tools have been developed to support the identification of code anomalies. However, those tools are restricted to only analyze source code structure and identify individual anomaly occurrences. These limitations are the main reasons why state-of-art tools are often unable to identify architecturally-relevant code anomalies, i.e. those related to architectural problems. To overcome these shortcomings we propose SCOOP, a tool that includes: (i) architecture-code traces in the analysis of the source code, and (ii) exploits relationships between multiple occurrences of code anomalies to detect the architecturally-relevant ones. Our preliminary evaluation indicated that SCOOP was able to detect anomalous code elements related to 293 out of 368 architectural problems found in 3 software systems.","n":0.087}}},{"i":275,"$":{"0":{"v":"On the Shape of Circular Dependencies in Java Programs","n":0.333},"1":{"v":"Circular dependencies between software artefacts are widely considered as problematic. However, empirical studies of Java programs have shown that most programs are riddled with circular dependencies. This seems to imply that not all circular dependencies are as detrimental to software quality as previously thought. Clearly, a better understanding of the types of circular dependency and their effect on software quality is required. In this paper, we provide precise definitions for different types of circular dependencies, analyse their topology and investigate the relationship between circular dependencies and the package containment tree. Our analysis is based on the popular Qualities Corpus data set. We find that in package dependency graphs, most circular dependencies are \"package local\": they are confined to branches of the package containment tree where they form around parent packages. Existing research indicates that these dependencies may not be critical. This may explain why circular dependencies are so common in widely-used real-world programs.","n":0.081}}},{"i":276,"$":{"0":{"v":"The Pricey Bill of Technical Debt: When and by Whom will it be Paid?","n":0.267},"1":{"v":"Software companies need to support continuous and fast delivery of customer value both in short and a long-term perspective. However, this can be hindered by evolution limitations and high maintenance efforts due to internal software quality issues by what is described as Technical Debt. Although significant theoretical work has been undertaken to describe the negative effects of Technical Debt, these studies tend to have a weak empirical basis and often lack quantitative data. The aim of this study is to estimate wasted time, caused by the Technical Debt interest during the software life-cycle. This study also investigates how practitioners perceive and estimate the impact of the negative consequences due to Technical Debt during the software development process. This paper reports the results of both an online web-survey provided quantitative data from 258 participants and follow-up interviews with 32 industrial software practitioners. The importance and originality of this study contributes and provides novel insights into the research on Technical Debt by quantifying the perceived interest and the negative effects it has on the software development life-cycle. The findings show that on average, 36% of all development time is estimated to be wasted due to Technical Debt; Complex Architectural Design and Requirement Technical Debt generates most negative effect; and that most time is wasted on understanding and/or measuring the Technical Debt. Moreover, the analysis of the professional roles and the age of the software system in the survey revealed that different roles are affected differently and that the consequences of Technical Debt are also influenced by the age of the software system.","n":0.062}}},{"i":277,"$":{"0":{"v":"Do Code Smells Impact the Effort of Different Maintenance Programming Activities","n":0.302},"1":{"v":"Empirical studies have shown so far that code smells have relatively low impact over maintenance effort at file level. We surmise that previous studies have found low effects of code smells because the effort considered is a \"sheer-effort\" that does not distinguish between the kinds of developers' activities. In our study, we investigate the effects of code smells at the activity level. Examples of activities are: reading, editing, searching, and navigating, which are performed independently over different files during maintenance. We conjecture that structural attributes represented in the form of different code smells do indeed have an effect on the effort for performing certain kinds of activities. To verify this conjecture, we revisit a previous study about the impact of code smell on maintenance effort, using the same dataset, but considering activity effort. Six professional developers were hired to perform three maintenance tasks on four functionally equivalent Java Systems. Each developer performs two maintenance tasks. During maintenance task, we monitor developers' logs. Then, we define an annotation schema to identify developers' activities and assess whether code smells affect different maintenance activities. Results show that different code smells affect differently activity effort. Yet, the size of the changes performed to solve the task impacts the effort of all activities more than code smells and file size. While code smells impact the editing and navigating effort more than file size, the file size impacts the reading and searching activities more than code smells. One major implication of these results is that if code smells indeed affect the effort of certain kinds of activities, it means that their effects are contingent on the type of maintenance task at hand, where some kinds of activities will become more predominant than others.","n":0.059}}},{"i":278,"$":{"0":{"v":"On evaluating the impact of the refactoring of architectural problems on software quality","n":0.277},"1":{"v":"We can improve software quality in different ways and by removing different kinds of problems. In this paper, we focus our attention on architectural problems, as architectural smells or antipatterns represent, we remove some of these problems through refactoring steps and we check the impact that the refactoring has on different quality metrics. In particular, we focus our attention on some Quality Indexes computed by four tools. These tools are used also for the detection of the architectural problems. We present the results and outline different issues related to the impact of the refactoring of these architectural problems on the Quality Indexes and the difficulties in the choice of the problems to be refactored.","n":0.094}}},{"i":279,"$":{"0":{"v":"Identifying and Evaluating Software Architecture Erosion","n":0.408}}},{"i":280,"$":{"0":{"v":"The SQALE Analysis Model: An Analysis Model Compliant with the Representation Condition for Assessing the Quality of Software Source Code","n":0.224},"1":{"v":"This paper presents the analysis model of the assessment method of software source code SQALE (Software Quality Assessment Based on Lifecycle Expectations). We explain what brought us to develop consolidation rules based in remediation indices. We describe how the analysis model can be implemented in practice.","n":0.147}}},{"i":281,"$":{"0":{"v":"Comparison and evaluation of code clone detection techniques and tools: A qualitative approach","n":0.277},"1":{"v":"Over the last decade many techniques and tools for software clone detection have been proposed. In this paper, we provide a qualitative comparison and evaluation of the current state-of-the-art in clone detection techniques and tools, and organize the large amount of information into a coherent conceptual framework. We begin with background concepts, a generic clone detection process and an overall taxonomy of current techniques and tools. We then classify, compare and evaluate the techniques and tools in two different dimensions. First, we classify and compare approaches based on a number of facets, each of which has a set of (possibly overlapping) attributes. Second, we qualitatively evaluate the classified techniques and tools with respect to a taxonomy of editing scenarios designed to model the creation of Type-1, Type-2, Type-3 and Type-4 clones. Finally, we provide examples of how one might use the results of this study to choose the most appropriate clone detection tool or technique in the context of a particular set of goals and constraints. The primary contributions of this paper are: (1) a schema for classifying clone detection techniques and tools and a classification of current clone detectors based on this schema, and (2) a taxonomy of editing scenarios that produce different clone types and a qualitative evaluation of current clone detectors based on this taxonomy.","n":0.068}}},{"i":282,"$":{"0":{"v":"NICAD: Accurate Detection of Near-Miss Intentional Clones Using Flexible Pretty-Printing and Code Normalization","n":0.277},"1":{"v":"This paper examines the effectiveness of a new language- specific parser-based but lightweight clone detection approach. Exploiting a novel application of a source transformation system, the method accurately finds near-miss clones using an efficient text line comparison technique. The transformation system assists the method in three ways. First, using agile parsing it provides user-specified flexible pretty- printing to remove noise, standardize formatting and break program statements into parts such that potential changes can be detected as simple linewise text differences. Second, it provides efficient flexible extraction of potential clones to be compared using island grammars and agile parsing to select granularities and enumerate potential clones. Third, using transformation rules it provides flexible code normalization to allow for local editing differences between similar code segments and filtering out of uninteresting parts of potential clones. In this paper we introduce the theory and practice of the framework and demonstrate its use in finding function clones in C code. Early experiments indicate that the method is capable of finding near-miss clones with high precision and recall, and with reasonable performance.","n":0.075}}},{"i":283,"$":{"0":{"v":"The Goal Question Metric Approach","n":0.447},"1":{"v":"As with any engineering discipline, software development requires a measurement mechanism for feedback and evaluation. Measurement is a mechanism for creating a corporate memory and an aid in answering a variety of questions associated with the enactment of any software process. It helps support project planning (e.g., How much will a new project cost?); it allows us to determine the strengths and weaknesses of the current processes and products (e.g., What is the frequency of certain types of errors?); it provides a rationale for adopting/refining techniques (e.g., What is the impact of the technique XX on the productivity of the projects?); it allows us to evaluate the quality of specific processes and products (e.g., What is the defect density in a specific system after deployment?). Measurement also helps, during the course of a project, to assess its progress, to take corrective action based on this assessment, and to evaluate the impact of such action.","n":0.081}}},{"i":284,"$":{"0":{"v":"How We Refactor, and How We Know It","n":0.354},"1":{"v":"Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.","n":0.075}}},{"i":285,"$":{"0":{"v":"An Exploratory Study of the Impact of Code Smells on Software Change-proneness","n":0.289},"1":{"v":"Code smells are poor implementation choices, thought to make object-oriented systems hard to maintain. In this study, we investigate if classes with code smells are more change-prone than classes without smells. Specifically, we test the general hypothesis: classes with code smells are not more change prone than other classes. We detect 29 code smells in 9 releases of Azureus and in 13 releases of Eclipse, and study the relation between classes with these code smells and class change-proneness. We show that, in almost all releases of Azureus and Eclipse, classes with code smells are more change-prone than others, and that specific smells are more correlated than others to change-proneness. These results justify a posteriori previous work on the specification and detection of code smells and could help focusing quality assurance and testing activities.","n":0.087}}},{"i":286,"$":{"0":{"v":"The evolution and impact of code smells: A case study of two open source systems","n":0.258},"1":{"v":"Code smells are design flaws in object-oriented designs that may lead to maintainability issues in the further evolution of the software system. This study focuses on the evolution of code smells within a system and their impact on the change behavior (change frequency and size). The study investigates two code smells, God Class and Shotgun Surgery, by analyzing the historical data over several years of development of two large scale open source systems. The detection of code smells in the evolution of those systems was performed by the application of an automated approach using detection strategies. The results show that we can identify different phases in the evolution of code smells during the system development and that code smell infected components exhibit a different change behavior. This information is useful for the identification of risk areas within a software system that need refactoring to assure a future positive evolution.","n":0.082}}},{"i":287,"$":{"0":{"v":"Identification of extract method refactoring opportunities for the decomposition of methods","n":0.302},"1":{"v":"The extraction of a code fragment into a separate method is one of the most widely performed refactoring activities, since it allows the decomposition of large and complex methods and can be used in combination with other code transformations for fixing a variety of design problems. Despite the significance of Extract Method refactoring towards code quality improvement, there is limited support for the identification of code fragments with distinct functionality that could be extracted into new methods. The goal of our approach is to automatically identify Extract Method refactoring opportunities which are related with the complete computation of a given variable (complete computation slice) and the statements affecting the state of a given object (object state slice). Moreover, a set of rules regarding the preservation of existing dependences is proposed that exclude refactoring opportunities corresponding to slices whose extraction could possibly cause a change in program behavior. The proposed approach has been evaluated regarding its ability to capture slices of code implementing a distinct functionality, its ability to resolve existing design flaws, its impact on the cohesion of the decomposed and extracted methods, and its ability to preserve program behavior. Moreover, precision and recall have been computed employing the refactoring opportunities found by independent evaluators in software that they developed as a golden set.","n":0.068}}},{"i":288,"$":{"0":{"v":"On the relation of refactorings and software defect prediction","n":0.333},"1":{"v":"This paper analyzes the influence of evolution activities such as refactoring on software defects. In a case study of five open source projects we used attributes of software evolution to predict defects in time periods of six months. We use versioning and issue tracking systems to extract 110 data mining features, which are separated into refactoring and non-refactoring related features. These features are used as input into classification algorithms that create prediction models for software defects. We found out that refactoring related features as well as non-refactoring related features lead to high quality prediction models. Additionally, we discovered that refactorings and defects have an inverse correlation: The number of software defects decreases, if the number of refactorings increased in the preceding time period. As a result, refactoring should be a significant part of both bug fixes and other evolutionary changes to reduce software defects.","n":0.083}}},{"i":289,"$":{"0":{"v":"Recommending automated extract method refactorings","n":0.447},"1":{"v":"Extract Method is a key refactoring for improving program comprehension. However, recent empirical research shows that refactoring tools designed to automate Extract Methods are often underused. To tackle this issue, we propose a novel approach to identify and rank Extract Method refactoring opportunities that are directly automated by IDE-based refactoring tools. Our approach aims to recommend new methods that hide structural dependencies that are rarely used by the remaining statements in the original method. We conducted an exploratory study to experiment and define the best strategies to compute the dependencies and the similarity measures used by the proposed approach. We also evaluated our approach in a sample of 81 extract method opportunities generated for JUnit and JHotDraw, achieving a precision of 48% (JUnit) and 38% (JHotDraw).","n":0.089}}},{"i":290,"$":{"0":{"v":"The Effect of GoF Design Patterns on Stability: A Case Study","n":0.302},"1":{"v":"Stability refers to a software system's resistance to the “ripple effect”, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected “shielding” of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern occurrences appear to be the least stable. The results can be used for assessing the benefits and liabilities of the use of patterns and for testing and refactoring prioritization, because less stable classes are expected to require more effort while testing, and urge for refactoring activities that would make them more resistant to change propagation.","n":0.071}}},{"i":291,"$":{"0":{"v":"Investigating the evolution of code smells in object-oriented systems","n":0.333},"1":{"v":"Software design problems are known and perceived under many different terms, such as code smells, flaws, non-compliance to design principles, violation of heuristics, excessive metric values and anti-patterns, signifying the importance of handling them in the construction and maintenance of software. Once a design problem is identified, it can be removed by applying an appropriate refactoring, improving in most cases several aspects of quality such as maintainability, comprehensibility and reusability. This paper, taking advantage of recent advances and tools in the identification of non-trivial code smells, explores the presence and evolution of such problems by analyzing past versions of code. Several interesting questions can be investigated such as whether the number of problems increases with the passage of software generations, whether problems vanish by time or only by targeted human intervention, whether code smells occur in the course of evolution of a module or exist right from the beginning and whether refactorings targeting at smell removal are frequent. In contrast to previous studies that investigate the application of refactorings in the history of a software project, we attempt to analyze the evolution from the point of view of the problems themselves. To this end, we classify smell evolution patterns distinguishing deliberate maintenance activities from the removal of design problems as a side effect of software evolution. Results are discussed for two open-source systems and four code smells.","n":0.066}}},{"i":292,"$":{"0":{"v":"Identification of refactoring opportunities introducing polymorphism","n":0.408},"1":{"v":"Polymorphism is one of the most important features offered by object-oriented programming languages, since it allows to extend/modify the behavior of a class without altering its source code, in accordance to the Open/Closed Principle. However, there is a lack of methods and tools for the identification of places in the code of an existing system that could benefit from the employment of polymorphism. In this paper we propose a technique that extracts refactoring suggestions introducing polymorphism. The approach ensures the behavior preservation of the code and the applicability of the refactoring suggestions based on the examination of a set of preconditions.","n":0.1}}},{"i":293,"$":{"0":{"v":"The Perception of Technical Debt in the Embedded Systems Domain: An Industrial Case Study","n":0.267},"1":{"v":"Technical Debt Management (TDM) has drawn the attention of software industries during the last years, including embedded systems. However, we currently lack an overview of how practitioners from this application domain perceive technical debt. To this end, we conducted a multiple case study in the embedded systems industry, to investigate: (a) the expected life-time of components that have TD, (b) the most frequently occurring types of TD in them, and (c) the significance of TD against run-time quality attributes. The case study was performed on seven embedded systems industries (telecommunications, printing, smart manufacturing, sensors, etc.) from five countries (Greece, Netherlands, Sweden, Austria, and Finland). The results of the case study suggest that: (a) maintainability is more seriously considered when the expected lifetime of components is larger than ten years, (b) the most frequent types of debt are test, architectural, and code debt, and (c) in embedded systems the run-time qualities are prioritized compared to design-time qualities that are usually associated with TD. The obtained results can be useful for both researchers and practitioners: the former can focus their research on the most industrially-relevant aspects of TD, whereas the latter can be informed about the most common types of TD and how to focus their TDM processes.","n":0.07}}},{"i":294,"$":{"0":{"v":"Identifying Extract Method Refactoring Opportunities Based on Functional Relevance","n":0.333},"1":{"v":"‘Extract Method’ is considered one of the most frequently applied and beneficial refactorings, since the corresponding Long Method smell is among the most common and persistent ones. Although Long Method is conceptually related to the implementation of diverse functionalities within a method, until now, this relationship has not been utilized while identifying refactoring opportunities. In this paper we introduce an approach (accompanied by a tool) that aims at identifying source code chunks that collaborate to provide a specific functionality, and propose their extraction as separate methods. The accuracy of the proposed approach has been empirically validated both in an industrial and an open-source setting. In the former case, the approach was capable of identifying functionally related statements within two industrial long methods (approx. 500 LoC each), with a recall rate of 93 percent. In the latter case, based on a comparative study on open-source data, our approach ranks better compared to two well-known techniques of the literature. To assist software engineers in the prioritization of the suggested refactoring opportunities the approach ranks them based on an estimate of their fitness for extraction. The provided ranking has been validated in both settings and proved to be strongly correlated with experts’ opinion.","n":0.071}}},{"i":295,"$":{"0":{"v":"Size and cohesion metrics as indicators of the long method bad smell: An empirical study","n":0.258},"1":{"v":"Source code bad smells are usually resolved through the application of well-defined solutions, i.e., refactorings. In the literature, software metrics are used as indicators of the existence and prioritization of resolving bad smells. In this paper, we focus on the long method smell (i.e. one of the most frequent and persistent bad smells) that can be resolved by the extract method refactoring. Until now, the identification of long methods or extract method opportunities has been performed based on cohesion, size or complexity metrics. However, the empirical validation of these metrics has exhibited relatively low accuracy with regard to their capacity to indicate the existence of long methods or extract method opportunities. Thus, we empirically explore the ability of size and cohesion metrics to predict the existence and the refactoring urgency of long method occurrences, through a case study on java open-source methods. The results of the study suggest that one size and four cohesion metrics are capable of characterizing the need and urgency for resolving the long method bad smell, with a higher accuracy compared to the previous studies. The obtained results are discussed by providing possible interpretations and implications to practitioners and researchers.","n":0.072}}},{"i":296,"$":{"0":{"v":"A mapping study on design-time quality attributes and metrics","n":0.333},"1":{"v":"Support to the quality attribute (QA) & metric selection process.Maintainability is the most studied QA for most domains and development phases.Quality attributes are usually assessed through a correlation to a single metric.Metrics are validated in empirical settings and may lack theoretical validity. Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.","n":0.065}}},{"i":297,"$":{"0":{"v":"An Embedded Multiple-Case Study on OSS Design Quality Assessment across Domains","n":0.302},"1":{"v":"Context: Investing on Open Source Software (OSS) as a \"code reuser\", involves certain risks, such as the difficulty in understanding the level of OSS design quality Aim: We investigate the levels of design quality of OSS projects, across different application domains. Method: We conducted a case study, which is the most fitting research method for observing a phenomenon in its real context, which is active for a long period of time, and for which variables cannot be controlled. Results: We present the values for seven design quality metrics of 546 OSS projects, as well as the statistically significant differences across application domains. Conclusions: The results of the study suggest that OSS application domains correlate with several design quality characteristics, in the sense that projects within one application domain appear to have similar levels of design quality. In addition to that, the results reveal application domains with high and low levels of design quality.","n":0.081}}},{"i":298,"$":{"0":{"v":"A Method for Assessing Class Change Proneness","n":0.378},"1":{"v":"Change proneness is a quality characteristic of software artifacts that represents their probability to change in the future due to: (a) evolving requirements, (b) bug fixing, or (c) ripple effects. In the literature, change proneness has been associated with many negative consequences along software evolution. For example, artifacts that are change-prone tend to produce more defects, and accumulate more technical debt. Therefore, identifying and monitoring modules of the system that are change-prone is of paramount importance. Assessing change proneness requires information from two sources: (a) the history of changes in the artifact as a proxy of how frequently the artifact itself is changing, and (b) the source code structure that affects the probability of a change being propagated among artifacts. In this paper, we propose a method for assessing the change proneness of classes based on the two aforementioned information sources. To validate the proposed approach, we performed a case study on five open-source projects. Specifically, we compared the accuracy of the proposed approach to the use of other software metrics and change history to assess change proneness, based on the 1061-1998 IEEE Standard on Software Measurement. The results of the case study suggest that the proposed method is the most accurate and reliable assessor of change proneness. The high accuracy of the method suggests that the method and accompanying tool can effectively aid practitioners during software maintenance and evolution.","n":0.066}}},{"i":299,"$":{"0":{"v":"Elaborating security requirements by construction of intentional anti-models","n":0.354},"1":{"v":"Caring for security at requirements engineering time is a message that has finally received some attention recently. However, it is not yet very clear how to achieve this systematically through the various stages of the requirements engineering process. The paper presents a constructive approach to the modeling, specification and analysis of application-specific security requirements. The method is based on a goal-oriented framework for generating and resolving obstacles to goal satisfaction. The extended framework addresses malicious obstacles (called anti-goals) set up by attackers to threaten security goals. Threat trees are built systematically through anti-goal refinement until leaf nodes are derived that are either software vulnerabilities observable by the attacker or anti-requirements implementable by this attacker. New security requirements are then obtained as countermeasures by application of threat resolution operators to the specification of the anti-requirements and vulnerabilities revealed by the analysis. The paper also introduces formal epistemic specification constructs and patterns that may be used to support a formal derivation and analysis process. The method is illustrated on a Web-based banking system for which subtle attacks have been reported recently.","n":0.075}}},{"i":300,"$":{"0":{"v":"Sustainability design and software: the karlskrona manifesto","n":0.378},"1":{"v":"Sustainability has emerged as a broad concern for society. Many engineering disciplines have been grappling with challenges in how we sustain technical, social and ecological systems. In the software engineering community, for example, maintainability has been a concern for a long time. But too often, these issues are treated in isolation from one another. Misperceptions among practitioners and research communities persist, rooted in a lack of coherent understanding of sustainability, and how it relates to software systems research and practice. This article presents a cross-disciplinary initiative to create a common ground and a point of reference for the global community of research and practice in software and sustainability, to be used for effectively communicating key issues, goals, values and principles of sustainability design for software-intensive systems. The centrepiece of this effort is the Karlskrona Manifesto for Sustainability Design, a vehicle for a much needed conversation about sustainability within and beyond the software community, and an articulation of the fundamental principles underpinning design choices that affect sustainability. We describe the motivation for developing this manifesto, including some considerations of the genre of the manifesto as well as the dynamics of its creation. We illustrate the collaborative reflective writing process and present the current edition of the manifesto itself. We assess immediate implications and applications of the articulated principles, compare these to current practice, and suggest future steps.","n":0.067}}},{"i":301,"$":{"0":{"v":"A generic model for sustainability with process- and product-specific instances","n":0.316},"1":{"v":"Motivation: Software systems as we know them often have a economic purpose and/or fulfill human or social needs of their users. The economic purpose is analysed by economy itself; the latter goals are analysed in software engineering by user-centric techniques, such as service orientation. Yet, as software systems have an impact on the environment, environmental sustainability should be supported as a major goal for software development projects.   Problem: Without applicable guidance, sustainability remains an untangible ideal. Therefore, we need a definition and a concrete decomposition of sustainability to relate it to software systems development. It is not sufficient to analyse environmental sustainbility on its own, but its interplay with other aspects in order to define appropriate actions and understand their effects.   Principal idea: We analyse the dimensions of sustainability, their values with respective indicators, and activities to support them. These elements compose a conceptual model that allows for analysing and constructing actions both for a company or a product point of view.   Contribution: We propose a generic sustainability model with instances for companies and projects from various case studies. We thus enable analysis, support and assessment of environmental sustainability in software engineering.","n":0.072}}},{"i":302,"$":{"0":{"v":"A review of the state of the practice in requirements modeling","n":0.302},"1":{"v":"A field study of ten organizations was conducted to determine their current practices on definition, interpretation, analysis, and use of the requirements for their software systems and products. The field study consisted of a series of in-depth, structured interviews with practitioners of various kinds. The findings of this study are summarized, and the implications for improving practice either by organizational and methodological interventions or by introducing new technology are explained. >","n":0.119}}},{"i":303,"$":{"0":{"v":"Towards a definition of sustainability in and for software engineering","n":0.316},"1":{"v":"Sustainability is not supported by traditional software engineering methods. This lack of support leads to inefficient efforts to address sustainability or complete omission of this important concept. Defining and developing adequate support requires a commonly accepted definition of what sustainability means in and for software engineering.   We contribute a description of the aspects of sustainability in software engineering.","n":0.131}}},{"i":304,"$":{"0":{"v":"Value-Based Software Engineering: Overview and Agenda","n":0.408},"1":{"v":"Much of current software engineering practice and research is done in a value-neutral setting, in which every requirement, use case, object, test case, and defect is equally important. However, most studies of the critical success factors distinguishing successful from failed software projects find that the primary critical success factors lie in the value domain. The value-based software engineering (VBSE) agenda discussed in this chapter and exemplified in the other chapters involves integrating value considerations into the full range of existing and emerging software engineering principles and practices. The chapter then summarizes the primary components of the agenda: value-based requirements engineering, architecting, design and development, verification and validation, planning and control, risk management, quality management, people management, and an underlying theory of VBSE. It concludes with a global road map for realizing the benefits of VBSE.","n":0.086}}},{"i":305,"$":{"0":{"v":"Systematic mapping study on software engineering for sustainability (SE4S)","n":0.333},"1":{"v":"Background/Context: The objective of achieving higher sustainability in our lifestyles by information and communication technology has lead to a plethora of research activities in related fields. Consequently, Software Engineering for Sustainability (SE4S) has developed as an active area of research. Objective/Aim: Though SE4S gained much attention over the past few years and has resulted in a number of contributions, there is only one rigorous survey of the field. We follow up on this systematic mapping study from 2012 with a more in-depth overview of the status of research, as most work has been conducted in the last 4 years. Method: The applied method is a systematic mapping study through which we investigate which contributions were made, which knowledge areas are most explored, and which research type facets have been used, to distill a common understanding of the state-of-the-art in SE4S. Results: We contribute an overview of current research topics and trends, and their distribution according to the research type facet and the application domains. Furthermore, we aggregate the topics into clusters and list proposed and used methods, frameworks, and tools. Conclusion: The research map shows that impact currently is limited to few knowledge areas and there is need for a future roadmap to fill the gaps.","n":0.07}}},{"i":306,"$":{"0":{"v":"Portfolio Selection: Efficient Diversification of Investments","n":0.408}}},{"i":307,"$":{"0":{"v":"The Blind Men and the Elephant: Towards an Empirical Evaluation Framework for Software Sustainability","n":0.267},"1":{"v":"Software sustainability has been identified as one of the key challenges in the development of scientific and engineering software as we move towards new paradigms of research and computing infrastructures. However, it is suggested that sustainability is not well understood within the software engineering community, which can led to ineffective and inefficient efforts to address the concept or result in its complete omission from the software system. This paper proposes a definition of software sustainability and considers how it can be measured empirically in the design and engineering process of software systems.","n":0.104}}},{"i":308,"$":{"0":{"v":"Making Architecture Design Decisions: An Economic Approach","n":0.378},"1":{"v":"Abstract : The resources available to build any system are finite. The decisions involved in building any nontrivial system are complex and typically involve many stakeholders, many requirements, and many technical decisions. The stakeholders have an interest in ensuring that good design decisions are made decisions that meet their technical objectives and their tolerance for risk. These decisions should, as much as possible, maximize the benefit that the system provides and minimize its cost. The Cost Benefit Analysis Method (CBAM) was created to provide some structure to this decision-making process. The CBAM analyzes architectural decisions from the perspectives of cost, benefit, schedule, and risk. While the CBAM does not make decisions for the stakeholders, it does serve as a tool to inform managers and to structure the inquiry so that rational decisions can be made. This report describes the steps of the CBAM and its application to a real-world system.","n":0.082}}},{"i":309,"$":{"0":{"v":"GuideArch: guiding the exploration of architectural solution space under uncertainty","n":0.316},"1":{"v":"A system's early architectural decisions impact its properties (e.g., scalability, dependability) as well as stakeholder concerns (e.g., cost, time to delivery). Choices made early on are both difficult and costly to change, and thus it is paramount that the engineer gets them \"right\". This leads to a paradox, as in early design, the engineer is often forced to make these decisions under uncertainty, i.e., not knowing the precise impact of those decisions on the various concerns. How could the engineer make the \"right\" choices in such circumstances? This is precisely the question we have tackled in this paper. We present GuideArch, a framework aimed at quantitative exploration of the architectural solution space under uncertainty. It provides techniques founded on fuzzy math that help the engineer with making informed decisions.","n":0.088}}},{"i":310,"$":{"0":{"v":"Sustainability Debt: A Metaphor to Support Sustainability Design Decisions","n":0.333},"1":{"v":"Sustainability, the capacity to endure, is fundamental for the societies on our planet. Despite its increasing recognition in software engineering, it remains difficult to assess the delayed systemic effects of decisions taken in requirements engineering and systems design. To support this difficult task, this paper introduces the concept of sustainability debt. The metaphor helps in the discovery, documentation, and communication of sustainability issues in requirements engineering. We build on the existing metaphor of technical debt and extend it to four other dimensions of sustainability to help think about sustainability-aware software systems engineering. We highlight the meaning of debt in each dimension and the relationships between those dimensions. Finally, we discuss the use of the metaphor and explore how it can help us to design sustainability-aware software intensive systems.","n":0.088}}},{"i":311,"$":{"0":{"v":"Using Economic Considerations to Choose Among Architecture Design Alternatives","n":0.333},"1":{"v":"Abstract : The software architecture forms an essential part of a complex software-intensive system. Architecture design decision-making involves addressing tradeoffs due to the presence of economic constraints. The problem is to develop a process that helps a designer choose amongst architectural options, during both initial design and its subsequent periods of upgrade, while being constrained to finite resources. To address this need for better decision-making, we have developed a method for performing economic modeling of software systems, centered on an analysis of their architecture. We call this method the Cost Benefit Analysis Method (CBAM). The CBAM incorporates the costs and benefits of architectural design decisions and provides an effective means of making such decisions. The CBAM provides a structured integrated assessment of the technical and economic issues and architectural decisions. The CBAM utilizes techniques in decision analysis, optimization, and statistics to help software architects characterize their uncertainty and choose a subset of changes that should be implemented from a larger set of alternatives. We also report on the application of this method to a real world case study.","n":0.075}}},{"i":312,"$":{"0":{"v":"The software value map—an exhaustive collection of value aspects for the development of software intensive products","n":0.25},"1":{"v":"In software intensive products such as cars or telecom systems, software has traditionally been associated with cost, and there has been no real perception of its value in relation to the entire product offering. However, because software is becoming a larger part of the main competitive advantage, driving innovation and product differentiation, hardware is becoming more standardized, thus the valuation of software is becoming critical. In existing literature, several value constructs and corresponding valuation/measurement solutions needed for making decisions about software product development are presented. However, the contributions are often isolated with respect to a certain perspective such as focusing on product's internal or external quality aspects only. Consequently, a complete view of value constructs relevant from different perspectives required for making decisions about software product development is missing. This paper presents a consolidated view of the software value concept utilizing the major perspectives and introduces a software value map. The created value map was evaluated through an industry case study through the development of impact evaluation patterns, which were subsequently used by professionals in industry, and experiences gathered. During industry evaluation, practitioners found substantial benefits of having a consolidated, vastly improved, and extended value aspect's view of software. Copyright © 2012 John Wiley & Sons, Ltd.","n":0.07}}},{"i":313,"$":{"0":{"v":"The Potential of Portfolio Analysis in Guiding Software Decisions","n":0.333},"1":{"v":"Developing a complex software system involves decisions about how to allocate a limited resource budget among a collection of costly software alternatives (such as technologies or analysis techniques) that have uncertain future benefits. Very little quantitative guidance is currently available to make these decisions. We suggest that these allocation problems are naturally viewed in the powerful portfolio selection framework of financial investment theory. We view each software activity as an investment opportunity (or security), the benefit from the activity as the return on investment, and the allocation problem as one of selecting the “optimal” portfolio of securities.","n":0.102}}},{"i":314,"$":{"0":{"v":"Modelling Architectural Decisions under Changing Requirements","n":0.408},"1":{"v":"One of the approaches for documenting software architecture is to treat it as a set of architectural design decisions. Such decisions are always made in the context of requirements that must be fulfilled and in the context of decisions that were made before. Currently, models for representing architectural decisions are mainly concentrated on showing the decision making process of the initial architectural design. However, decisions that have been made in such a process may need to be changed during further evolution and maintenance of the software architecture, typically in response to the new or changed requirements. A graphical modelling notation for documenting architectural decisions (Maps of Architectural Decisions) has been developed by our team. In this paper, it is presented how this notation could be used to model architectural decisions under changing requirements. It is proposed how one decision change could be effectively propagated through the rest of the architectural decision model and how a rigorous and tool-supported process of updating such models could be organized.","n":0.078}}},{"i":315,"$":{"0":{"v":"The “Physics” of Notations: Toward a Scientific Basis for Constructing Visual Notations in Software Engineering","n":0.258},"1":{"v":"Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.","n":0.068}}},{"i":316,"$":{"0":{"v":"Model-Driven Software Engineering in Practice","n":0.447},"1":{"v":"This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The book is targeted to a diverse set of readers, spanning: professionals, CTOs, CIOs, and team managers that need to have a bird's eye vision on the matter, so as to take the appropriate decisions when it comes to choosing the best development techniques for their company or team; software analysts, developers, or designers that expect to use MDSE for improving everyday work productivity, either by applying the basic modeling techniques and notations or by defining new domain-specific modeling languages and applying end-to-end MDSE practices in the software factory; and academic teachers and students to address undergrad and postgrad courses on MDSE. In addition to the contents of the book, more resources are provided on the book's website http://www.mdse-book.com/, including the examples presented in the book. Table of Contents: Introduction / MDSE Principles / MDSE Use Cases / Model-Driven Architecture (MDA) / Integration of MDSE in your Development Process / Modeling Languages at a Glance / Developing your Own Modeling Language / Model-to-Model Transformations / Model-to-Text Transformations / Managing Models / Summary","n":0.05}}},{"i":317,"$":{"0":{"v":"Theoretical and practical issues in evaluating the quality of conceptual models: current state and future directions","n":0.25},"1":{"v":"An international standard has now been established for evaluating the quality of software products. However there is no equivalent standard for evaluating the quality of conceptual models. While a range of quality frameworks have been proposed in the literature, none of these have been widely accepted in practice and none has emerged as a potential standard. As a result, conceptual models continue to be evaluated in practice in an ad hoc way, based on common sense, subjective opinions and experience. For conceptual modelling to progress from an \"art\" to an engineering discipline, quality standards need to be defined, agreed and applied in practice. This paper conducts a review of research in conceptual model quality and identifies the major theoretical and practical issues which need to be addressed. We consider how conceptual model quality frameworks can be structured, how they can be developed, how they can be empirically validated and how to achieve acceptance in practice. We argue that the current proliferation of quality frameworks is counterproductive to the progress of the field, and that researchers and practitioners should work together to establish a common standard (or standards) for conceptual model quality. Finally, we describe some initial efforts towards developing a common standard for data model quality, which may provide a model for future standardisation efforts.","n":0.068}}},{"i":318,"$":{"0":{"v":"Managing technical debt","n":0.577}}},{"i":319,"$":{"0":{"v":"Definitions and approaches to model quality in model-based software development - A review of literature","n":0.258},"1":{"v":"More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.","n":0.1}}},{"i":320,"$":{"0":{"v":"Managing Model Quality in UML-Based Software Development","n":0.378},"1":{"v":"With the advent of UML and MDA, models play an increasingly important role in software development. Hence, the management of the quality of models is of key importance for completing projects succesfully. However, existing approaches towards software quality focus on the implementation and execution of systems. These existing quality models cannot be straightforwardly mapped to the domain of UML models as source code and models differ in several essential ways (level of abstraction, precision, completeness and consistency). In this paper we present a quality model for managing UML-based software development. This model enables identifying the need for actions for quality improvement already in early stages of the life-cycle. Early actions for quality improvement are less resource intensive and, hence, less cost intensive than later actions. We discuss our experiences in applying the quality model to several industrial case studies. Finally we present a tool that visualizes our quality model. This tool helps in relating management level quality data to detailed data about specific quality subcharacteristics","n":0.078}}},{"i":321,"$":{"0":{"v":"A tool environment for quality assurance based on the Eclipse Modeling Framework","n":0.289},"1":{"v":"The paradigm of model-based software development has become more and more popular since it promises an increase in the efficiency and quality of software development. Following this paradigm, models become primary artifacts in the software development process. Therefore, software quality and quality assurance frequently leads back to the quality and quality assurance of the involved models. In our approach, we propose a model quality assurance process that can be adapted to project-specific and domain-specific needs. This process is based on static model analysis using model metrics and model smells. Based on the outcome of the model analysis, appropriate model refactoring steps can be performed. In this paper, we present a tool environment conveniently supporting the proposed model quality assurance process. In particular, the presented tools support metrics reporting, smell detection, and refactoring for models being based on the Eclipse Modeling Framework, a widely used open source technology in model-based software development.","n":0.081}}},{"i":322,"$":{"0":{"v":"Developing a Quality Framework for Model-Driven Engineering","n":0.378},"1":{"v":"This paper presents some related work on quality frameworks and requirements for evaluating them. It also discusses characteristics of model-driven engineering that are important when building a quality framework, such as its use of models in several stages of development and maintenance, generation of other artifacts from models and its multi-abstraction level approach that requires consistency and traceability. We present a 7-step process on how to define a quality framework that is adapted to model-driven engineering, and which integrates quality engineering with quality evaluation. As an example, the framework is applied on transformation quality. We maintain that the transformation process and transformation mapping should be discussed separately, as they require different approaches, and suggest quality goals, quality-carrying properties to achieve the quality goals and methods for evaluating these properties.","n":0.088}}},{"i":323,"$":{"0":{"v":"Towards an Operationalization of the Physics of Notations for the Analysis of Visual Languages","n":0.267},"1":{"v":"We attempt to validate the conceptual framework \"Physics of Notation\" PoN as a means for analysing visual languages by applying it to UML Use Case Diagrams. We discover that the PoN, in its current form, is neither precise nor comprehensive enough to be applied in an objective way to analyse practical visual software engineering notations. We propose an operationalization of a part of the PoN, highlight conceptual shortcomings of the PoN, and explore ways to address them.","n":0.114}}},{"i":324,"$":{"0":{"v":"Conceptual-Model Programming: A Manifesto","n":0.5},"1":{"v":"In order to promote conceptual-model programming (CMP), we present these CMP articles. We hold these articles to be the defining principles for model-complete software development.","n":0.2}}},{"i":325,"$":{"0":{"v":"Patterns of Data Modeling","n":0.5},"1":{"v":"Best-selling author and database expert with more than 25 years of experience modeling application and enterprise data, Dr. Michael Blaha provides tried and tested data model patterns, to help readers avoid common modeling mistakes and unnecessary frustration on their way to building effective data models. Unlike the typical methodology book, Patterns of Data Modeling provides advanced techniques for those who have mastered the basics. Recognizing that database representation sets the path for software, determines its flexibility, affects its quality, and influences whether it succeeds or fails, the text focuses on databases rather than programming. It is one of the first books to apply the popular patterns perspective to database systems and data models. It offers practical advice on the core aspects of applications and provides authoritative coverage of mathematical templates, antipatterns, archetypes, identity, canonical models, and relational database design.","n":0.085}}},{"i":326,"$":{"0":{"v":"A quality model for conceptual models of MDD environments","n":0.333},"1":{"v":"In Model-Driven Development (MDD) processes, models are key artifacts that are used as input for code generation. Therefore, it is very important to evaluate the quality of these input models in order to obtain high-quality software products. The detection of defects is a promising technique to evaluate software quality, which is emerging as a suitable alternative for MDD processes. The detection of defects in conceptual models is usually manually performed. However, since current MDD standards and technologies allow both the specification of metamodels to represent conceptual models and the implementation of model transformations to automate the generation of final software products, it is possible to automate defect detection from the defined conceptual models. This paper presents a quality model that not only encapsulates defect types that are related to conceptual models but also takes advantage of current standards in order to automate defect detection in MDD environments.","n":0.082}}},{"i":327,"$":{"0":{"v":"On the Uncertainty of Technical Debt Measurements","n":0.378},"1":{"v":"Measurements are subject to random and systematic errors, yet almost no study in software engineering makes significant efforts in reporting these errors. Whilst established statistical techniques are well suited for the analysis of random error, such techniques are not valid in the presence of systematic errors. We propose a departure from de- facto methods of reporting results of technical debt measurements for more rigorous techniques drawn from established methods in the physical sciences. This line of inquiry focuses on technical debt calculations; however it can be generalized to quantitative software engineering studies. We pose research questions and seek answers to the identification of systematic errors in metric-based tools, as well as the reporting of such errors when subjected to propagation. Exploratory investigations reveal that the techniques suggested allow for the comparison of uncertainties that come from differing sources. We suggest the study of error propagation of technical debt is a worthwhile subject for further research and techniques seeded from the physical sciences present viable options that can be used in software engineering reporting.","n":0.076}}},{"i":328,"$":{"0":{"v":"Quality of Models","n":0.577},"1":{"v":"In this chapter, we describe a framework for understanding quality in conceptual modelling (SEQUAL), including examples of means to achieve model quality of different levels (such as tool functionality and modelling techniques being appropriate for the development of models of high quality). Quality is discussed on seven levels: physical, empirical, syntactic, semantic, pragmatic, social, and deontic. How different quality types build upon each other is also indicated.","n":0.122}}},{"i":329,"$":{"0":{"v":"Measuring and visualising the quality of models","n":0.378},"1":{"v":"The quality of graphical software or business process models is influenced by several aspects such as correctness of the formal syntax, understandability or compliance to existing rules. Motivated by a standardised software quality model, we discuss characteristics and subcharacteristics of model quality and sugest measures for those quality (sub)characteristics. Also, we extended SonarQube, a well-known tool for aggregating and visualising different measures for software quality such that it can now be used with repositories of business process models as well. This allows assessing the quality of a collection of models in the same way that is already well-established for assessing the quality of software code. Given the fact that models are early software development artifacts (and can even be executable and thus become a part of a software product), such a quality control can lead to the detection of possible problems in the early phases of the software development process.","n":0.082}}},{"i":330,"$":{"0":{"v":"Towards the Reconstruction and Evaluation of Conceptual Model Quality Discourses – Methodical Framework and Application in the Context of Model Understandability","n":0.218},"1":{"v":"Within the information systems (IS) discipline conceptual models have gained tremendous importance in the past years. Different approaches for systematic model quality evaluation have emerged. However, these approaches are based on different understandings, definitions as well as operationalizations of the term “model quality”. In this article we refrain from conceptualizing and operationalizing model quality a priori. In contrast, assuming that the determination of model quality and appropriate criteria are negotiated in a discourse between modelers and model users based on their different perspectives, we develop a methodical framework for the critical reconstruction and evaluation of conceptual model quality discourses in order to identify relevant model quality criteria and understandings. Our method is exemplarily applied for the reconstruction of the discourse on the quality criterion model understandability based on relevant laboratory experiments. This application shows that many research results on model understandability are hardly comparable due to their different basic assumptions and should preferably be interpreted based on a methodical reconstruction of underlying understandings.","n":0.078}}},{"i":331,"$":{"0":{"v":"2013 IEEE 1st International Workshop on Communicating Business Process and Software Models : quality, understandability, and maintainability (CPSM), September 23, 2013, Eindhoven, The Netherlands)","n":0.204}}},{"i":332,"$":{"0":{"v":"Evolution of software in automated production systems","n":0.378},"1":{"v":"Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted","n":0.073}}},{"i":333,"$":{"0":{"v":"Modularity and architecture of PLC-based software for automated production systems: An analysis in industrial companies","n":0.258},"1":{"v":"Adaptive and flexible production systems require modular, reusable software as a prerequisite for their long-term life cycle of up to 50 years. We introduce a benchmark process to measure software maturity for industrial control software of automated production systems.","n":0.16}}},{"i":334,"$":{"0":{"v":"Design for future: managed software evolution","n":0.408},"1":{"v":"Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future--Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority programme is integrated and focused research in software engineering to develop methods for the continuous evolution of software and software/hardware systems for making systems adaptable to changing requirements and environments. For evaluation, we focus on two specific application domains: information systems and production systems in automation engineering. In particular two joint case studies from these application domains promote close collaborations among the individual projects of the priority programme. We consider several research topics that are of common interest, for instance co-evolution of models and implementation code, of models and tests, and among various types of models. Another research topic of common interest are run-time models to automatically synchronise software systems with their abstract models through continuous system monitoring. Both concepts, co-evolution and run-time models contribute to our vision to which we refer to as knowledge carrying software. We consider this as a major need for a long life of such software systems.","n":0.06}}},{"i":335,"$":{"0":{"v":"Analysis and state of the art of modules in industrial automation","n":0.302},"1":{"v":"The benefit of modular concepts in plant automation is seen ambivalent. On one hand it offers advantages, on the other hand it also sets requirements on the system structure as well as designers’ discipline. The main reason to use modularity in systems design for automation applications are reusability and reduction of complexity, but up to now modular concepts are rare in plant automation. This paper analyses the reasons and proposes measures and draft solutions. An analysis of the  work flow and the working results of some companies in several branches show different proposals of modularity. These different proposals in production and process engineering are integrated in one model and represent different perspectives of an integrated system.","n":0.093}}},{"i":336,"$":{"0":{"v":"Analysis framework for evaluating PLC software: An application of Semantic Web technologies","n":0.289},"1":{"v":"Control software in the automated production systems domain is becoming increasingly complex. As a consequence, appropriate methods to improve control software quality need to be identified. Although sophisticated frameworks and tools for software quality improvement exist in the computer science domain, support for industrial control software development is still limited. As a first step towards addressing this issue, this paper proposes an analysis framework that aims at evaluating control software by means of Semantic Web technologies.","n":0.115}}},{"i":337,"$":{"0":{"v":"Software metrics: good, bad and missing","n":0.408},"1":{"v":"The software industry is an embarrassment when it comes to measurement and metrics. Many software managers and practitioners, including tenured academics in software engineering and computer science, seem to know little or nothing about these topics. Many of the measurements found in the software literature are not used with enough precision to replicate the author's findings-a canon of scientific writing in other fields. Several of the most widely used software metrics have been proved unworkable, yet they continue to show up in books, encyclopedias, and refereed journals. So long as these invalid metrics are used carelessly, there can be no true software engineering, only a kind of amateurish craft that uses rough approximations instead of precise measurement. The paper considers three significant and widely used software metrics that are invalid under various conditions: lines of code or LOC metrics, software science or Halstead metrics, and the cost-per-defect metric. Fortunately, two metrics that actually generate useful information-complexity metrics and function-point metrics-are growing in use and importance. >","n":0.078}}},{"i":338,"$":{"0":{"v":"Strategic Management of Technical Debt: Tutorial Synopsis","n":0.378},"1":{"v":"The technical debt metaphor acknowledges that software development teams sometimes accept compromises in a system in one dimension (for example, modularity) to meet an urgent demand in some other dimension (for example, a deadline), and that such compromises incur a \"debt\". If not properly managed the interest on this debt may continue to accrue, severely hampering system stability and quality and impacting the team's ability to deliver enhancements at a pace that satisfies business needs. Although unmanaged debt can have disastrous results, strategically managed debt can help businesses and organizations take advantage of time-sensitive opportunities, fulfill market needs and acquire stakeholder feedback. Because architecture has such leverage within the overall development life cycle, strategic management of architectural debt is of primary importance. Some aspects of technical debt -- but not all technical debt -- affects product quality. This tutorial introduces the technical debt metaphor, the various types of technical debt, and in particular structural or architectural debt, the techniques for measuring and communicating this technical debt, and its relationship with software quality, both internal and external quality.","n":0.075}}},{"i":339,"$":{"0":{"v":"International Organization for Standardization (ISO)","n":0.447}}},{"i":340,"$":{"0":{"v":"A Systematic Literature Review on Fault Prediction Performance in Software Engineering","n":0.302},"1":{"v":"Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.","n":0.073}}},{"i":341,"$":{"0":{"v":"Object-oriented design heuristics","n":0.577},"1":{"v":"From the Publisher:\r\nObject-Oriented Design Heuristics offers insight into object-oriented design improvement. The more than sixty guidelines presented in this book are language-independent and allow you to rate the integrity of a software design. The heuristics are not written as hard and fast rules; they are meant to serve as warning mechanisms which allow the flexibility of ignoring the heuristic as necessary. This tutorial-based approach, born out of the author's extensive experience developing software, teaching thousands of students, and critiquing designs in a variety of domains, allows you to apply the guidelines in a personalized manner. The heuristics cover important topics ranging from classes and objects (with emphasis on their relationships including association, uses, containment, and both single and multiple inheritance) to physical object-oriented design. You will gain an understanding of the synergy that exists between design heuristics and the popular concept of design patterns; heuristics can highlight a problem in one facet of a design while patterns can provide the solution. Programmers of all levels will find value in this book. The newcomer will discover a fast track to understanding the concepts of object-oriented programming. At the same time, experienced programmers seeking to strengthen their object-oriented development efforts will appreciate the insightful analysis. In short, with Object-Oriented Design Heuristics as your guide, you have the tools to become a better software developer.","n":0.067}}},{"i":342,"$":{"0":{"v":"Software Project Dynamics: An Integrated Approach","n":0.408},"1":{"v":"Presents a scientific model of the software project management process based on focused field interviews and includes a detailed case study that was conducted to test the model. Covers human resource management, software production, controlling and planning.","n":0.164}}},{"i":343,"$":{"0":{"v":"Managerial use of metrics for object-oriented software: an exploratory analysis","n":0.316},"1":{"v":"With the increasing use of object-oriented methods in new software development, there is a growing need to both document and improve current practice in object-oriented design and development. In response to this need, a number of researchers have developed various metrics for object-oriented systems as proposed aids to the management of these systems. In this research, an analysis of a set of metrics proposed by Chidamber and Kemerer (1994) is performed in order to assess their usefulness for practising managers. First, an informal introduction to the metrics is provided by way of an extended example of their managerial use. Second, exploratory analyses of empirical data relating the metrics to productivity, rework effort and design effort on three commercial object-oriented systems are provided. The empirical results suggest that the metrics provide significant explanatory power for variations in these economic variables, over and above that provided by traditional measures, such as size in lines of code, and after controlling for the effects of individual developers.","n":0.078}}},{"i":344,"$":{"0":{"v":"A Quantitative Investigation of the Acceptable Risk Levels of Object-Oriented Metrics in Open-Source Systems","n":0.267},"1":{"v":"Object-oriented metrics have been validated empirically as measures of design complexity. These metrics can be used to mitigate potential problems in the software complexity. However, there are few studies that were conducted to formulate the guidelines, represented as threshold values, to interpret the complexity of the software design using metrics. Classes can be clustered into low and high risk levels using threshold values. In this paper, we use a statistical model, derived from the logistic regression, to identify threshold values for the Chidamber and Kemerer (CK) metrics. The methodology is validated empirically on a large open-source system-the Eclipse project. The empirical results indicate that the CK metrics have threshold effects at various risk levels. We have validated the use of these thresholds on the next release of the Eclipse project-Version 2.1-using decision trees. In addition, the selected threshold values were more accurate than those were selected based on either intuitive perspectives or on data distribution parameters. Furthermore, the proposed model can be exploited to find the risk level for an arbitrary threshold value. These findings suggest that there is a relationship between risk levels and object-oriented metrics and that risk levels can be used to identify threshold effects.","n":0.071}}},{"i":345,"$":{"0":{"v":"A probability-based approach for measuring external attributes of software artifacts","n":0.316},"1":{"v":"The quantification of so-called external software attributes, which are the product qualities with real relevance for developers and users, has often been problematic. This paper introduces a proposal for quantifying external software attributes in a unified way. The basic idea is that external software attributes can be quantified by means of probabilities. As a consequence, external software attributes can be estimated via probabilistic models, and not directly measured via software measures. This paper discusses the reasons underlying the proposals and shows the pitfalls related to using measures for external software attributes. We also show that the theoretical bases for our approach can be found in so-called “probability representations,” a part of Measurement Theory that has not yet been used in Software Engineering Measurement. By taking the definition and estimation of reliability as reference, we show that other external software attributes can be defined and modeled by a probability-based approach.","n":0.082}}},{"i":346,"$":{"0":{"v":"Deriving models of software fault-proneness","n":0.447},"1":{"v":"The effectiveness of the software testing process is a key issue for meeting the increasing demand of quality without augmenting the overall costs of software development. The estimation of software fault-proneness is important for assessing costs and quality and thus better planning and tuning the testing process. Unfortunately, no general techniques are available for estimating software fault-proneness and the distribution of faults to identify the correct level of test for the required quality. Although software complexity and testing thoroughness are intuitively related to the costs of quality assurance and the quality of the final product, single software metrics and coverage criteria provide limited help in planning the testing process and assuring the required quality.By using logistic regression, this paper shows how models can be built that relate software measures and software fault-proneness for classes of homogeneous software products. It also proposes the use of cross-validation for selecting valid models even for small data sets.The early results show that it is possible to build statistical models based on historical data for estimating fault-proneness of software modules before testing, and thus better planning and monitoring the testing activities.","n":0.073}}},{"i":347,"$":{"0":{"v":"Measurement theory with applications to decisionmaking, utility, and the social sciences","n":0.302},"1":{"v":"Introduction 1. Relations 2. Fundamental Measurement, Derived Measurement, and the Uniqueness Problem 3. Three Representation Problems: Ordinal, Extensive, and Difference Measurement 4. Applications to Psychophysical Scaling 5. Project Structures 6. Nontransitive Indifference, Probabilistic Consistency, and Measurement without Numbers 7. Decisionmaking under Risk or Uncertainty 8. Subjective Probability.","n":0.146}}},{"i":348,"$":{"0":{"v":"Evaluating inheritance depth on the maintainability of object-oriented software","n":0.333},"1":{"v":"This empirical research was undertaken as part of a multi-method programme of research to investigate unsupported claims made of object-oriented technology. A series of subject-based laboratory experiments, including an internal replication, tested the effect of inheritance depth on the maintainability of object-oriented software. Subjects were timed performing identical maintenance tasks on object-oriented software with a hierarchy of three levels of inheritance depth and equivalent object-based software with no inheritance. This was then replicated with more experienced subjects. In a second experiment of similar design, subjects were timed performing identical maintenance tasks on object-oriented software with a hierarchy of five levels of inheritance depth and the equivalent object-based software. The collected data showed that subjects maintaining object-oriented software with three levels of inheritance depth performed the maintenance tasks significantly quicker than those maintaining equivalent object-based software with no inheritance. In contrast, subjects maintaining the object-oriented software with five levels of inheritance depth took longer, on average, than the subjects maintaining the equivalent object-based software (although statistical significance was not obtained). Subjects' source code solutions and debriefing questionnaires provided some evidence suggesting subjects began to experience difficulties with the deeper inheritance hierarchy. It is not at all obvious that object-oriented software is going to be more maintainable in the long run. These findings are sufficiently important that attempts to verify the results should be made by independent researchers.","n":0.067}}},{"i":349,"$":{"0":{"v":"Investigation of logistic regression as a discriminant of software quality","n":0.316},"1":{"v":"Investigates the possibility that logistic regression functions (LRFs), when used in combination with Boolean discriminant functions (BDFs), which we had previously developed, would improve the quality classification ability of BDFs when used alone; this was found to be the case. When the union of a BDF and LRF was used to classify quality, the predictive accuracy of quality and inspection cost was improved over that of using either function alone for the Space Shuttle. Also, the LRFs proved useful for ranking the quality of modules in a build. The significance of these results is that very high-quality classification accuracy (1.25% error) can be obtained while reducing the inspection cost incurred in achieving high quality. This is particularly important for safety-critical systems. Because the methods are general and not particular to the Shuttle, they could be applied to other domains. A key part of the LRF development was a method for identifying the critical value (i.e. threshold) that could discriminate between high and low quality, and at the same time constrain the cost of inspection to a reasonable value.","n":0.075}}},{"i":350,"$":{"0":{"v":"Predicting object-oriented class reuse-proneness using internal quality attributes","n":0.354},"1":{"v":"Class reuse-proneness is the likelihood that a class can be reused and is a key quality characteristic in object-oriented design. Because it is clearly impossible to know with certainty when, where, how, and how often a class will be reused, this likelihood can only be estimated when a class is developed. At that stage, the internal quality attributes of a class such as cohesion, coupling, and size can be measured. In this paper, we empirically study the ability of 29 internal class quality measures, individually and in combination, to estimate class reuse-proneness. Specifically, we take into account both class inheritance and class instantiation, which are two ways in which a class can be reused. Our results show that most of the considered measures are predictors for the considered reuse-proneness attributes to some degree. The capability of the considered internal quality measures to predict class reuse-proneness is enhanced when the measures are combined using an optimized multivariate statistical model. The results also show that the size and coupling attributes of a class have positive impacts on its reuse-proneness via inheritance and instantiation. The cohesion of a class has a negative impact on its inheritance reuse-proneness and a positive impact on its instantiation reuse-proneness. The overall results of the empirical study show that software developers can use a number of predictors to assess the reuse-proneness of classes and possibly improve the reuse-proneness of the classes by controlling their internal quality attributes. Our results contribute to the evaluation of the quality of a class, but a comprehensive evaluation of the quality of a class should take into account many more qualities. Also, not all classes in a software system are built with reuse-proneness in mind, so our results should be applied only to those that are designed and implemented to be reusable.","n":0.058}}},{"i":351,"$":{"0":{"v":"Managing Technical Debt: Insights from Recent Empirical Evidence","n":0.354},"1":{"v":"Technical debt refers to maintenance obligations that software teams accumulate as a result of their actions. Empirical research has led researchers to suggest three dimensions along which software development teams should map their technical-debt metrics: customer satisfaction needs, reliability needs, and the probability of technology disruption.","n":0.147}}},{"i":352,"$":{"0":{"v":"Risk-averse slope-based thresholds: Definition and empirical evaluation","n":0.378},"1":{"v":"Abstract    Background . Practical use of a measure  X  for an internal attribute (e.g., size, complexity, cohesion, coupling) of software modules often requires setting a threshold on  X , to make decisions as to which modules may be estimated to be potentially faulty. To keep quality under control, practitioners may want to set a threshold on  X  to identify “early symptoms” of possible faultiness of those modules that should be closely monitored and possibly modified.   Objective . We propose and evaluate a risk-averse approach to setting thresholds on  X  based on properties of the slope of statistically significant fault-proneness models, to identify “early symptoms” of module faultiness.   Method . To this end, we introduce four ways for setting thresholds on  X . First, we use the value of  X  where a fault-proneness model curve changes direction the most, i.e., it has maximum convexity. Then, we use the values of  X  where the slope has specific values: one-half of the maximum slope, and the median and mean slope in the interval between minimum and maximum slopes.   Results . We provide the theoretical underpinnings for our approach and we apply our approach to data from the PROMISE repository by building Binary Logistic and Probit regression fault-proneness models. The empirical study shows that the proposed thresholds effectively detect “early symptoms” of module faultiness, while achieving a level of accuracy in classifying faulty modules close to other usual fault-proneness thresholds.   Conclusions . Our method can be practically used for setting “early symptom” thresholds based on evidence captured by statistically significant models. Also, the thresholds depend on characteristics of the models alone, so project managers do not need to devise the thresholds themselves. The proposed thresholds correspond to increasing risk levels, so project managers can choose the threshold that best suits their needs in a risk-averse framework.","n":0.058}}},{"i":353,"$":{"0":{"v":"Improving usefulness of software quality classification models based on Boolean discriminant functions","n":0.289},"1":{"v":"BDF (Boolean discriminant functions) are an attractive technique for software quality estimation. Software quality classification models based on BDF provide stringent rules for classifying not fault-prone modules (nfp), thereby predicting a large number of modules as fp. Such models are practically not useful from software quality assurance and software management points of view. This is because, given the large number of modules predicted as fp, project management will face a difficult task of deploying, cost-effectively, the always-limited reliability improvement resources to all the fp modules. This paper proposes the use of generalized Boolean discriminant functions (GBDF) as a solution for improving the practical and managerial usefulness of classification models based on BDF. In addition, the use of GBDF avoids the need to build complex hybrid classification models in order to improve usefulness of models based on BDF. A case study of a full-scale industrial software system is presented to illustrate the promising results obtained from using the proposed classification technique using GBDF.","n":0.079}}},{"i":354,"$":{"0":{"v":"Towards Probabilistic Models to Predict Availability, Accessibility and Successability of Web Services","n":0.289},"1":{"v":"Web Services are gaining increasing attention as programming components and so is their quality. The external qualities of Web Services (i.e., qualities that are perceived by their users) such as the OASIS sub-quality factors Availability, Accessibility, and Successability can only be measured at late stages after the deployment and the provisioning of the Web Service. This may necessitate expensive rework if the targeted levels of qualities are not satisfactorily met. A reliable prediction of the values of the external qualities at early phases during development may totally remove the need for reworking and hence save valuable resources. In this paper, we describe an approach for building and empirically evaluating probabilistic prediction models for the Web Services external sub-quality factors Availability, Accessibility, and Successability based on internal static and dynamic quality measures (e.g., Cyclomatic Complexity and Distinct Method Invocations). A methodology was established that involves the collection of a set of predefined quality measures and then performing regression analysis to identify any correlation between them and the above mentioned external qualities. For this purpose, a framework for data collection and evaluation was designed, implemented and tested. The results of the preliminary evaluation of the framework showed that it is feasible to collect all the data points necessary for the regression analysis and model building activities. We are currently working towards adding about 18 more Web Services to our testbed in order to carry out a wider controlled experiment and then to build possibly accurate probabilistic prediction models for Availability, Accessibility, and Successability.","n":0.063}}},{"i":355,"$":{"0":{"v":"The PageRank Citation Ranking : Bringing Order to the Web","n":0.316},"1":{"v":"The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.","n":0.103}}},{"i":356,"$":{"0":{"v":"Agile Software Development with SCRUM","n":0.447},"1":{"v":"From the Publisher:\r\n&#147Agile development methods are key to the future of flexible software systems. Scrum is one of the vangards of the new way to buy and manage software development when business conditions are changing. This book distills both the theory and practive and is essential reading for anyone who needs to cope with software in a volatile world.&#148\r\n&#151Martin Fowler, industry consultant and CTO, ThoughtWorks\r\n\r\n&#147Most executives today are not happy with their organization's ability to deliver systems at reasonable cost and timeframes. Yet, if pressed, they will admit that they don't think their software developers are not competent. If it's not the engineers, then what is it that prevents fast development at reasonable cost? Scrum gives the answer to the question and the solution to the problem.&#148\r\n&#151Alan Buffington, industry consultant, former Present, Fidelity Systems Company\r\n\r\nArguably the most important book about managing technology and systems development efforts, this book describes building systems using the deceptively simple process, Scrum. Readers will come to understand a new approach to systems development projects that cuts through the ocmplexity and ambiguity of complex, emergent requiremetns and unstable technology to iteratively and quickly produce quality software. \r\n\r\nBENEFITS Learn how to immediately start producing software incrementally regardless of existing engineering practices or methodologies\r\nLearn how to simplify the implementation of Agile processes\r\nLearn how to simplify XP implementation through a Scrum wrapper\r\nLearn why Agile processes work and how to manage them\r\nUnderstand the theoretical underpinnings of Agile processes","n":0.065}}},{"i":357,"$":{"0":{"v":"Automated Software Testing: Introduction, Management, and Performance","n":0.378},"1":{"v":"Preface. I. WHAT IS AUTOMATED TESTING? 1. The Birth and Evolution of Automated Testing. Automated Testing. Background on Software Testing. The Automated Test Life-Cycle Methodology (ATLM). Decision to Automate Test. Test Tool Acquisition. Automated Testing Introduction Phase. Test Planning, Design, and Development. Execution and Management of Tests. Test Program Review and Assessment. ATLM's Role in the Software Testing Universe. ATLM Relationship to System Development Life Cycle. Test Maturity Model (TMM)-Augmented by Automated SoftwareTesting Maturity. Test Automation Development. Test Effort. Software Testing Careers. 2. Decision to Automate Test. Overcoming False Expectations for Automated Testing. Automatic Test Plan Generation. Test Tool Fits All. Immediate Test Effort Reduction. Immediate Schedule Reduction. Tool Ease of Use. Universal Application of Test Automation. One Hundred Percent Test Coverage. Benefits of Automated Testing. Production of a Reliable System. Improvement of the Quality of the Test Effort. Reduction of Test Effort and Minimization of Schedule. Acquiring Management Support. Test Tool Proposal. 3. Automated Test Tool Selection and Evaluation. Organization's Systems Engineering Environment. Third-Party Input from Management, Staff, and Customers andUsers. Tool Criteria Reflecting the Systems Engineering Environment. Level of Software Quality. Help Desk Problem Reports. Budget Constraints. Types of Tests. Long-Term Investment Considerations. Test Tool Process. Avoiding Shortcuts. Tools That Support the Testing Life Cycle. Business Analysis Phase Tools. Requirements Definition Phase Tools. Tools for the Analysis and Design Phase. Programming Phase Tools. Metrics Tools. Other Testing Life-Cycle Support Tools. Testing Phase Tools. Test Tool Research. Improvement Opportunities. Evaluation Domain Definition. Hands-On Tool Evaluation. Evaluation Report. License Agreement. II. INTRODUCTION OF AUTOMATED TESTING TO A PROJECT. 4. Automated Testing Introduction Process. Test Process Analysis. Process Review. Goals and Objectives of Testing. Case Study: Test Objectives and Strategies. Test Strategies. Test Tool Consideration. Review of Project-Specific System Requirements. Application-Under-Test Overview. Review of Project Schedule. Test Tool Compatibility Check. Demonstration of the Tool to the Project Team. Test Tool Support Profile. Review of Training Requirements 5. Test Team Management. Organizational Structure of a Test Team. Stovepipe Test Team. Centralized Test Team. IV&V Test Team. Systems Methodology and Test Team. Test Team Summary. Test Program Tasks. Test Effort Sizing. Test Team Sizing Methods: An Overview. Development Ratio Method. Percentage Method. Test Procedure Method. Task Planning Method. Test Effort Sizing Factors. Test Engineer Recruiting. Test Engineer Qualities. Test Team Composition. Job Requisition. Recruiting Activities. Locating Test Engineers. Test Engineer Interviews. Distinguishing the Best Candidate. Roles and Responsibilities. III. TEST PLANNING AND PREPARATION. 6. Test Planning: Smart Application of Testing. Test Planning Activities. Test Program Scope. System Description. Critical/High-Risk Functions. Test Goals, Objectives, and Strategies. Test Tools. Test Program Parameters. Verification Methods. Test Requirements Definition. Test Requirements Management. Requirements Management Tools. Assessing the Test Requirements Risk. Prioritization of Tests. Requirements Traceability Matrix. Test Program Events, Activities, and Documentation. Events. Activities. Documentation. The Test Environment. Test Environment Preparations. Test Environment Integration and Setup. The Test Plan. Test Completion/Acceptance Criteria. Sample Test Plan. 7. Test Analysis and Design. Test Requirements Analysis. Development-Level Test Analysis (Structural Approach). System-Level Test Analysis (Behavioral Approach). Test Program Design. Test Program Design Models. White-Box Techniques (Development-Level Tests). Black-Box Techniques (System-Level Tests). Test Design Documentation. Test Procedure Design. Test Procedure Definition. Automated Versus Manual Test Analysis. Automated Test Design Standards. Case Study: Naming Conventions. Manual Test Design Guidelines. Detailed Test Design. Test Data Requirements. 8. Test Development. Test Development Architecture. Technical Environment. Environment Readiness Checks. Automation Reuse Analysis. Test Procedure Development/Execution Schedule. Modularity-Relationship Analysis. Explanation of the Sample Modularity- Relationship Matrix. Calibration of the Test Tool. Compatibility Work-Around Solutions. Case Study: Incompatibility Work-Around Solution. Manual Execution of Test Procedures. Test Procedure Inspections-Peer Reviews. Test Procedure Configuration Management. Test Development Guidelines. Design-to-Development Transiftion. Reusable Test Procedures. Case Study: Navigation Using Tabs or Mouse Clicks. Case Study: Testing Bitmaps Using a Capture/Playback Tool. Maintainable Test Procedures. Case Study: Automating Documentation. Case Study: Automated Random Testing. Other Guidelines. Automation Infrastructure. Table-Driven Test Automation. PC Environment Automated Setup Script. Automated Recording Options. Login Function. Exit Function. Navigation. Verifying GUI Standards. Smoke Test. Case Study: Smoke Test Application. Error-Logging Routine. Help Function Verification Script. Timed Message Boxes Function. Advanced Math Functions. IV. TEST EXECUTION AND REVIEW. 9. Test Execution. Executing/Evaluating Test Phases. Unit Test Execution and Evaluation. Integration Test Execution and Evaluation. System Test Execution and Evaluation. Test Results Analysis of Regression Tests. User Acceptance Test Execution and Evaluation. Defect Tracking and New Build Process. Defect Life-Cycle Model. Test Program Status Tracking. Earned Value Management System. Case Study: System Test Status Tracking. Test Metrics Collection and Analysis. 10. Test Program Review and Assessment. Test Program Lessons Learned-Corrective Actions andImprovement Activity. Test Program Return on Investment. Case Study: TestProgram Return on Investment. Case Study: Quantify Tool Return on Investment. V. APPENDIXES. A. How to Test Requirements. Requirements Testing Approach. Abstract. The Quality Gateway. Make the Requirement Measurable. Quantifiable Requirements. Nonquantifiable Requirements. Keeping Track. Coherency and Consistency. Completeness. Relevance. Requirement or Solution? Stakeholder Value. Traceability. Order in a Disorderly World. Conclusions. References. B. Tools That Support the Automated Testing Life Cycle. Introduction. Business Analysis Phase. Business Modeling Tools. Configuration Management Tools. Defect Tracking Tools. Technical Review Management. Documentation Generators. Requirements Definition Phase. Requirements Management Tools. Requirements Verifiers. Use Case Generators. Analysis and Design Phase. Visual Modeling Tools. Structure Charts, Flowcharts, and Sequence Diagrams. Test Procedure Generators. Programming Phase. Syntax Checkers/Debuggers. Memory Leak and Runtime Error Detection Tools. Code Checkers. Static and Dynamic Analyzers. Unit Testing Tools. Metrics Tools. Code (Test) Coverage Analyzers and Code Instrumentors. Usability Measurement Tools. Testing Support Tools. Test Data Generators. File Comparison Tools. Simulation Tools. Testing Phase. Test Management Tools. Network Testing Tools. GUI Application Testing Tools. Load/Performance Testing Tools. Web Testing Tools. Year 24710 Testing Tools. Other Test Tool Vendors. C. Test Engineer Development. Technical Skills Stage. Test Process Stage. Team Effort Stage. Technical Stewardship Stage. Test/Project Management Stage. Business/Product Management Stage. D. Sample Test Plan. Introduction. Purpose. Background. System Overview. Applicable Documents. Master Schedule. Roles and Responsibilities. Project Organization. Project Roles and Responsibilities. Test Task Structure. Test Team Resources. Test Program. Scope. Test Approach. Test Strategies. Automated Tools. Qualification Methods. Test Requirements. Test Design. Test Development. Test Environment. Test Environment Configuration. Test Data. Test Execution. Test Program Reporting. Test Program Metrics. Defect Tracking. Configuration Management. Detailed Test Schedule. Appendixes. D.A Test Procedure Development Guidelines. D.B Test Verification Summary and Matrix. D.C Test Procedures and Test Scripts E. Best Practices. Documented Process. Managing Expectations. Pilot Project. Test Tool Compatibility Checks. Test Tool Upgrades. Baselined System Setup and Configuration. Software Installations in the Test Environment Baseline. Overall Test Program Objectives. Keep Automation Simple. Test Procedure Design and Development Standards. Automated Versus Manual Test Analysis. Reuse Analysis. Test Team Communication with Other Teams. Schedule Compatibility. Customer Involvement. Defect Documentation and Reporting. Automated Test Advocates and Experts. Test Team Assignments. User Group Participation. Test Tool Improvement Suggestions. Become a Beta Testing Site. Specialty Topic Experts. 0201432870T04062001","n":0.03}}},{"i":358,"$":{"0":{"v":"Eclipse integrated development environment","n":0.5},"1":{"v":"This chapter explores the Eclipse integrated development environment (IDE). Eclipse is a kind of universal tool platform—an open, extensible IDE for anything and nothing in particular. It provides a feature-rich development environment that allows the developer to efficiently create tools that integrate seamlessly into the Eclipse platform. Eclipse itself is not an IDE, but is rather an open platform for developing IDEs and rich client applications. Eclipse is the most professionally executed open source project. It is well thought out and meticulously implemented. While it has tremendous power and flexibility, the most common operations are relatively simple and intuitive. The documentation, while still reference in nature and not tutorial, is nevertheless readable, and accurate for the most part. The plug-in mechanism makes Eclipse infinitely extensible. This opens up opportunities for both open source and commercial extensions to address a wide range of applications. The chapter also lists a couple of repositories of Eclipse plug-ins.","n":0.081}}},{"i":359,"$":{"0":{"v":"Manual vs. Automated Vulnerability Assessment: A Case Study","n":0.354},"1":{"v":"The dream of every software development team is to assess the security of their software using only a tool. In this paper, we attempt to evaluate and quantify the effectiveness of automated source code anal- ysis tools by comparing such tools to the results of an in-depth manual evaluation of the same system. We present our manual vulnerability as- sessment methodology, and the results of applying this to a major piece of software. We then analyze the same software using two commercial products, Coverity Prevent and Fortify SCA, that perform static source code analysis. These tools found only a few of the fifteen serious vulner- abilities discovered in the manual assessment, with none of the problems found by these tools requiring a deep understanding of the code. Each tool reported thousands of defects that required human inspection, with only a small number being security related. And, of this small number of security-related defects, there did not appear to be any that indicated significant vulnerabilities beyond those found by the manual assessment.","n":0.076}}},{"i":360,"$":{"0":{"v":"Software Metrics","n":0.707}}},{"i":361,"$":{"0":{"v":"Software Assessment: Reliability, Safety, Testability","n":0.447},"1":{"v":"From the Publisher:\r\nComprised of techniques to guide those responsible for creating computer programs that are integrated into engineering designs and are essential for the device to function in a reliable manner. Emphasizes how to assess the qualities of reliability and safety in software design and development. Discusses design for testability (DFT) to demonstrate how the software engineer should devise and code a program to maximize testability. Includes a chapter on the generation of test cases to support testing and testability analysis.","n":0.111}}},{"i":362,"$":{"0":{"v":"Deriving metric thresholds from benchmark data","n":0.408},"1":{"v":"A wide variety of software metrics have been proposed and a broad range of tools is available to measure them. However, the effective use of software metrics is hindered by the lack of meaningful thresholds. Thresholds have been proposed for a few metrics only, mostly based on expert opinion and a small number of observations.","n":0.135}}},{"i":363,"$":{"0":{"v":"SquaRE The next generation of the ISO/IEC 9126 and 14598 international standards series on software product quality","n":0.243},"1":{"v":"In this presentation, overview of current 9126 and 14598 series are introduced followed by some problems to be solved. Then the concept and architecture of SQuaRE are discussed. Finally, new parts, i.e., Quality in Use, Quality Requirements, and Elementally Metrics are explained.","n":0.154}}},{"i":364,"$":{"0":{"v":"Standardized code quality benchmarking for improving software maintainability","n":0.354},"1":{"v":"We provide an overview of the approach developed by the Software Improvement Group for code analysis and quality consulting focused on software maintainability. The approach uses a standardized measurement model based on the ISO/IEC 9126 definition of maintainability and source code metrics. Procedural standardization in evaluation projects further enhances the comparability of results. Individual assessments are stored in a repository that allows any system at hand to be compared to the industry-wide state of the art in code quality and maintainability. When a minimum level of software maintainability is reached, the certification body of TUV Informationstechnik GmbH issues a Trusted Product Maintainability certificate for the software product.","n":0.097}}},{"i":365,"$":{"0":{"v":"Evaluating the impact of object-oriented design on software quality","n":0.333},"1":{"v":"Describes the results of a study where the impact of object-oriented (OO) design on software quality characteristics is experimentally evaluated. A suite of Metrics for OO Design (MOOD) was adopted to measure the use of OO design mechanisms. Data collected on the development of eight small-sized information management systems based on identical requirements were used to assess the referred impact. Data obtained in this experiment show how OO design mechanisms such as inheritance, polymorphism, information hiding and coupling, can influence quality characteristics like reliability or maintainability. Some predictive models based on OO design metrics are also presented.","n":0.102}}},{"i":366,"$":{"0":{"v":"Experimental assessment of the effect of inheritance on the maintainability of object-oriented systems","n":0.277},"1":{"v":"In this paper, we describe an empirical investigation into the modifiability and understandability of object-oriented (OO) software. A controlled experiment was conducted to establish the effects of varying levels of inheritance on understandability and modifiability. The software used in this experiment consisted of a C++ system without any inheritance and a corresponding version containing three levels of inheritance, as well as a second larger C++ system without inheritance and a corresponding version with five levels of inheritance. For both of the systems, the application modelled a database for a University personnel system. A number of statistical hypotheses were tested. Results indicated that the systems without inheritance were easier to modify than the corresponding systems containing three or five levels of inheritance. Also, it was easier to understand the system without inheritance than a corresponding version containing three levels of inheritance. Results also indicated that larger systems are equally difficult to understand whether or not they contain inheritance. The results contained in this paper highlight the need for further empirical investigations in this area, particularly into the benefits of using inheritance.","n":0.075}}},{"i":367,"$":{"0":{"v":"Backfiring: converting lines of code to function points","n":0.354},"1":{"v":"The availability of empirical data from projects that use both function-point and lines-of-code metrics has led to a useful technique called backfiring. Backfiring is the direct mathematical conversion of LOC data into equivalent function-point data. Because the backfiring equations are bidirectional, they also provide a powerful way of sizing, or predicting, source-code volume for any known programming language or combination of languages. The function-point metric, invented by A.J. Albrecht of IBM in the middle 1970s, is a synthetic metric derived by a weighted formula that includes five elements: inputs, outputs, logical files, inquiries, and interfaces. IBM put it into the public domain in 1979, and its use spread rapidly, particularly after the formation of the International Function Point Users Group (IFPUG) in the mid-1980s. By then, hundreds of software projects had been measured using both function points and lines of source code. Since an application's function-point total is independent of the source code, this dual analysis has led to several important discoveries. >","n":0.078}}},{"i":368,"$":{"0":{"v":"Indicators of Issue Handling Efficiency and their Relation to Software Maintainability","n":0.302}}},{"i":369,"$":{"0":{"v":"The economics of technical advance.","n":0.447}}},{"i":370,"$":{"0":{"v":"The Capital Asset Pricing Model: Theory and Evidence","n":0.354},"1":{"v":"The capital asset pricing model (CAPM) of William Sharpe (1964) and John Lintner (1965) marks the birth of asset pricing theory (resulting in a Nobel Prize for Sharpe in 1990). Before their breakthrough, there were no asset pricing models built from first principles about the nature of tastes and investment opportunities and with clear testable predictions about risk and return. Four decades later, the CAPM is still widely used in applications, such as estimating the cost of equity capital for firms and evaluating the performance of managed portfolios. And it is the centerpiece, indeed often the only asset pricing model taught in MBA level investment courses. \r\n\r\nThe attraction of the CAPM is its powerfully simple logic and intuitively pleasing predictions about how to measure risk and about the relation between expected return and risk. Unfortunately, perhaps because of its simplicity, the empirical record of the model is poor - poor enough to invalidate the way it is used in applications. The model's empirical problems may reflect true failings. (It is, after all, just a model.) But they may also be due to shortcomings of the empirical tests, most notably, poor proxies for the market portfolio of invested wealth, which plays a central role in the model's predictions. We argue, however, that if the market proxy problem invalidates tests of the model, it also invalidates most applications, which typically borrow the market proxies used in empirical tests. \r\n\r\nFor perspective on the CAPM's predictions about risk and expected return, we begin with a brief summary of its logic. We then review the history of empirical work on the model and what it says about shortcomings of the CAPM that pose challenges to be explained by more complicated models.","n":0.059}}},{"i":371,"$":{"0":{"v":"Performance Measurement in a Downside Risk Framework","n":0.378}}},{"i":372,"$":{"0":{"v":"Data Analysis Using Regression and Multilevel/Hierarchical Models","n":0.378},"1":{"v":"Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.","n":0.092}}},{"i":373,"$":{"0":{"v":"When is nearest neighbor meaningful","n":0.447},"1":{"v":"We explore the effect of dimensionality on the nearest neighbor problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10-15 dimensions. These results should not be interpreted to mean that high-dimensional indexing is never meaningful; we illustrate this point by identifying some high-dimensional workloads for which this effect does not occur. However, our results do emphasize that the methodology used almost universally in the database literature to evaluate high-dimensional indexing techniques is flawed, and should be modified. In particular, most such techniques proposed in the literature are not evaluated versus simple linear scan, and are evaluated over workloads for which nearest neighbor is not meaningful. Often, even the reported experiments, when analyzed carefully, show that linear scan would outperform the techniques being proposed on the workloads studied in high (10-15) dimensionality!.","n":0.074}}},{"i":374,"$":{"0":{"v":"An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression","n":0.354},"1":{"v":"Abstract Nonparametric regression is a set of techniques for estimating a regression curve without making strong assumptions about the shape of the true regression function. These techniques are therefore useful for building and checking parametric models, as well as for data description. Kernel and nearest-neighbor regression estimators are local versions of univariate location estimators, and so they can readily be introduced to beginning students and consulting clients who are familiar with such summaries as the sample mean and median.","n":0.113}}},{"i":375,"$":{"0":{"v":"Modeling bug report quality","n":0.5},"1":{"v":"Software developers spend a significant portion of their resources handling user-submitted bug reports. For software that is widely deployed, the number of bug reports typically outstrips the resources available to triage them. As a result, some reports may be dealt with too slowly or not at all.    We present a descriptive model of bug report quality based on a statistical analysis of surface features of over 27,000 publicly available bug reports for the Mozilla Firefox project. The model predicts whether a bug report is triaged within a given amount of time. Our analysis of this model has implications for bug reporting systems and suggests features that should be emphasized when composing bug reports.    We evaluate our model empirically based on its hypothetical performance as an automatic filter of incoming bug reports. Our results show that our model performs significantly better than chance in terms of precision and recall. In addition, we show that our modelcan reduce the overall cost of software maintenance in a setting where the average cost of addressing a bug report is more than 2% of the cost of ignoring an important bug report.","n":0.073}}},{"i":376,"$":{"0":{"v":"Predicting the fix time of bugs","n":0.408},"1":{"v":"Two important questions concerning the coordination of development effort are which bugs to fix first and how long it takes to fix them. In this paper we investigate empirically the relationships between bug report attributes and the time to fix. The objective is to compute prediction models that can be used to recommend whether a new bug should and will be fixed fast or will take more time for resolution. We examine in detail if attributes of a bug report can be used to build such a recommender system. We use decision tree analysis to compute and 10-fold cross validation to test prediction models. We explore prediction models in a series of empirical studies with bug report data of six systems of the three open source projects Eclipse, Mozilla, and Gnome. Results show that our models perform significantly better than random classification. For example, fast fixed Eclipse Platform bugs were classified correctly with a precision of 0.654 and a recall of 0.692. We also show that the inclusion of postsubmission bug report data of up to one month can further improve prediction models. Preprint accepted for publication in the Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering (RSSE), Cape Town (South Africa) May, 2010","n":0.069}}},{"i":377,"$":{"0":{"v":"Predicting Eclipse Bug Lifetimes","n":0.5},"1":{"v":"In non-trivial software development projects planning and allocation of resources is an important and difficult task. Estimation of work time to fix a bug is commonly used to support this process. This research explores the viability of using data mining tools to predict the time to fix a bug given only the basic information known at the beginning of a bug's lifetime. To address this question, a historical portion of the Eclipse Bugzilla database is used for modeling and predicting bug lifetimes. A bug history transformation process is described and several data mining models are built and tested. Interesting behaviours derived from the models are documented. The models can correctly predict up to 34.9% of the bugs into a discretized log scaled lifetime class.","n":0.09}}},{"i":378,"$":{"0":{"v":"Predicting bug-fixing time: an empirical study of commercial software projects","n":0.316},"1":{"v":"For a large and evolving software system, the project team could receive many bug reports over a long period of time. It is important to achieve a quantitative understanding of bug-fixing time. The ability to predict bug-fixing time can help a project team better estimate software maintenance efforts and better manage software projects. In this paper, we perform an empirical study of bug-fixing time for three CA Technologies projects. We propose a Markov-based method for predicting the number of bugs that will be fixed in future. For a given number of defects, we propose a method for estimating the total amount of time required to fix them based on the empirical distribution of bug-fixing time derived from historical data. For a given bug report, we can also construct a classification model to predict slow or quick fix (e.g., below or above a time threshold). We evaluate our methods using real maintenance data from three CA Technologies projects. The results show that the proposed methods are effective.","n":0.078}}},{"i":379,"$":{"0":{"v":"Technical Debt","n":0.707},"1":{"v":"In Episode 224 of Software Engineering Radio, Eberhard Wolff and Sven Johann discuss technical debt and how to handle it.","n":0.224}}},{"i":380,"$":{"0":{"v":"On predicting the time taken to correct bug reports in open source projects","n":0.277},"1":{"v":"Existing studies on the maintenance of open source projects focus primarily on the analyses of the overall maintenance of the projects and less on specific categories like the corrective maintenance. This paper presents results from an empirical study of bug reports from an open source project, identifies user participation in the corrective maintenance process through bug reports, and constructs a model to predict the corrective maintenance effort for the project in terms of the time taken to correct faults. Our study focuses on 72482 bug reports from over nine releases of Ubuntu, a popular Linux distribution. We present three main results 1) 95% of the bug reports are corrected by people participating in groups of size ranging from 1 to 8 people, 2) there is a strong linear relationship (about 92%) between the the number of people participating in a bug report and the time taken to correct it, 3) a linear model can be used to predict the time taken to correct bug reports.","n":0.078}}},{"i":381,"$":{"0":{"v":"Studying the fix-time for bugs in large open source projects","n":0.316},"1":{"v":"Background: Bug fixing lies at the core of most software maintenance efforts. Most prior studies examine the effort needed to fix a bug (fix-effort). However, the effort needed to fix a bug may not correlate with the calendar time needed to fix it (fix-time). For example, the fix-time for bugs with low fix-effort may be long if they are considered to be of low priority.   Aims: We study the fix-time for bugs in large open source projects.   Method: We study the fix-time along three dimensions: (1) the location of the bug (e.g., which component), (2) the reporter of the bug, and (3) the description of the bug. Using these three dimensions and their associated attributes, we examine the fix-time for bugs in two large open source projects: Eclipse and Mozilla, using a random forest classifier.   Results: We show that we can correctly classify ~65% of the time the fix-time for bugs in these projects. We perform a sensitivity analysis to identify the most important attributes in each dimension. We find that the time of the filing of a bug and its location are the most important attributes in the Mozilla project for determining the fix-time of a bug. On the other hand, the fix-time in the Eclipse project is highly dependant on the severity of the bug. Surprisingly, the priority of the bug is not an important attribute when determining the fix-time for a bug in both projects.   Conclusion: Attributes affecting the fix-time vary between projects and vary over time within the same project.","n":0.063}}},{"i":382,"$":{"0":{"v":"Filtering Bug Reports for Fix-Time Analysis","n":0.408},"1":{"v":"Several studies have experimented with data mining algorithms to predict the fix-time of reported bugs. Unfortunately, the fix-times as reported in typical open-source cases are heavily skewed with a significant amount of reports registering fix-times less than a few minutes. Consequently, we propose to include an additional filtering step to improve the quality of the underlying data in order to gain better results. Using a small-scale replication of a previously published bug fix-time prediction experiment, we show that the additional filtering of reported bugs indeed improves the outcome of the results.","n":0.105}}},{"i":383,"$":{"0":{"v":"Refactoring for Software Design Smells: Managing Technical Debt","n":0.354},"1":{"v":"Awareness of design smells - indicators of common design problems - helps developers or software engineers understand mistakes made while designing, what design principles were overlooked or misapplied, and what principles need to be applied properly to address those smells through refactoring. Developers and software engineers may \"know\" principles and patterns, but are not aware of the \"smells\" that exist in their design because of wrong or mis-application of principles or patterns. These smells tend to contribute heavily to technical debt - further time owed to fix projects thought to be complete - and need to be addressed via proper refactoring. Refactoring for Software Design Smells presents 25 structural design smells, their role in identifying design issues, and potential refactoring solutions. Organized across common areas of software design, each smell is presented with diagrams and examples illustrating the poor design practices and the problems that result, creating a catalog of nuggets of readily usable information that developers or engineers can apply in their projects. The authors distill their research and experience as consultants and trainers, providing insights that have been used to improve refactoring and reduce the time and costs of managing software projects. Along the way they recount anecdotes from actual projects on which the relevant smell helped address a design issue. Contains a comprehensive catalog of 25 structural design smells (organized around four fundamental design principles) that contribute to technical debt in software projects Presents a unique naming scheme for smells that helps understand the cause of a smell as well as points toward its potential refactoring Includes illustrative examples that showcase the poor design practices underlying a smell and the problems that result Covers pragmatic techniques for refactoring design smells to manage technical debt and to create and maintain high-quality software in practice Presents insightful anecdotes and case studies drawn from the trenches of real-world projects","n":0.057}}},{"i":384,"$":{"0":{"v":"Index-based code clone detection: incremental, distributed, scalable","n":0.378},"1":{"v":"Although numerous different clone detection approaches have been proposed to date, not a single one is both incremental and scalable to very large code bases. They thus cannot provide real-time cloning information for clone management of very large systems. We present a novel, index-based clone detection algorithm for type 1 and 2 clones that is both incremental and scalable. It enables a new generation of clone management tools that provide real-time cloning information for very large software. We report on several case studies that show both its suitability for real-time clone detection and its scalability: on 42 MLOC of Eclipse code, average time to retrieve all clones for a file was below 1 second; on 100 machines, detection of all clones in 73 MLOC was completed in 36 minutes.","n":0.088}}},{"i":385,"$":{"0":{"v":"Chapter 1 – Technical Debt","n":0.447},"1":{"v":"Technical debt is the debt that accrues when developers knowingly or unknowingly make wrong or nonoptimal design decisions. Technical debt is akin to financial debt. Allowing a small debt to occur is acceptable provided it is paid soon enough; not paying the debt for a longer period invites bigger troubles. Similarly, in a software project, technical debt needs to be repaid regularly to avoid its accumulation. Large technical debt significantly degrades the quality of the software system and affects the productivity of the development team. In extreme cases, when the accumulated technical debt becomes so huge that it cannot be paid off anymore, the product has to be abandoned. This chapter emphasizes the importance of the concept of technical debt, the factors that contribute to it, and its impact on software projects.","n":0.087}}},{"i":386,"$":{"0":{"v":"Towards a Principle-based Classification of Structural Design Smells","n":0.354},"1":{"v":"Fred Brooks in his book \"The Mythical Man Month\" describes how the inherent properties of software (i.e. complexity, conformity, change- ability, and invisibility) make its design an \"essential\" diculty. Good design practices are fundamental requisites to address this diculty. One such good practice is that a software designer should be aware of and address \"design smells\" that can manifest as a result of his design decisions. However, our study of the vast literature on object-oriented design smells reveals the lack of an eective organization of smells that could better guide a designer in understanding and addressing potential issues in his design. In order to address this gap, we have adopted a novel approach to classify and catalog a number of recurring structural design smells based on how they violate key object oriented (OO) design principles. To evaluate the usefulness of our design smell catalog, we first asked Siemens CT DC AA architects to use it to identify design smells in their projects, and later elicited feedback from them about their experience. The feedback received indicates that these architects found the catalog to be very useful. In this paper, we present our catalog, classification, and naming scheme for design smells and also highlight several interesting observations and insights that result from our work.","n":0.069}}},{"i":387,"$":{"0":{"v":"Investigating the impact of design debt on software quality","n":0.333},"1":{"v":"Technical debt is a metaphor describing situations where developers accept sacrifices in one dimension of development (e.g. software quality) in order to optimize another dimension (e.g. implementing necessary features before a deadline). Approaches, such as code smell detection, have been developed to identify particular kinds of debt, e.g. design debt. What has not yet been understood is the impact design debt has on the quality of a software product. Answering this question is important for understanding how growing debt affects a software product and how it slows down development, e.g. though introducing rework such as fixing bugs. In this case study we investigate how design debt, in the form of god classes, affects the maintainability and correctness of software products by studying two sample applications of a small-size software development company. The results show that god classes are changed more often and contain more defects than non-god classes. This result complements findings of earlier research and suggests that technical debt has a negative impact on software quality, and should therefore be identified and managed closely in the development process.","n":0.075}}},{"i":388,"$":{"0":{"v":"A case study on effectively identifying technical debt","n":0.354},"1":{"v":"Context: The technical debt (TD) concept describes a tradeoff between short-term and long-term goals in software development. While it is highly useful as a metaphor, it has utility beyond the facilitation of discussion, to inspire a useful set of methods and tools that support the identification, measurement, monitoring, management, and payment of TD. Objective: This study focuses on the identification of TD. We evaluate human elicitation of TD and compare it to automated identification. Method: We asked a development team to identify TD items in artifacts from a software project on which they were working. We provided the participants with a TD template and a short questionnaire. In addition, we also collected the output of three tools to automatically identify TD and compared it to the results of human elicitation. Results: There is little overlap between the TD reported by different developers, so aggregation, rather than consensus, is an appropriate way to combine TD reported by multiple developers. The tools used are especially useful for identifying defect debt but cannot help in identifying many other types of debt, so involving humans in the identification process is necessary. Conclusion: We have conducted a case study that focuses on the practical identification of TD, one area that could be facilitated by tools and techniques. It contributes to the TD landscape, which depicts an understanding of relationships between different types of debt and how they are best discovered.","n":0.065}}},{"i":389,"$":{"0":{"v":"Investigating the impact of code smells debt on quality code evaluation","n":0.302},"1":{"v":"Different forms of technical debt exist that have to be carefully managed. In this paper we focus our attention on design debt, represented by code smells. We consider three smells that we detect in open source systems of different domains. Our principal aim is to give advice on which design debt has to be paid first, according to the three smells we have analyzed. Moreover, we discuss if the detection of these smells could be tailored to the specific application domain of a system.","n":0.109}}},{"i":390,"$":{"0":{"v":"Examining the Impact of Self-Admitted Technical Debt on Software Quality","n":0.316},"1":{"v":"Technical debt refers to incomplete or temporary workarounds that allow us to speed software development in the short term at the cost of paying a higher price later on. Recently, studies have shown that technical debt can be detected from source code comments, referred to as self-admitted technical debt. Researchers have examined the detection, classification and removal of self-admitted technical debt. However, to date there is no empirical evidence on the impact of self-admitted technical debt on software quality. Therefore, in this paper, we examine the relation between self-admitted technical debt and software quality by investigating whether (i) files with self-admitted technical debt have more defects compared to files without self-admitted technical debt, (ii) whether self-admitted technical debt changes introduce future defects, and (iii) whether self-admitted technical debt-related changes tend to be more difficult. We measured the difficulty of a change using well-known measures proposed in prior work such as the amount of churn, the number of files, the number of modified modules in a change, as well as the entropy of a change. An empirical study using five open source projects, namely Hadoop, Chromium, Cassandra, Spark and Tomcat, showed that: i) there is no clear trend when it comes to defects and self-admitted technical debt, although the defectiveness of the technical debt files increases after the introduction of technical debt, ii) self-admitted technical debt changes induce less future defects than none technical debt changes, however, iii) self-admitted technical debt changes are more difficult to perform, i.e., they are more complex. Our study indicates that although technical debt may have negative effects, its impact is not only related to defects, rather making the system more difficult to change in the future.","n":0.06}}},{"i":391,"$":{"0":{"v":"A large-scale empirical study on self-admitted technical debt","n":0.354},"1":{"v":"Technical debt is a metaphor introduced by Cunningham to indicate \"not quite right code which we postpone making it right\". Examples of technical debt are code smells and bug hazards. Several techniques have been proposed to detect different types of technical debt. Among those, Potdar and Shihab defined heuristics to detect instances of self-admitted technical debt in code comments, and used them to perform an empirical study on five software systems to investigate the phenomenon. Still, very little is known about the diffusion and evolution of technical debt in software projects.This paper presents a differentiated replication of the work by Potdar and Shihab. We run a study across 159 software projects to investigate the diffusion and evolution of self-admitted technical debt and its relationship with software quality. The study required the mining of over 600K commits and 2 Billion comments as well as a qualitative analysis performed via open coding.Our main findings show that self-admitted technical debt (i) is diffused, with an average of 51 instances per system, (ii) is mostly represented by code (30%), defect, and requirement debt (20% each), (iii) increases over time due to the introduction of new instances that are not fixed by developers, and (iv) even when fixed, it survives long time (over 1,000 commits on average) in the system.","n":0.068}}},{"i":392,"$":{"0":{"v":"Qualitative Data Analysis: An Expanded Sourcebook","n":0.408},"1":{"v":"Matthew B. Miles, Qualitative Data Analysis A Methods Sourcebook, Third Edition. The Third Edition of Miles & Huberman's classic research methods text is updated and streamlined by Johnny Saldana, author of The Coding Manual for Qualitative Researchers. Several of the data display strategies from previous editions are now presented in re-envisioned and reorganized formats to enhance reader accessibility and comprehension. The Third Edition's presentation of the fundamentals of research design and data management is followed by five distinct methods of analysis: exploring, describing, ordering, explaining, and predicting. Miles and Huberman's original research studies are profiled and accompanied with new examples from Saldana's recent qualitative work. The book's most celebrated chapter, \"Drawing and Verifying Conclusions,\" is retained and revised, and the chapter on report writing has been greatly expanded, and is now called \"Writing About Qualitative Research.\" Comprehensive and authoritative, Qualitative Data Analysis has been elegantly revised for a new generation of qualitative researchers. Johnny Saldana, The Coding Manual for Qualitative Researchers, Second Edition. The Second Edition of Johnny Saldana's international bestseller provides an in-depth guide to the multiple approaches available for coding qualitative data. Fully up-to-date, it includes new chapters, more coding techniques and an additional glossary. Clear, practical and authoritative, the book: describes how coding initiates qualitative data analysis; demonstrates the writing of analytic memos; discusses available analytic software; suggests how best to use the book for particular studies. In total, 32 coding methods are profiled that can be applied to a range of research genres from grounded theory to phenomenology to narrative inquiry. For each approach, Saldana discusses the method's origins, a description of the method, practical applications, and a clearly illustrated example with analytic follow-up. A unique and invaluable reference for students, teachers, and practitioners of qualitative inquiry, this book is essential reading across the social sciences. Stephanie D. H. Evergreen, Presenting Data Effectively Communicating Your Findings for Maximum Impact. This is a step-by-step guide to making the research results presented in reports, slideshows, posters, and data visualizations more interesting. Written in an easy, accessible manner, Presenting Data Effectively provides guiding principles for designing data presentations so that they are more likely to be heard, remembered, and used. The guidance in the book stems from the author's extensive study of research reporting, a solid review of the literature in graphic design and related fields, and the input of a panel of graphic design experts. Those concepts are then translated into language relevant to students, researchers, evaluators, and non-profit workers - anyone in a position to have to report on data to an outside audience. The book guides the reader through design choices related to four primary areas: graphics, type, color, and arrangement. As a result, readers can present data more effectively, with the clarity and professionalism that best represents their work.","n":0.046}}},{"i":393,"$":{"0":{"v":"Software risk management: principles and practices","n":0.408},"1":{"v":"The emerging discipline of software risk management is described. It is defined as an attempt to formalize the risk-oriented correlates of success into a readily applicable set of principles and practices. Its objectives are to identify, address, and eliminate risk items before they become either threats to successful software operation or major sources of software rework. The basic concepts are set forth, and the major steps and techniques involved in software risk management are explained. Suggestions for implementing risk management are provided. >","n":0.11}}},{"i":394,"$":{"0":{"v":"Sustainable Software Development: An Agile Perspective","n":0.408},"1":{"v":"\"Over the years I have seen the software development pendulum swing from one extreme to the other, as deficiencies in 'best practices' at one end of the spectrum spawned a new set of 'best practices' at the opposite end. Kevin Tate's book has finally brought the pendulum to a screeching halt, right about dead center. This book provides a balanced and practical guide to what's important if your goal is to develop software that lasts.\"-Mary Poppendieck, Poppendieck.LLC. Author of \"Lean Software Development\"\"1) In this very practical and accessible book interspersed with real-world examples and personal opinions, Kevin has distilled his years of developing quality software into a set of principles and practices that have been proven to work. If you are thinking of introducing an agile development environment (ADE) into your organization or of improving the one you already have, this book will help you clearly understand the benefits of a sustainable ADE, establish the practices to make it happen and coach you through the follow-up required to change the culture of your organization to make sure the changes take hold.I am currently faced with exactly this challenge and this book has already given me several ideas I am looking forward to trying out.2) In an industry plagued with missed deadlines despite long overtime hours, this book offers a refreshing alternative: a set of guiding principles and simple practices to follow that allow you to get the job done by working smarter, not harder. Drawing on the author's extensive experience developing quality software, the book clearly explains the principles behind a sustainable agile development environment, why it works, the practices to make it happen and the follow through required to turn these practices into habits.\"-Peter Schoeler, Technical Director, Artificial Mind & Movement\"It's a familiar scene-the schedule's tight, people are putting in heroic efforts to get everything done, then at the last minute a change request comes in that wipes out the gains you had finally managed to make in meeting your ship date. Looks like it's pizza at your desk for the weekend again! An unfortunate situation to be in but a pattern that repeats itself all too often. \"Sustainable Software Development\" offers hope to break this cycle. It shows how a change in mindset can free you from the tyranny of unrealistic expectations and brings development realities out onto the table for everyone to see. By following these techniques you will be able to define and manage a software development environment that will work for the long haul.\"-Kevin PicottSoftware development for immediate success and long-term sustainabilitySustainable Software Development brings together principles and practices for building software that is technically superior, delivers exceptional business value, and can evolve rapidly to reflect any change to your business or technical environment.Kevin Tate shows how to eliminate practices that make development unsustainable and replaces these practices with a sustainable approach that draws on the best ideas from both agile and conventional development. Tate demonstrates how to balance rapid releases and long-term sustainability, achieving both rich functionality and superior quality. You'll learn how to build a development organization that is more productive and can continually improve its capability to handle complexity and change.Writing for developers, architects, project leaders, and other software team members, Tate shows how to: Take control of your development environment, so you can outship your competitors, leveraging new technologies and responding to new business opportunities Maintain a consistent pace that optimally balances short- versus long-term requirements Keep your code base in a \"near-shippable\" state between releases Prevent defects, rather than just recognizing and fixing them Invest continually and cost-effectively in software design improvements Leverage the fundamentals of the craft of software development Integrate sustainable processes with Agile and traditional methodologies© Copyright Pearson Education. All rights reserved.","n":0.04}}},{"i":395,"$":{"0":{"v":"Strategies for managing requirements creep","n":0.447},"1":{"v":"One of the most chronic problems in software development is the fact that application requirements are almost never stable and fixed. Frequent changes in requirements are not always caused by capricious clients (although sometimes they are). The root cause of requirements volatility is that many applications are attempting to automate domains that are only partly understood. As software design and development proceeds, the process of automation begins to expose these ill-defined situations. Therefore, although creeping requirements are troublesome, they are often a technical necessity. Several threads of research and some emerging technologies are aimed at either clarifying requirements earlier in development or minimizing the disruptive effect of changing requirements later.","n":0.095}}},{"i":396,"$":{"0":{"v":"Robust statistics for outlier detection","n":0.447},"1":{"v":"When analyzing data, outlying observations cause problems because they may strongly influence the result. Robust statistics aims at detecting the outliers by searching for the model fitted by the majority of the data. We present an overview of several robust methods and outlier detection tools. We discuss robust procedures for univariate, low-dimensional, and high-dimensional data such as estimation of location and scatter, linear regression, principal component analysis, and classification. © 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 73-79 DOI: 10.1002/widm.2\r\n\r\nThis article is categorized under: \r\n\r\nAlgorithmic Development > Biological Data Mining\r\nAlgorithmic Development > Spatial and Temporal Data Mining\r\nApplication Areas > Health Care\r\nTechnologies > Structure Discovery and Clustering","n":0.094}}},{"i":397,"$":{"0":{"v":"Quantifying the Effect of Code Smells on Maintenance Effort","n":0.333},"1":{"v":"Context: Code smells are assumed to indicate bad design that leads to less maintainable code. However, this assumption has not been investigated in controlled studies with professional software developers. Aim: This paper investigates the relationship between code smells and maintenance effort. Method: Six developers were hired to perform three maintenance tasks each on four functionally equivalent Java systems originally implemented by different companies. Each developer spent three to four weeks. In total, they modified 298 Java files in the four systems. An Eclipse IDE plug-in measured the exact amount of time a developer spent maintaining each file. Regression analysis was used to explain the effort using file properties, including the number of smells. Result: None of the 12 investigated smells was significantly associated with increased effort after we adjusted for file size and the number of changes; Refused Bequest was significantly associated with decreased effort. File size and the number of changes explained almost all of the modeled variation in effort. Conclusion: The effects of the 12 smells on maintenance effort were limited. To reduce maintenance effort, a focus on reducing code size and the work practices that limit the number of changes may be more beneficial than refactoring code smells.","n":0.071}}},{"i":398,"$":{"0":{"v":"Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports (N)","n":0.267},"1":{"v":"Bug localization refers to the automated process of locating the potential buggy files for a given bug report. To help developers focus their attention to those files is crucial. Several existing automated approaches for bug localization from a bug report face a key challenge, called lexical mismatch, in which the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. This paper presents a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files and documentation if they appear frequently enough in the pairs of reports and buggy files. Our empirical evaluation on real-world projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, HyLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. Two out of three cases, a correct buggy file is in the list of three suggested files.","n":0.066}}},{"i":399,"$":{"0":{"v":"Decision Making Under Uncertainty—Real Options to the Rescue?","n":0.354},"1":{"v":"ABSTRACT Real options analysis (ROA) has been identified in the literature as a quantitative means to evaluate the flexibility inherent in the decision-making process. From an engineering economics perspective, this paper highlights applications, real-world users, modeling approaches, ROA assumptions, and future research directions. Through identifying and systematizing the current literature, a concise summary of modeling concerns and a road map for future modeling efforts and applications is discussed. More specifically, this paper supports research efforts to combine decision analysis tools with financial option pricing techniques to develop a real option framework that will be accepted in industry to make decisions in today's fast-paced and highly competitive business environment.","n":0.096}}},{"i":400,"$":{"0":{"v":"The markovchain Package: A Package for Easily Handling Discrete Markov Chains in R","n":0.277},"1":{"v":"The markovchain package aims to ll a gap within the R framework providing S4 classes and methods for easily handling discrete time Markov chains, homogeneous and simple inhomogeneous ones. The S4 classes for handling and analysing discrete time Markov chains are presented, as well as functions and method for performing probabilistic and statistical analysis. Finally, some examples in which the package’s functions are applied to Economics, Finance and Natural Sciences topics are shown.","n":0.117}}},{"i":401,"$":{"0":{"v":"Costs and obstacles encountered in technical debt management - A case study","n":0.289},"1":{"v":"Various costs and a cost pattern of TD management were identified.TD management has a large up-front cost, with very low ongoing costs over time.Obstacles to TD management, such as lack of process integration, were discovered.Strategies are proposed to improve adoption of TD management in software projects.Insights are also provided for research on TD management. Technical debt (TD) is a metaphor that characterizes the effect of immature software artifacts. The costs and benefits of TD, along with the uncertainty of its interest repayment, provide leverage for software managers, but also could lead to problems such as increased costs and lower quality during maintenance if it is left unattended. Therefore, effective approaches to TD management are needed by software practitioners. As one of our series of studies on TD management, this study was originally designed to reveal the cost side of explicit TD management. The study design required applying a simple proposed TD management approach to the subject project and then collecting cost information. Not surprisingly, we observed some deviation of the actual management process from our proposed one, which provided us with an opportunity to investigate the obstacles to explicitly managing TD. We also identified some costs and cost patterns related to TD management. Based on the insights gained from this study, we further propose strategies to overcome the obstacles and improve the application of TD management in practice.","n":0.066}}},{"i":402,"$":{"0":{"v":"A decision model for software maintenance","n":0.408}}},{"i":403,"$":{"0":{"v":"Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility","n":0.243},"1":{"v":"With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing 'Storage Clouds' for high performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st century vision.","n":0.063}}},{"i":404,"$":{"0":{"v":"QoS-aware middleware for Web services composition","n":0.408},"1":{"v":"The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.","n":0.079}}},{"i":405,"$":{"0":{"v":"Real Options: Managerial Flexibility and Strategy in Resource Allocation","n":0.333},"1":{"v":"In the 1970s and the 1980s, developments in the valuation of capital-investment opportunities based on options pricing revolutionized capital budgeting. Managerial flexibility to adapt and revise future decisions in order to capitalize on favorable future opportunities or to limit losses has proven vital to long-term corporate success in an uncertain and changing marketplace.In this book Lenos Trigeorgis, who has helped shape the field of real options, brings together a wealth of previously scattered knowledge and research on the new flexibility in corporate resource allocation and in the evaluation of investment alternatives brought about by the shift from static cash-flow approaches to the more dynamic paradigm of real options -- an approach that incorporates decisions on whether to defer, expand, contract, abandon, switch use, or otherwise alter a capital investment.Comprehensive in scope, Real Options reviews current techniques of capital budgeting and details an approach (based on the pricing of options) that provides a means of quantifying the elusive elements of managerial flexibility in the face of unexpected changes in the market. Also discussed are the strategic value of new technology, project interdependence, and competitive interaction.The ability to value real options has so dramatically altered the way in which corporate resources are allocated that future textbooks on capital budgeting will bear little resemblance to those of even the recent past. Real Options is a pioneer in this area, coupling a coherent picture of how option theory is used with practical insights in into real-world applications.","n":0.064}}},{"i":406,"$":{"0":{"v":"Determinants of corporate borrowing","n":0.5},"1":{"v":"Abstract   Many corporate assets, particularly growth opportunities, can be viewed as call options. The value of such ‘real options’ depends on discretionary future investment by the firm. Issuing risky debt reduces the present market value of a firm holding real options by inducing a suboptimal investment strategy or by forcing the firm and its creditors to bear the costs of avoiding the suboptimal strategy. The paper predicts that corporate borrowing is inversely related to the proportion of market value accounted for by real options. It also rationalizes other aspects of corporate borrowing behavior, for example the practice of matching maturities of assets and debt liabilities.","n":0.098}}},{"i":407,"$":{"0":{"v":"Real Options:: Managing Strategic Investment in an Uncertain World","n":0.333},"1":{"v":"This book is about the real options approach to strategic investments, showing how to capitalize on uncertainty through strategic investments, contracts, and use of the financial markets.","n":0.192}}},{"i":408,"$":{"0":{"v":"Enabling Agility Through Architecture","n":0.5},"1":{"v":"Abstract : Industry and government stakeholders continue to demand increasingly rapid innovation and the ability to adjust products and systems to emerging needs. Amongst all the enthusiasm for using Agile practices to meet these needs, the critical role of the underlying architecture is often overlooked.","n":0.149}}},{"i":409,"$":{"0":{"v":"Irreversible Investment, Capacity Choice, and the Value of the Firm","n":0.316},"1":{"v":"A model of capacity choice and utilization is developed consistent with value maximization when investment is irreversible and future demand is uncertain. Investment requires the full value of a marginal unit of capacity to be at least as large as its full cost. The former includes the value of the firms option not to utilize the unit, and the latter includes the opportunity cost of exercising the investment option. We show that for moderate amounts of uncertainty, the firm's optimal capacity is much smaller than it would be if investment were reversible, and a large fraction of the firm's value is due to the possibility of future growth. We also characterize the behavior of capacity and capacity utilization, and discuss implications far the measurement of marginal cost and Tobin's q.","n":0.088}}},{"i":410,"$":{"0":{"v":"Software economics: a roadmap","n":0.5},"1":{"v":"The fundamental goal of all good design and engineering is to create maximal value added for any given investment. There are many dimensions in which value can be assessed, from monetary profit to the solution of social problems. The benefits sought are often domain-specific, yet the logic is the same: design is an investment activity. Software economics is the field that seeks to enable significant improvements in software design and engineering through economic reasoning about product, process, program, and portfolio and policy issues. We summarize the state of the art and identify shortfalls in existing knowledge. Past work focuses largely on costs, not on benefits, thus not on value added; nor are current technical software design criteria linked clearly to value creation. We present a roadmap for research emphasizing the need for a strategic investment approach to software engineering. We discuss how software economics can lead to fundamental improvements in software design and engineering, in theory and practice.","n":0.08}}},{"i":411,"$":{"0":{"v":"A decentralized self-adaptation mechanism for service-based applications in the cloud","n":0.316},"1":{"v":"Cloud computing, with its promise of (almost) unlimited computation, storage, and bandwidth, is increasingly becoming the infrastructure of choice for many organizations. As cloud offerings mature, service-based applications need to dynamically recompose themselves to self-adapt to changing QoS requirements. In this paper, we present a decentralized mechanism for such self-adaptation, using market-based heuristics. We use a continuous double-auction to allow applications to decide which services to choose, among the many on offer. We view an application as a multi-agent system and the cloud as a marketplace where many such applications self-adapt. We show through a simulation study that our mechanism is effective for the individual application as well as from the collective perspective of all applications adapting at the same time.","n":0.091}}},{"i":412,"$":{"0":{"v":"Easy web service discovery: A query-by-example approach","n":0.378},"1":{"v":"Web services have acquired enormous popularity among software developers. This popularity has motivated developers to publish a large number of Web service descriptions in UDDI registries. Although these registries provide search facilities, they are still rather difficult to use and often require service consumers to spend too much time manually browsing and selecting service descriptions. This paper presents a novel search method for Web services called WSQBE that aims at both easing query specification and assisting discoverers by returning a short and accurate list of candidate services. In contrast with previous approaches, WSQBE discovery process is based on an automatic search space reduction mechanism that makes this approach more efficient. Empirical evaluations of WSQBE search space reduction mechanism, retrieval performance, processing time and memory usage, using a registry with 391 service descriptions, are presented.","n":0.086}}},{"i":413,"$":{"0":{"v":"Formal Versus Agile: Survival of the Fittest","n":0.378},"1":{"v":"Many research have focused on new formal methods, integrating formal methods into agile ones, and assessing the agility of formal methods. This paper proves that formal methods can survive in an agile world; they are not obsolete and can be integrated into it. The potential for combining agile and formal methods holds promise. It might not always be an easy partnership, and succeeding will depend on a fruitful interchange of expertise between the two communities. Conducting a realistic trial project using a combined approach with an appropriate formal methods tool in a controlled environment will help assess the effectiveness of such an approach.","n":0.099}}},{"i":414,"$":{"0":{"v":"Finance Theory and Financial Strategy","n":0.447},"1":{"v":"Despite its major advances, finance theory has had scant impact on strategic planning. Strategic planning needs finance and should learn to apply finance theory correctly. However, finance theory must be extended in order to reconcile financial and strategic analysis.","n":0.16}}},{"i":415,"$":{"0":{"v":"Software Design as an Investment Activity: A Real Options Perspective","n":0.316},"1":{"v":"Key software design principles‐e.g., information hiding, spiral processes, and guidelines for timing design decisions—remain idiosyncratic, ad hoc, and not unified or explained in theory. This makes them hard to teach, learn, and use in principled ways. We see the potential to build stronger foundations for softwar e design by viewing it as an investment activity and by relating design concepts direct ly to well developed financial investment theories. In this paper we focus on the specific is sue of the value of flexibility in the face of uncertainty. Uncertainty puts a premium on flex ibility to change products and plans, but flexibility also incurs costs. The theories on which earlier approaches to software engineering economics were based—primarily static net present value—cannot account for the value of flexibility. Real options theory, wh ich seeks to make this value tangible in capital investment through an analogy to financi al (e.g., stock) options, provides a framework within which we analyze software design concepts. We sketch our interpretation of important design principles in real opti ons terms, making the idea precise for the specific case of optimal timing of design decisio ns. We discuss the validity of the approach for software; impediments to quantitative application; and how qualitative options thinking can nevertheless improve design decision making.","n":0.069}}},{"i":416,"$":{"0":{"v":"Software growth options","n":0.577},"1":{"v":"Today's business environment is characterized by global competition and buyers' markets. Under such conditions, flexible response to changes in the environment is a key success factor for any firm. Consequently, there is increasing pressure for information systems to be readily adaptable to changing business processes. This paper investigates ways of introducing this aspect of IS extendability into quantitative IT investment decision models via the application of real options models. In particular, it examines methods for evaluating sequential exchange options in order to obtain estimates for the value of software growth options--that is, IS functions that are embedded in an IT platform and that can be employed once the particular base system is installed and their use is economically justified. On the basis of these models, we look at the determinants of the value of software growth options and draw general conclusions with regard to decision making in the field of IT investment.","n":0.081}}},{"i":417,"$":{"0":{"v":"On Context-Specific Substitutability of Web Services","n":0.408},"1":{"v":"Web service substitution refers to the problem of identifying a service that can replace another service in the context of a composition with a specified functionality. Existing solutions to this problem rely on detecting the functional and behavioral equivalence of a particular service to be replaced and candidate services that could replace it. We introduce the notion of context-specific substitutability, where context refers to the overall functionality of the composition that is required to be maintained after replacement of its constituents. Using the context information, we investigate two variants of the substitution problem, namely environment-independent and environment- dependent, where environment refers to the constituents of a composition and show how the substitutability criteria can be relaxed within this model. We provide a logical formulation of the resulting criteria based on model checking techniques as well as prove the soundness and completeness of the proposed approach.","n":0.083}}},{"i":418,"$":{"0":{"v":"Value based software reuse investment","n":0.447},"1":{"v":"A number of issues are covered in this paper. Chief among them is the need for greater discipline in understanding the economic benefits of software reuse within the context of a broader business strategy. While traditional methods fail to account for growth opportunities and flexibility generated by investments in reuse, the introduction of option pricing theory can greatly enhance the design and evaluation of software reuse projects. Similarly, the disciplines of business strategy hold promise to help to fill the void of “strategic context” within which reuse investment happens.","n":0.106}}},{"i":419,"$":{"0":{"v":"Web Services Discovery Based on Latent Semantic Approach","n":0.354},"1":{"v":"With an ever-increasing number of Web services being available, finding desired Web service is crucial for service users. Current keyword search and most existing approaches are inefficient in two main aspects: poor scalability and lack of semantics. Firstly, users are overwhelmed by the huge number of irrelevant services returned. Secondly, the intentions of users and the semantics in Web services are ignored. Inspired by the success of the divide and conquer approach used to handle the complex information decomposition, we use a novel approach to partition a large set of search results into a set of smaller groups by employing a clustering approach. Then we utilize singular value decomposition (SVD) to capture the main semantics hidden behind the words in a query and the descriptions in the services, so that service matching can be carried out at the concept level. We report here on the preliminary experimental evaluation that shows improvements overall precision.","n":0.081}}},{"i":420,"$":{"0":{"v":"Perfectionists in a World of Finite Resources","n":0.378},"1":{"v":"The metaphor of technical debt, originally coined by Ward Cunninghamhas helped me recently get a handle on this type of issue. Almost invariably in software projects, developers can be so focused on accomplishing the needed functionality that the software itself grows less understandable, more complex, and harder to modify. Since this system deterioration usually reflects a lack of activity spent in refactoring, documentation, and other aspects of the project infrastructure, we can view it as a kind of debt that developers owe the system. Ward Cunningham's metaphor helps make clear an important trade-off: although a little debt can speed up software development in the short run, this benefit is achieved at the cost of extra work in the future, as if paying interest on the debt. Technical debt gives us a frame work for thinking about the fact that not doing some good things today, no matter how valuable they seem on their own merits, allows us to invest in other good things. In short, there are always trade-offs in life.","n":0.076}}},{"i":421,"$":{"0":{"v":"Migration to Cloud as Real Option: Investment Decision under Uncertainty","n":0.316},"1":{"v":"If cloud is so good then why aren't companies using it more? In this paper we look at how companies should make a decision to move some IT services or their IT infrastructure into the cloud. The move may initially look attractive in that it offers cost benefits but there is also considerable uncertainty, not least around security and information stewardship. Within the paper we propose the use of a real option model to help think about when to switch to cloud based on the expected benefits, uncertainties and the value a company puts on money.","n":0.102}}},{"i":422,"$":{"0":{"v":"Web Service Substitution Based on Preferences Over Non-functional Attributes","n":0.333},"1":{"v":"In many applications involving composite Web services, one or more component services may become unavailable. This presents us with the problem of identifying other components that can take their place, while maintaining the overall functionality of the composite service. Given a choice of candidate substitutions that offer the desired functionality, it is often necessary to select the most preferred substitution based on non-functional attributes of the service, e.g., security, reliability, etc. We propose an approach to this problem using preference networks for representing and reasoning about preferences over non-functional properties. We present algorithms for solving several variants of this problem: a) when the choice of the preferred substitution is independent of the other constituents of the composite service; b) when the choice of the preferred substitution depends on the other constituents of the composite service; and c) when multiple constituents of a composite service need to be replaced simultaneously. The proposed solutions to the service substitution problem based on preferences over non-functional properties are independent of the specific formalism used to represent functional requirements of a composite service as well as the specific algorithm used to assemble the composite service.","n":0.073}}},{"i":423,"$":{"0":{"v":"Quantitative approaches for assessing the value of COTS-centric development","n":0.333},"1":{"v":"Software development based on commercial off-the-shelf or COTS, components is currently drawing considerable attention. This paper presents the results of two complementary quantitative valuation methods applied to the assessment of the COTS-centric software development projects. We use a standard corporate finance tool, Net Present Value, as a basis for both methods. The first method, comparative valuation, investigates the economic incentive to choose the COTS centric strategy in a project vis a vis the alternative, the custom development strategy, through an incentive metric based on NPV. The analysis concentrates on the impact of product risk and development time on the defined metric. The second method, real options valuation, primarily deals with uncertainty. It is employed to investigate the value of strategic flexibility inherent in COTS-centric development. We provide examples of several such options and summarize qualitatively the results of their analyses. Using these two approaches, some common anecdotes of COTS-centric development can be substantiated by sound financial arguments. Through scenarios and sensitivity analyses, we show that different circumstances and assumptions give rise to different winning conditions. Some general principles are summarized at the end.","n":0.074}}},{"i":424,"$":{"0":{"v":"Keep Your Options Open: Extreme Programming and the Economics of Flexibility","n":0.302},"1":{"v":"Financial evaluation and strategic analysis have long been considered two distinct approaches to evaluating new capital initiatives. An emerging valuation approach, known as real options, attempts to align finance and strategy through a new perspective: The value of an asset lies not only in the amount of direct revenues that it is expected to generate, but also in the options that it creates for flexible decision making in the future. In general, the more uncertain the future is, the higher the value of flexibility embedded in an asset, whether financial or real. This perspective has significant implications for the economics of flexible processes. Applied to software development, it could imply that a lightweight process that is well positioned to respond to change and future opportunities creates more value than a heavy-duty process that tends to freeze development decisions early. Thus, the feasibility of Extreme Programming (XP) can be supported by the option value of flexibility inherent in it. What is the theory that underlies this statement? How does it relate to the fundamental assumptions of XP? How does it impact the value of an XP project? What are the implications of such value propositions for project decisions? If you are curious, read on ...","n":0.07}}},{"i":425,"$":{"0":{"v":"Using real options to select stable middleware-induced software architectures","n":0.333},"1":{"v":"The requirements that force decisions towards building distributed system architectures are usually of a non-functional nature. Scalability, openness, heterogeneity, and fault-tolerance are examples of such non-functional requirements. The current trend is to build distributed systems with middleware, which provide the application developer with primitives for managing the complexity of distribution, system resources, and for realising many of the non-functional requirements. As non-functional requirements evolve, the 'coupling' between the middleware and architecture becomes the focal point for understanding the stability of the distributed software system architecture in the face of change. It is hypothesised that the choice of a stable distributed software architecture depends on the choice of the underlying middleware and its flexibility in responding to future changes in non-functional requirements. Drawing on a case study that adequately represents a medium-size component-based distributed architecture, it is reported how a likely future change in scalability could impact the architectural structure of two versions, each induced with a distinct middleware: one with CORBA and the other with J2EE. An option-based model is derived to value the flexibility of the induced-architectures and to guide the selection. The hypothesis is verified to be true for the given change. The paper concludes with some observations that could stimulate future research in the area of relating requirements to software architectures.","n":0.068}}},{"i":426,"$":{"0":{"v":"Software design: the options approach","n":0.447}}},{"i":427,"$":{"0":{"v":"Design Patterns: Elements of Reusable Object-Oriented Software","n":0.378}}},{"i":428,"$":{"0":{"v":"Introduction to Software Testing","n":0.5},"1":{"v":"Extensively class tested, this text takes an innovative approach to explaining the process of software testing: it defines testing as the process of applying a few well-defined, general-purpose test criteria to a structure or model of the software. The structure of the text directly reflects the pedagogical approach and incorporates the latest innovations in testing, including techniques to test modern types of software such as OO, web applications, and embedded software.","n":0.119}}},{"i":429,"$":{"0":{"v":"UML Distilled: A Brief Guide to the Standard Object Modeling Language","n":0.302},"1":{"v":"From the Book:\r\nTwo years ago, Addison-Wesley approached me to write a book about the then-new UML. At that time, there was a lot of interest in the UML, but only a standards document from which to learn about it. We broke many records to quickly produce a short introductory guide to the new UML, something that would provide some guidance until the more detailed and official books were to appear later that year. \r\n\r\nWe didnit expect this book to last after more detailed books appeared. Most people believed that given the choice between a slim overview and a detailed text, everyone would pick the detailed text. Although that was the general view, I believed that even in the presence of detailed books, there was still room for a concise summary. \r\n\r\nTwo years later, my hopes have been realized more than I could have wished. UML Distilled has been, in computer industry terms, a best-seller. Even though good detailed books have appeared on the UML, the book still sells well. Better than that, more slim books have appeared, confirming my belief that in a world with so much information, there is value in well-chosen brevity. \r\n\r\nNow, thatis all very well, but should you buy this book? \r\n\r\nIim going to assume youive heard about the UML. It has become the standard way to draw diagrams of object-oriented designs, and it has also spread into non-OO fields. The major pre-UML methods have all died out. The UML has arrived and ishere to stay. \r\n\r\nIf you want to learn about the UML, this book is one way to do it. The main reason for starting with this book is that itis a small book. Buying a big book will give you more information, but it will also take you longer to read. Iive selected the most important parts of the UML so that you donit have to. With this book, youill pick up the key elements of the notation and what they mean. If you want to move further, you can move to a more detailed book later. \r\n\r\nIf you want a longer tutorial to the UML, I suggest the Unified Modeling Language User Guide (Booch, Rumbaugh, and Jacobson 1999). The User Guide is able to cover a lot more ground. Itis well written and organized in a way that explains how to apply the UML to various modeling problems. \r\n\r\nBoth this book and the User Guide assume that you know something about OO development. Although many people have told me that this book is a good introduction to objects, I didnit write it with that in mind. If youire looking for an introduction to objects with the UML, you should also consider Craig Larmanis book (Larman 1998). \r\n\r\nAlthough the main point of this book is the UML, Iive also added material that complements the UML material. The UML is a relatively small part of what you need to know to succeed with objects, and I think that itis important to point out some of the other things here. \r\n\r\nThe most important of these is software process. The UML is designed to be independent of process. You can do anything you like; all the UML does is say what your diagrams mean. However, the diagrams donit make much sense without a process to give them context. I also believe that process is important and that a good process doesnit need to be complicated. \r\n\r\nSo, Iive described a lightweight outline process for OO software development. This provides a context for the techniques and will help to get you going in using objects. \r\n\r\nThe other topics include patterns, refactoring, self-testing code, design by contract, and CRC cards. None of these are part of the UML, yet they are valuable techniques that I use regularly. \r\nStructure of the Book\r\nChapter 1 looks at what the UML is, the history of its development, and the reasons why you might want to use it.\r\n\r\nChapter 2 discusses the object-oriented development process. Although the UML exists independent of process, I find it hard to discuss modeling techniques without talking about where they fit in with object-oriented development. \r\n\r\nChapters 3 through 6 discuss the three most important techniques in the UML: use cases, class diagrams, and interaction models. The UML is a large beast, but you donit need all of it. These three techniques are the core that almost everyone needs. Start with these and add the others as you need them. (Note that since class diagrams are so complicated in themselves, Iive put the key parts of class diagrams in Chapter 4 and the advanced concepts in Chapter 6. ) \r\n\r\nChapters 7 through 10 explore the remaining techniques. All of these are valuable, but not every project needs every technique. So these chapters provide enough information to tell you what the technique is and whether you need it. \r\n\r\nFor all of these techniques, I describe the notation, explain what the notation means, and provide tips about using the techniques. My philosophy is to make clear what the UML says and, at the same time, to give you my opinions on how best to use it. Iive also added pointers to other books that provide more detail. \r\n\r\nChapter 11 gives a small example to show how the UML fits in with programming using (of course) Java. \r\n\r\nThe inside covers summarize the UML notation. You may find it useful to refer to these as you are reading the chapters so that you can check on the notation for the various modeling concepts. \r\n\r\nIf you find this book interesting, you can find other information on my work related to using the UML, patterns, and refactoring at my home page (see page xxi). \r\nChanges for the Second Edition\r\nAs the UML evolved, and I received feedback about the first edition of the book, I continually updated it. We reprinted every two or three months; nearly every printing contained updates, which resulted in considerable strain on the processes of the publishing industry. \r\n\r\nWith the change from UML 1.2 to 1.3, we decided to do a more thorough overhaul of the book, enough to produce a second edition. Since the book has been so popular, Iive tried not to change the essential spirit of the book. Iive carefully tried to not add much, and to see whether there are things I can take away. \r\n\r\nThe biggest changes are in Chapter 3, about use cases, and Chapter 9, about activity diagrams, which have each received a severe rewrite. Iive also added a section on collaborations to Chapter 7. Elsewhere, Iive taken the opportunity to make a host of smaller changes, based on feedback and my experiences over the last couple of years. \r\n\r\nMartin Fowler \r\n\r\nMelrose, Massachusetts \r\n\r\nApril 1999 \r\n\r\nfowler@acm.org\r\n\r\nhttp://ourworld.compuserve.com/homepages/Martin_Fowler","n":0.03}}},{"i":430,"$":{"0":{"v":"Introduction to Software Testing: List of Figures","n":0.378}}},{"i":431,"$":{"0":{"v":"Basics of Software Engineering Experimentation","n":0.447},"1":{"v":"Basics of Software Engineering Experimentation is a practical guide to experimentation in a field which has long been underpinned by suppositions, assumptions, speculations and beliefs. It demonstrates to software engineers how Experimental Design and Analysis can be used to validate their beliefs and ideas. The book does not assume its readers have an in-depth knowledge of mathematics, specifying the conceptual essence of the techniques to use in the design and analysis of experiments and keeping the mathematical calculations clear and simple. Basics of Software Engineering Experimentation is practically oriented and is specially written for software engineers, all the examples being based on real and fictitious software engineering experiments.","n":0.096}}},{"i":432,"$":{"0":{"v":"Behavior protocols for software components","n":0.447},"1":{"v":"In this paper, we propose a means to enhance an architecture description language with a description of component behavior. A notation used for this purpose should be able to express the \"interplay\" on the component's interfaces and reflect step-by-step refinement of the component's specification during its design. In addition, the notation should be easy to comprehend and allow for formal reasoning about the correctness of the specification refinement and also about the correctness of an implementation in terms of whether it adheres to the specification. Targeting all these requirements together, the paper proposes employing behavior protocols which are based on a notation similar to regular expressions. As proof of the concept, the behavior protocols are used in the SOFA architecture description language at three levels: interface, frame, and architecture. Key achievements of this paper include the definitions of bounded component behavior and protocol conformance relation. Using these concepts, the designer can verify the adherence of a component's implementation to its specification at runtime, while the correctness of refining the specification can be verified at design time.","n":0.075}}},{"i":433,"$":{"0":{"v":"Refactoring UML Models","n":0.577},"1":{"v":"Software developers spend most of their time modifying and maintaining existing products. This is because systems, and consequently their design, are in perpetual evolution before they die. Nevertheless, dealing with this evolution is a complex task. Before evolving a system, structural modifications are often required. The goal of this kind of modification is to make certain elements more extensible, permitting the addition of new features. However, designers are seldom able to evaluate the impact, on the whole model, of a single modification. That is, they cannot precisely verify if a change modifies the behavior of the modeled system. A possible solution for this problem is to provide designers with a set of basic transformations, which can ensure behavior preservation. These transformations, also known as refactorings, can then be used, step by step, to improve the design of the system. In this paper we present a set of refactorings and explain how they can be designed so as to preserve the behavior of a UML model. Some of these refactorings are illustrated with examples.","n":0.076}}},{"i":434,"$":{"0":{"v":"How Software Designs Decay: A Pilot Study of Pattern Evolution","n":0.316},"1":{"v":"A common belief is that software designs decay as systems evolve. This research examines the extent to which software designs actually decay by studying the aging of design patterns in successful object oriented systems. Aging of design patterns is measured using various types of decay indices developed for this research. Decay indices track the internal structural changes of a design pattern realization and the code that surrounds the realization. Hypotheses for each kind of decay are tested. We found that the original design pattern functionality remains, and pattern decay is due to the \"grime \", non-pattern code, that grows around the pattern realization.","n":0.099}}},{"i":435,"$":{"0":{"v":"A multiple case study of design pattern decay, grime, and rot in evolving software systems","n":0.258},"1":{"v":"Software designs decay as systems, uses, and operational environments evolve. Decay can involve the design patterns used to structure a system. Classes that participate in design pattern realizations accumulate grime--non-pattern-related code. Design pattern realizations can also rot, when changes break the structural or functional integrity of a design pattern. Design pattern rot can prevent a pattern realization from fulfilling its responsibilities, and thus represents a fault. Grime buildup does not break the structural integrity of a pattern but can reduce system testability and adaptability. This research examined the extent to which software designs actually decay, rot, and accumulate grime by studying the aging of design patterns in three successful object-oriented systems. We generated UML models from the three implementations and employed a multiple case study methodology to analyze the evolution of the designs. We found no evidence of design pattern rot in these systems. However, we found considerable evidence of pattern decay due to grime. Dependencies between design pattern components increased without regard for pattern intent, reducing pattern modularity, and decreasing testability and adaptability. The study of decay and grime showed that the grime that builds up around design patterns is mostly due to increases in coupling.","n":0.071}}},{"i":436,"$":{"0":{"v":"The Role-Based Metamodeling Language for Specifying Design Patterns","n":0.354}}},{"i":437,"$":{"0":{"v":"Generalizing fault contents from a few classes","n":0.378},"1":{"v":"The challenges in fault prediction today are to get a prediction as early as possible, at as low a cost as possible, needing as little data as possible and preferably in such a language that your average developer can understand where it came from. This paper presents a fault sampling method where a summary of a few, easily available metrics is used together with the results of a few sampled classes to generalize the fault content to an entire system. The method is tested on a large software system written in Java, that currently consists of around 2000 classes and 300,000 lines of code. The evaluation shows that the fault generalization method is good at predicting fault-prone clusters and that it is possible to generalize the values of a few representative classes.","n":0.087}}},{"i":438,"$":{"0":{"v":"Object oriented design pattern decay: a taxonomy","n":0.378},"1":{"v":"Software designs decay over time. While most studies focus on decay at the system level, this research studies design decay on well understood micro architectures, design patterns. Formal definitions of design patterns provide a homogeneous foundation that can be used to measure deviations as pattern realizations evolve. Empirical studies have shown modular grime to be a significant contributor to design pattern decay. Modular grime is observed when increases in the coupling of design pattern classes occur in ways unintended by the original designer. Further research is necessary to formally categorize distinct forms of modular grime. We identify three properties of coupling relationships that are used to classify subsets of modular grime. A taxonomy is presented which uses these properties to group modular grime into six disjoint categories. Illustrative examples of grime build-up are provided to demonstrate the taxonomy. A pilot study is used to validate the taxonomy and provide initial empirical evidence of the proposed classification.","n":0.08}}},{"i":439,"$":{"0":{"v":"A meta-modeling approach to specifying patterns","n":0.408},"1":{"v":"A major goal in software development is to produce quality products in less time and with less cost. Systematic reuse of software artifacts that encapsulate high-quality development experience can help achieve the goal. Design patterns are a common form of reusable design experience. Use of design patterns can help developers reduce development time. Prevalent design patterns are, however, described informally (e.g., [36]). This prevents systematic use of patterns. \r\nThe research documented in this dissertation is aimed at developing a practical pattern specification technique that supports the systematic use of patterns during design modeling. To achieve this goal, the research developed a pattern specification language called the Role-Based Metamodeling Language (RBML). The RBML uses the Unified Modeling Language (UML) syntax to the extent possible, and specifies a pattern as a specialization of the UML metamodel. The RBML uses the UML as a syntactic base to make it easier for UML modelers to create, understand, and evolve pattern specifications, and to enable the use of UML modeling tools for creating and evolving pattern specifications. \r\nThe research used the RBML to develop specifications for design patterns in the Design Patterns book [36] including Abstract Factory, Bridge, Decorator, Observer, State, Iterator, and Visitor. The research also used the RBML to define a large application domain pattern for checkin-checkout systems, and used the resulting specification to develop UML designs for a library system and a car rental system. The research also used the RBML to specify access control mechanisms as patterns including Role-Based Access Control (RBAC), Mandatory Access Control (MAC), and a Hybrid Access Control (HAC) that is a composition of RBAC and MAC. The RBML is currently used at NASA for modeling pervasive requirements as patterns. NASA also uses the RBML in the development of Weather CTAS System that is a weather forecasting system. To determine the potential of the RBML to support the development of tools that enable systematic use of patterns, the research developed a prototype tool called RBML-Pattern Instantiator (RBML-PI) that generates conforming UML models from RBML pattern specifications.","n":0.054}}},{"i":440,"$":{"0":{"v":"Decay and grime buildup in evolving object oriented design patterns","n":0.316},"1":{"v":"Software designs decay as systems, uses, and operational environments evolve. As software ages the original realizations of design patterns may remain in place, while participants in design pattern realizations accumulate grime—non-pattern-related code. This research examines the extent to which software designs actually decay, rot and accumulate grime by studying the aging of design patterns in successful object oriented systems. By focusing on design patterns we can identify code constructs that conflict with well formed pattern structures. Design pattern rot is the deterioration of the structural integrity of a design pattern realization. Grime buildup in design patterns is a form of decay that does not break the structural integrity of a pattern but can reduce system testability and adaptability. Grime is measured using various types of indices developed and adapted for this research. Grime indices track the internal structural changes in a design pattern realization and the code that surrounds the realization. In general we find that the original pattern functionality remains, and pattern decay is primarily due to grime and not rot. We characterize the nature of grime buildup in design patterns, provide quantifiable evidence of such grime buildup, and find that grime can be classified at organizational, modular and class levels. Organizational level grime refers to namespace and physical file constitution and structure. Metrics at this level help us understand if rot and grime buildup play a role in fomenting disorganization of design patterns. Measures of modular level grime can help us to understand how the coupling of classes belonging to a design pattern develops. As dependencies between design pattern components increase without regard for pattern intent, the modularity of a pattern deteriorates. Class level grime is focused on understanding how classes that participate in design patterns are modified as systems evolve. For each level we use different measurements and surrogate indicators to help analyze the consequences that grime buildup has on testability and adaptability of design patterns. Test cases put in place during the design phase and initial implementation of a project can become ineffective as the system matures. The evolution of a design due to added functionality or defect fixing increases the coupling and dependencies between classes that must be tested. We show that as systems age, the growth of grime and the appearance of anti-patterns (a form of decay) increase testing requirements. Additionally, evidence suggests that, as pattern realizations evolve, the levels of efferent and afferent coupling of the classifiers that participate in patterns increase. Increases in coupling measurements suggest dependencies to and from other software artifacts thus reducing the adaptability and comprehensibility of the pattern. In general we find that grime buildup is most serious at a modular level. We find little evidence of class and organizational grime. Furthermore, we find that modular grime appears to have higher impacts on testability than adaptability of design patterns. \r\nIdentifying grime helps developers direct refactoring efforts early in the evolution of software, thus keeping costs in check by minimizing the effects of software aging. Long term goals of this research are to curtail the effects of decay by providing the understanding and means necessary to diminish grime buildup.","n":0.044}}},{"i":441,"$":{"0":{"v":"Cost models for future software life cycle processes: COCOMO 2.0","n":0.316},"1":{"v":"Current software cost estimation models, such as the 1981 Constructive Cost Model (COCOMO) for software cost estimation and its 1987 Ada COCOMO update, have been experiencing increasing difficulties in estimating the costs of software developed to new life cycle processes and capabilities. These include non-sequential and rapid-development process models; reuse-driven approaches involving commercial off-the-shelf (COTS) packages, re-engineering, applications composition, and applications generation capabilities; object-oriented approaches supported by distributed middleware; and software process maturity initiatives. This paper summarizes research in deriving a baseline COCOMO 2.0 model tailored to these new forms of software development, including rationale for the model decisions. The major new modeling capabilities of COCOMO 2.0 are a tailorable family of software sizing models, involving Object Points, Function Points, and Source Lines of Code; nonlinear models for software reuse and re-engineering; an exponentdriver approach for modeling relative software diseconomies of scale; and several additions, deletions and updates to previous COCOMO effort-multiplier cost drivers. This model is serving as a framework for an extensive current data collection and analysis effort to further refine and calibrate the model's estimation capabilities.","n":0.075}}},{"i":442,"$":{"0":{"v":"How the Apache community upgrades dependencies: an evolutionary study","n":0.333},"1":{"v":"Software ecosystems consist of multiple software projects, often interrelated by means of dependency relations. When one project undergoes changes, other projects may decide to upgrade their dependency. For example, a project could use a new version of a component from another project because the latter has been enhanced or subject to some bug-fixing activities. In this paper we study the evolution of dependencies between projects in the Java subset of the Apache ecosystem, consisting of 147 projects, for a period of 14 years, resulting in 1,964 releases. Specifically, we investigate (i) how dependencies between projects evolve over time when the ecosystem grows, (ii) what are the product and process factors that can likely trigger dependency upgrades, (iii) how developers discuss the needs and risks of such upgrades, and (iv) what is the likely impact of upgrades on client projects. The study results--qualitatively confirmed by observations made by analyzing the developers' discussion--indicate that when a new release of a project is issued, it triggers an upgrade when the new release includes major changes (e.g., new features/services) as well as large amount of bug fixes. Instead, developers are reluctant to perform an upgrade when some APIs are removed. The impact of upgrades is generally low, unless it is related to frameworks/libraries used in crosscutting concerns. Results of this study can support the understanding of the of library/component upgrade phenomenon, and provide the basis for a new family of recommenders aimed at supporting developers in the complex (and risky) activity of managing library/component upgrade within their software projects.","n":0.063}}},{"i":443,"$":{"0":{"v":"Trusting a library: A study of the latency to adopt the latest Maven release","n":0.267},"1":{"v":"With the popularity of open source library (re)use in both industrial and open source settings, ‘trust’ plays vital role in third-party library adoption. Trust involves the assumption of both functional and non-functional correctness. Even with the aid of dependency management build tools such as Maven and Gradle, research have still found a latency to trust the latest release of a library. In this paper, we investigate the trust of OSS libraries. Our study of 6,374 systems in Maven Super Repository suggests that 82% of systems are more trusting of adopting the latest library release to existing systems. We uncover the impact of maven on latent and trusted library adoptions.","n":0.096}}},{"i":444,"$":{"0":{"v":"An exploratory study on library aging by monitoring client usage in a software ecosystem","n":0.267},"1":{"v":"In recent times, use of third-party libraries has become prevalent practice in contemporary software development. Much like other code components, unmaintained libraries are a cause for concern, especially when it risks code degradation over time. Therefore, awareness of when a library should be updated is important. With the emergence of large libraries hosting repositories such as Maven Central, we can leverage the dynamics of these ecosystems to understand and estimate when a library is due for an update. In this paper, based on the concepts of software aging, we empirically explore library usage as a means to describe its age. The study covers about 1,500 libraries belonging to the Maven software ecosystem. Results show that library usage changes are not random, with 81.7% of the popular libraries fitting typical polynomial models. Further analysis show that ecosystem factors such as emerging rivals has an effect on aging characteristics. Our preliminary findings demonstrate that awareness of library aging and its characteristics is a promising step towards aiding client systems in the maintenance of their libraries.","n":0.076}}},{"i":445,"$":{"0":{"v":"Managing and assessing the risk of component upgrades","n":0.354},"1":{"v":"This paper describes the experience, carried out by Ericsson Telecomunicazioni S.p.A (Italy), in managing the migration of their legacy products towards a product line approach and, specifically, how the update of third-party software products is handled in such product lines. The paper describes the Ericsson application scenario in the development and evolution of network management products. Then, it provides an overview of how the company adopts (i) an internal toolkit to manage third party software products, with the aim of determining the impact of their updates upon variants of the network management system, and (ii) a risk management framework, which helps the developer to decide whether and when update third-party products.","n":0.095}}},{"i":446,"$":{"0":{"v":"On the Rate of Growth of the Population of the United States since 1790 and Its Mathematical Representation.","n":0.236},"1":{"v":"Pearl and Reed begin their paper with a discussion of exponential and parabolic growth formulas for past U. S. population growth, which we have omitted.","n":0.2}}},{"i":447,"$":{"0":{"v":"Listening to programmers Taxonomies and characteristics of comments in operating system code","n":0.289},"1":{"v":"Innovations from multiple directions have been proposed to improve software reliability. Unfortunately, many of the innovations are not fully exploited by programmers. To bridge the gap, this paper proposes a new approach to “listen” to thousands of programmers: studying their programming comments. Since comments express programmers' assumptions and intentions, comments can reveal programmers' needs, which can provide guidance (1) for language/-tool designers on where they should develop new techniques or enhance the usability of existing ones, and (2) for programmers on what problems are most pervasive and important so that they should take initiatives to adopt some existing tools or language extensions. We studied 1050 comments randomly sampled from the latest versions of Linux, FreeBSD, and OpenSolaris. We found that 52.6% of these comments could be leveraged by existing or to-be-proposed tools for improving reliability. Our findings include: (1) many comments describe code relationships, code evolutions, or the usage and meaning of integers and integer macros, (2) a significant amount of comments could be expressed by existing annotation languages, and (3) many comments express synchronization related concerns but are not well supported by annotation languages.","n":0.074}}},{"i":448,"$":{"0":{"v":"Introduction to Information","n":0.577},"1":{"v":"Information is central to all applied studies in economics and other sciences. It has many facets. As empirical data it provides the basis for testing an economic model or theory. It is also intimately connected with decision making under conditions of risk and uncertainty. Hence the choice of optimal policy under an uncertain environment depends on the type of information structures e.g., is it partial or total, incomplete or complete and imprecise or precise?","n":0.116}}},{"i":449,"$":{"0":{"v":"Impact of Size and Productivity on Testing and Rework Efforts for Web-based Development Projects","n":0.267},"1":{"v":"In the planning of a software development project, a major challenge faced by project managers is to predict the rework effort. (Rework effort is the effort required to fix the software defects identified during system testing). The project manager's objective is to deliver the software within the time, cost and quality requirements given by the client. To ensure the quality of the software, many testing cycles will be conducted before it is finally delivered to the client for acceptance. Each testing cycle is a costly affair as it involves running all possible test scenarios in all possible environments, followed by defect fixing and re-verification of defect fixes. On average, two to three testing cycles are conducted but this depends on the number of defects identified during testing. The number of defects will depend on the team expertise and whether they earlier worked on similar projects and technologies. Therefore, it becomes critical to predict the number of defects that will be identified during testing and it is a very challenging task as it requires a good model to predict the rework effort. In this paper, we describe the relationships among software size, number of software defects, productivity and efforts for web-based development projects. These relationships are established by using the multiple linear regression technique on the benchmarking data published by International Software Benchmarking Standard Group. Results suggest that in web-based projects the number of defects identified is directly proportional to the productivity, i.e. higher productivity will led to more defects found and lower productivity will lead to fewer defects found, therefore, less testing and rework effort will be required if project is planned with lower productivity because we can spend more time on development (i.e. time spend till construction phase) to reduce the number of defects found during testing and it will directly contribute in reducing the rework efforts. We infer from the relationship that software size has a significant impact on the total number of defect identified. We also infer that while planning a software project we should use appropriate tools to reduce the margin of error in size estimation and we should also re-estimate the size after every phase of the development life cycle to re-calibrate overall efforts and to minimize the impact on the project plan.","n":0.051}}},{"i":450,"$":{"0":{"v":"A framework to aid in decision making for technical debt management","n":0.302},"1":{"v":"Current technical debt management approaches mainly address specific types of technical debt. This paper introduces a framework to aid in decision making for technical debt management, and it includes those elements considered in technical debt management in the available literature, which are classified in three groups and mapped into three stakeholders' points of view. The research method was systematic mapping. In contrast to current approaches, the framework is not constrained by a concrete type of technical debt. Using this framework it will be possible to build specific models to assist in decision making for technical debt management.","n":0.102}}},{"i":451,"$":{"0":{"v":"Software Cost Estimation with COCOMO II","n":0.408},"1":{"v":"Follow-on book to Dr. Barry Boehm's classic Software Cost Estimation, this book will show professional developers how to use the COCOMO (Cost Comparison Model) II model developed by Dr. Boehm at USC to generate end-to-end cost analysis figures for software development projects.","n":0.154}}},{"i":452,"$":{"0":{"v":"Technical debt: towards a crisper definition report on the 4th international workshop on managing technical debt","n":0.25},"1":{"v":"As the pace of software delivery increases and technology rapidly changes, organizations seek guidance on how to insure the sustainability of their software development effort. Over the past four years running the workshops on Managing Technical Debt, we have seen increased interest from the software industry to understanding and managing technical debt. A better understanding of the concept of technical debt, and how to approach it, both from a theoretical and a practical perspective is necessary to advance its state of the art and practice. In this paper, we highlight the current confusion in industry on the definition of technical debt, their contributions that have led to a deeper understanding of this concept and the limits of the metaphor, the criteria to discriminate what is technical debt and not, and areas of further investigation.","n":0.086}}},{"i":453,"$":{"0":{"v":"On the role of requirements in understanding and managing technical debt","n":0.302},"1":{"v":"Technical debt is the trading of long-term software quality in favor of short-term expediency. While the concept has traditionally been applied to tradeoffs at the code and architecture phases, it also manifests itself in the system requirements analysis phase. Little attention has been paid to requirements over time in software: requirements are often badly out of synch with the implementation, or not used at all. However, requirements are the ultimate validation of project success, since they are the manifestation of the stakeholder's desires for the system. In this position paper, we define technical debt in requirements as the distance between the implementation and the actual state of the world. We highlight how a requirements modeling tool, RE-KOMBINE, makes requirements, domain constraints and implementation first-class concerns. RE-KOMBINE represents technical debt using the notion of optimal solutions to a requirements problem. We show how this interpretation of technical debt may be useful in deciding how much requirements analysis is sufficient.","n":0.08}}},{"i":454,"$":{"0":{"v":"Software project portfolio optimization with advanced multiobjective evolutionary algorithms","n":0.333},"1":{"v":"Large software companies have to plan their project portfolio to maximize potential portfolio return and strategic alignment, while balancing various preferences, and considering limited resources. Project portfolio managers need methods and tools to find a good solution for complex project portfolios and multiobjective target criteria efficiently. However, software project portfolios are challenging to describe for optimization in a practical way that allows efficient optimization. In this paper we propose an approach to describe software project portfolios with a set of multiobjective criteria for portfolio managers using the COCOMO II model and introduce a multiobjective evolutionary approach, mPOEMS, to find the Pareto-optimal front efficiently. We evaluate the new approach with portfolios choosing from a set of 50 projects that follow the validated COCOMO II model criteria and compare the performance of the mPOEMS approach with state-of-the-art multiobjective optimization evolutionary approaches. Major results are as follows: the portfolio management approach was found usable and useful; the mPOEMS approach outperformed the other approaches.","n":0.079}}},{"i":455,"$":{"0":{"v":"An Analysis of Fuzzy Approaches for COCOMO II","n":0.354},"1":{"v":"Software cost estimation is one of the most challenging task in project management. However, the process of estimation is uncertain in nature as it largely depends upon some attributes that are quite unclear during the early stages of development. In this paper a soft computing technique is explored to overcome the uncertainty and imprecision in estimation. The main objective of this research is to investigate the role of fuzzy logic technique in improving the effort estimation accuracy using COCOMO II by characterizing inputs parameters using Gaussian, trapezoidal and triangular membership functions and comparing their results. NASA (93) dataset is used in the evaluation of the proposed Fuzzy Logic COCOMO II. After analyzing the results it had been found that effort estimation using Gaussian member function yields better results for maximum criterions when compared with the other methods.","n":0.085}}},{"i":456,"$":{"0":{"v":"Empirical study on technical debt as viewed by software practitioners","n":0.316}}},{"i":457,"$":{"0":{"v":"A hierarchical model for object-oriented design quality assessment","n":0.354},"1":{"v":"The paper describes an improved hierarchical model for the assessment of high-level design quality attributes in object-oriented designs. In this model, structural and behavioral design properties of classes, objects, and their relationships are evaluated using a suite of object-oriented design metrics. This model relates design properties such as encapsulation, modularity, coupling, and cohesion to high-level quality attributes such as reusability, flexibility, and complexity using empirical and anecdotal information. The relationship or links from design properties to quality attributes are weighted in accordance with their influence and importance. The model is validated by using empirical and expert opinion to compare with the model results on several large commercial object-oriented systems. A key attribute of the model is that it can be easily modified to include different relationships and weights, thus providing a practical quality assessment tool adaptable to a variety of demands.","n":0.084}}},{"i":458,"$":{"0":{"v":"Object-oriented metrics that predict maintainability","n":0.447},"1":{"v":"Abstract   Software metrics have been studied in the procedural paradigm as a quantitative means of assessing the software development process as well as the quality of software products. Several studies have validated that various metrics are useful indicators of maintenance effort in the procedural paradigm. However, software metrics have rarely been studied in the object-oriented paradigm. Very few metrics have been proposed to measure object-oriented systems, and the proposed ones have not been validated. This research concentrates on several object-oriented software metrics and the validation of these metrics with maintenance effort in two commercial systems. Statistical analyses of a prediction model incorporating 10 metrics were performed. In addition, a more compact model with fewer metrics is presented.","n":0.092}}},{"i":459,"$":{"0":{"v":"The Multivariate Social Scientist","n":0.5}}},{"i":460,"$":{"0":{"v":"Statistics notes: The normal distribution","n":0.447},"1":{"v":"When we measure a quantity in a large number of individuals we call the pattern of values obtained a distribution. For example, figure 1 shows the distribution of serum albumin concentration in a sample of adults displayed as a histogram. This is an empirical distribution. There are also theoretical distributions, of which the best known is the normal distribution (sometimes called the Gaussian distribution), which is shown in figure 2. Although widely referred to in statistics, the normal distribution remains a mysterious concept to many. Here we try to explain what it is and why it is important.\n\n\n\nFIG 3 \n(left)—Serum albumin values in 248 adults FIG 2 (right)—Normal distribution with the same mean and standard deviation as the serum albumin values\n\n\n\nIn this context the name “normal” causes much confusion. In statistics it is just a name; …","n":0.085}}},{"i":461,"$":{"0":{"v":"Predicting object-oriented software maintainability using multivariate adaptive regression splines","n":0.333},"1":{"v":"Accurate software metrics-based maintainability prediction can not only enable developers to better identify the determinants of software quality and thus help them improve design or coding, it can also provide managers with useful information to help them plan the use of valuable resources. In this paper, we employ a novel exploratory modeling technique, multiple adaptive regression splines (MARS), to build software maintainability prediction models using the metric data collected from two different object-oriented systems. The prediction accuracy of the MARS models are evaluated and compared using multivariate linear regression models, artificial neural network models, regression tree models, and support vector models. The results suggest that for one system MARS can predict maintainability more accurately than the other four typical modeling techniques, and that for the other system MARS is as accurate as the best modeling technique.","n":0.086}}},{"i":462,"$":{"0":{"v":"An application of Bayesian network for predicting object-oriented software maintainability","n":0.316},"1":{"v":"As the number of object-oriented software systems increases, it becomes more important for organizations to maintain those systems effectively. However, currently only a small number of maintainability prediction models are available for object-oriented systems. This paper presents a Bayesian network maintainability prediction model for an object-oriented software system. The model is constructed using object-oriented metric data in Li and Henry's datasets, which were collected from two different object-oriented systems. Prediction accuracy of the model is evaluated and compared with commonly used regression-based models. The results suggest that the Bayesian network model can predict maintainability more accurately than the regression-based models for one system, and almost as accurately as the best regression-based model for the other system.","n":0.093}}},{"i":463,"$":{"0":{"v":"A Financial Approach for Managing Interest in Technical Debt","n":0.333},"1":{"v":"Technical debt (TD) is a metaphor that is used by both technical and management stakeholders to acknowledge and discuss issues related to compromised design-time qualities. Until now, despite the inherent relevance of technical debt to economics, the TD community has not sufficiently exploited economic methods/models. In this paper we present a framework for managing interest in technical debt, founded on top of Liquidity Preference, a well-known economics theory. To tailor this theory to fit the TD context, we exploit the synthesized knowledge as presented in two recent studies. Specifically, in our framework, we discuss aspects related to technical debt interest, such as: types of TD interest, TD interest characteristics, and a proposed TD interest theory. Finally, to boost the amount of empirical studies in TD research, we propose several tentative research designs that could be used for exploring the notion of interest in technical debt practice.","n":0.083}}},{"i":464,"$":{"0":{"v":"Requirements engineering: a roadmap","n":0.5},"1":{"v":"This paper presents an overview of the field of software systems requirements engineering (RE). It describes the main areas of RE practice, and highlights some key open research issues for the future.","n":0.177}}},{"i":465,"$":{"0":{"v":"A cost-value approach for prioritizing requirements","n":0.408},"1":{"v":"Developing software systems that meet stakeholders' needs and expectations is the ultimate goal of any software provider seeking a competitive edge. To achieve this, you must effectively and accurately manage your stakeholders' system requirements: the features, functions, and attributes they need in their software system. Once you agree on these requirements, you can use them as a focal point for the development process and produce a software system that meets the expectations of both customers and users. However, in real world software development, there are usually more requirements than you can implement given stakeholders' time and resource constraints. Thus, project managers face a dilemma: how do you select a subset of the customers' requirements and still produce a system that meets their needs? The authors developed a cost-value approach for prioritizing requirements and applied it to two commercial projects.","n":0.085}}},{"i":466,"$":{"0":{"v":"Project Management Under Risk: Using the Real Options Approach to Evaluate Flexibility in R&D","n":0.267},"1":{"v":"Managerial flexibility has value in the context of uncertain R&D projects, as management can repeatedly gather information about uncertain project and market characteristics and, based on this information, change its course of action. This value is now well accepted and referred to as \"real option value.\" We introduce, in addition to the familiar real option of abandonment, the option of corrective action that management can take during the project. The intuition from options pricing theory is that higher uncertainty in project payoffs increases the real option value of managerial decision flexibility. However, R&D managers face uncertainty not only in payoffs, but also from many other sources. We identify five example types of R&D uncertainty, in market payoffs, project budgets, product performance, market requirements, and project schedules. How do they influence the value from managerial flexibility? We find that if uncertainty is resolved or costs/revenues occurafter all decisions have been made, more variability may \"smear out\" contingencies and thus reduce the value of flexibility. In addition, variability may reduce the probability of flexibility ever being exercised, which also reduces its value. This result runs counter to established option pricing theory intuition and contributes to a better risk management in R&D projects. Our model builds intuition for R&D managers as to when it is and when it is not worthwhile to delay commitments--for example, by postponing a design freeze, thus maintaining flexibility in R&D projects.","n":0.066}}},{"i":467,"$":{"0":{"v":"Uncertainty, risk, and information value in software requirements and architecture","n":0.316},"1":{"v":"Uncertainty complicates early requirements and architecture decisions and may expose a software project to significant risk. Yet software architects lack support for evaluating uncertainty, its impact on risk, and the value of reducing uncertainty before making critical decisions. We propose to apply decision analysis and multi-objective optimisation techniques to provide such support. We present a systematic method allowing software architects to describe uncertainty about the impact of alternatives on stakeholders' goals; to calculate the consequences of uncertainty through Monte-Carlo simulation; to shortlist candidate architectures based on expected costs, benefits and risks; and to assess the value of obtaining additional information before deciding. We demonstrate our method on the design of a system for coordinating emergency response teams. Our approach highlights the need for requirements engineering and software cost estimation methods to disclose uncertainty instead of hiding it.","n":0.085}}},{"i":468,"$":{"0":{"v":"The fundamental nature of requirements engineering activities as a decision-making process","n":0.302},"1":{"v":"The requirements engineering (RE) process is a decision-rich complex problem solving activity. This paper examines the elements of organization-oriented macro decisions as well as process-oriented micro decisions in the RE process and illustrates how to integrate classical decision-making models with RE process models. This integration helps in formulating a common vocabulary and model to improve the manageability of the RE process, and contributes towards the learning process by validating and verifying the consistency of decision-making in RE activities.","n":0.113}}},{"i":469,"$":{"0":{"v":"Calculating and improving ROI in software and system programs","n":0.333},"1":{"v":"The investment value of innovation follows from a technology's uncertain net present value and derived ROI calculations.","n":0.243}}},{"i":470,"$":{"0":{"v":"Technical Debt in Test Automation","n":0.447},"1":{"v":"Automated test execution is one of the more popular and available strategies to minimize the cost for software testing, and is also becoming one of the central concepts in modern software development as methods such as test-driven development gain popularity. Published studies on test automation indicate that the maintenance and development of test automation tools commonly encounter problems due to unforeseen issues. To further investigate this, we performed a case study on a telecommunication subsystem to seek factors that contribute to inefficiencies in use, maintenance, and development of the automated testing performed within the scope of responsibility of a software design team. A qualitative evaluation of the findings indicates that the main areas of improvement in this case are in the fields of interaction design and general software design principles, as applied to test execution system development.","n":0.085}}},{"i":471,"$":{"0":{"v":"A value-based approach in requirements engineering: explaining some of the fundamental concepts","n":0.289},"1":{"v":"Today's rapid changes and global competition forces software companies to become increasingly competitive and responsive to consumers and market developments. The purpose of requirements engineering activities is to add business value that is accounted for in terms of return-on-investment of a software product. This article introduces some of the fundamental aspects of value by borrowing theories from economic theory, discusses a number of the challenges that face requirements engineers and finally provides a model that illustrates value from business, product and project perspectives.","n":0.11}}},{"i":472,"$":{"0":{"v":"Preparing for an Uncertain Future: Managing Organizations for Strategic Flexibility","n":0.316},"1":{"v":"In recent years, the rapid development of major new technologies, the increasing globalization of markets, the rise of innovative new forms of organizations, and the appearance of new patterns of intense competition have created unprecedented levels of environmental change and uncertainty for organizations of all types. As organizations try to prepare for futures with significant uncertainties, they are finding that many traditional management concepts that have helped to achieve organizational success in stable environments do not effectively prepare organizations for an increasingly dynamic and uncertain future. In the worst cases, following traditional management emphases on optimizing the efficiency of current processes may commit an organization to a narrow focus that severely limits its ability to respond to a changing environment. As an alternative approach to managing for an uncertain future, new management theory and practice have begun to focus on developing an organization's strategic flexibility to respond more readily to changing technological and market opportunities. This paper draws together ideas from recent work on strategic flexibility to present the essential features of a new conceptualization of what strategic managers can do to help prepare organizations for a dynamic and uncertain future (Sanchez, 1991, 1993, 1994, 1995, 1996, 1997; Sanchez and Heene, 1996, 1997; Sanchez and Mahoney, 1994, 1996; Sanchez and Thomas 1996). We begin by defining strategic Ron Sanchez is Assistant Professor at the Graduate School of Management, University of Western Australia, Nedlands, Western Australia 6907, Australia.","n":0.065}}},{"i":473,"$":{"0":{"v":"Valuation of Learning Options in Software Development Under Private and Market Risk","n":0.289},"1":{"v":"ABSTRACT Commercial software development is an inherently uncertain activity. Private risk is high, schedule and cost overruns are common, and market success is elusive. Such circumstances call for a disciplined project evaluation approach. This paper addresses the use of market and earned value management data in assessing the economic value of commercial software development projects that are simultaneously subject to schedule, development cost, and market risk. The assessment is based on real options analysis, a financial valuation technique that can tackle dynamic investment decisions under uncertainty. The paper demonstrates the application of real options analysis to a development scenario that consists of two consecutive stages: a mandatory prototyping stage and an optional full-development stage. The full-development stage is undertaken only if the prototype is successful and the market outlook is sufficiently positive at the end of the prototyping stage, thus giving the full-development stage the...","n":0.083}}},{"i":474,"$":{"0":{"v":"Real options valuation","n":0.577},"1":{"v":"Managerial flexibility has value. The ability of their managers to make smart decisions in the face of volatile market and technological conditions is essential for firms in any competitive industry. This advanced tutorial describes the use of Monte Carlo simulation and stochastic optimization for the valuation of real options that arise from the abilities of managers to influence the cash flows of the projects under their control.   Option pricing theory supplements discounted cash flow methods of valuation by considering managerial flexibility. Managers' options to take actions that affect real investment projects are comparable to options on the sale or purchase of financial assets. Just as a financial option derives much of its value from the potential price movements of the underlying financial asset, a real option derives much of its value from the potential fluctuations of the cash flows generating the value of the investment project.","n":0.083}}},{"i":475,"$":{"0":{"v":"ArchOptions: A Real Options-Based Model for Predicting the Stability of Software Architectures","n":0.289},"1":{"v":"Architectural stability refers to the extent an architecture is flexible to endure evolutionary changes in stakeholders\\' requirements and the environment. We assume that the primary goal of software architecture is to guide the system\\'s evolution. We contribute to a novel model that exploits options theory to predict architectural stability. The model is predictive: it provides \\\"insights\\\" on the evolution of the software system based on valuing the extent an architecture can endure a set of likely evolutionary changes. The model builds on Black and Scholes financial options theory (Noble Prize wining) to value such extent. We show how we have derived the model: the analogy and assumptions made to reach the model, its formulation, and possible interpretations. We refer to this model as ArchOptions.","n":0.09}}},{"i":476,"$":{"0":{"v":"The politics of requirements management","n":0.447},"1":{"v":"There are all sorts of requirements management, engineering, and specification methods out there. Most if not all were created by people who like precision, diagnosticity, and rigor-and who care about the software design and development process to the relative neglect of other, more organizational processes. Requirements management is a political process, not a technical one. Programmers need good user and software requirements specifications to write \"good\" code. And many contracting officers need to see requirements documentation before they'll pay invoices. But requirements management's real importance lies in its other functions: to control expectations, to focus or diffuse blame for the inevitable, and to provide air cover for the otherwise well-meaning but naive programming teams that actually think they're satisfying user requirements. Let's step back a moment and ask some fundamental questions about where software projects come from. Some are obviously well bred: users (end users and business partners) discover some real problems that can be solved by modifying a computer program somewhere. Others come from strategic decisions about a company's line of business. But most come from preferences, from intuition about value, from the stores of pet projects we all carry around with us, or from programmers' lists of changes they'd like to make but weren't initially funded to implement. The author opines that requirements data is highly perishable, often inaccurate, and frequently manipulated to serve all-but-invisible political agendas. In short, most projects could not pass a purposeful requirements analysis test to save their life.","n":0.064}}},{"i":477,"$":{"0":{"v":"Detection strategies: metrics-based rules for detecting design flaws","n":0.354},"1":{"v":"In order to support the maintenance of an object-oriented software system, the quality of its design must be evaluated using adequate quantification means. In spite of the current extensive use of metrics, if used in isolation metrics are oftentimes too fine grained to quantify comprehensively an investigated design aspect (e.g., distribution of system's intelligence among classes). To help developers and maintainers detect and localize design problems in a system, we propose a novel mechanism - called detection strategy - for formulating metrics-based rules that capture deviations from good design principles and heuristics. Using detection strategies an engineer can directly localize classes or methods affected by a particular design flaw (e.g., God Class), rather than having to infer the real design problem from a large set of abnormal metric values. We have defined such detection strategies for capturing around ten important flaws of object-oriented design found in the literature and validated the approach experimentally on multiple large-scale case-studies.","n":0.08}}},{"i":478,"$":{"0":{"v":"A systematic literature review on agile requirements engineering practices and challenges","n":0.302},"1":{"v":"We mapped out 17 requirements engineering practices adopted by agile practitioners so far.Identified 5 challenges of traditional requirements engineering overcome by adopting agile requirements engineering.Found 8 challenges posed by following agile requirements engineering.Findings suggest that research context needs attention in terms of more empirical.The empirical results can help to analyse impact of adopting agile requirements engineering. Unlike traditional software development methods, agile methods are marked by extensive collaboration, i.e. face-to-face communication. Although claimed to be beneficial, the software development community as a whole is still unfamiliar with the role of the requirements engineering practices in agile methods. The term \"agile requirements engineering\" is used to define the \"agile way\" of planning, executing and reasoning about requirements engineering activities. Moreover, not much is known about the challenges posed by collaboration-oriented agile way of dealing with requirements engineering activities. Our goal is to map the evidence available about requirements engineering practices adopted and challenges faced by agile teams in order to understand how traditional requirements engineering issues are resolved using agile requirements engineering. We conducted a systematic review of literature published between 2002 and June 2013 and identified 21 papers, that discuss agile requirements engineering. We formulated and applied specific inclusion and exclusion criteria in two distinct rounds to determine the most relevant studies for our research goal. The review identified 17 practices of agile requirements engineering, five challenges traceable to traditional requirements engineering that were overcome by agile requirements engineering, and eight challenges posed by the practice of agile requirements engineering. However, our findings suggest that agile requirements engineering as a research context needs additional attention and more empirical results are required to better understand the impact of agile requirements engineering practices e.g. dealing with non-functional requirements and self-organising teams.","n":0.059}}},{"i":479,"$":{"0":{"v":"Software Product Quality Control","n":0.5},"1":{"v":"Quality is not a fixed or universal property of software; it depends on the context and goals of its stakeholders. Hence, when you want to develop a high-quality software system, the first step must be a clear and precise specification of quality. Yet even if you get it right and complete, you can be sure that it will become invalid over time. So the only solution is continuous quality control: the steady and explicit evaluation of a products properties with respect to its updated quality goals. This book guides you in setting up and running continuous quality control in your environment. Starting with a general introduction on the notion of quality, it elaborates what the differences between process and product quality are and provides definitions for quality-related terms often used without the required level of precision. On this basis, the work then discusses quality models as the foundation of quality control, explaining how to plan desired product qualities and how to ensure they are delivered throughout the entire lifecycle. Next it presents the main concepts and techniques of continuous quality control, discussing the quality control loop and its main techniques such as reviews or testing. In addition to sample scenarios in all chapters, the book is rounded out by a dedicated chapter highlighting several applications of different subsets of the presented quality control techniques in an industrial setting. The book is primarily intended for practitioners working in software engineering or quality assurance, who will benefit by learning how to improve their current processes, how to plan for quality, and how to apply state-of-the-art quality control techniques. Students and lecturers in computer science and specializing in software engineering will also profit from this book, which they can use in practice-oriented courses on software quality, software maintenance and quality assurance.","n":0.058}}},{"i":480,"$":{"0":{"v":"3-D visualization of dynamic runtime structures","n":0.408},"1":{"v":"Continued development and maintenance of software requires understanding its design and behavior. Software at runtime creates a complex network of call--callee relationships that are hard to determine but that developers need to understand to optimize software performance. Existing tools typically focus on static aspects (e.g., Structure101 or SonarQube), or they are difficult to use and require high expertise (e.g., software profiling tools).   Unfortunately, these dependencies are hard to derive from static code analysis: For one, static analysis will reveal potential call--callee relationships not actual ones. Second, they are often difficult to detect, since information systems today increasingly use abstraction patterns and code injection, which obscures runtime behavior.   In this paper, we present our efforts towards accessible and informative means of visualizing software runtime processes. We designed a novel visualization approach that utilizes a hierarchical and interactive 3-D city layout based on force-directed graphs to display the runtime structure of an application. This promises to reduce the time and effort invested in debugging programming errors or in finding bottlenecks of software performance.   Our approach extends the city metaphor for translating programmatic relationships into accessible 3D visualizations. With the identified goals and constraints in mind, we designed a novel visual debugging system, which maps programming code structures to 3D city layouts based on force-directed graphs. Exploration of the animated visualization allows the user to investigate not only the static relationships of large software projects but also its dynamic runtime behavior.   We conducted a formative evaluation of the approach with a preliminary version of a prototype. In a series of six interviews with experts in software development and dynamic analysis, we were able to confirm that the approach is useful and supports identifying bottlenecks. The interviews raised and prioritized potential future improvements, several of which we implemented into the final version of our prototype.","n":0.058}}},{"i":481,"$":{"0":{"v":"Dynamische Analyse mit dem Software-EKG","n":0.447},"1":{"v":"Dieses Papier zeigt, wie man komplexe, heterogene Systeme analysiert, wenn die einfachen Methoden (Debugger, Profiler) nicht ausreichen. Wir erlautern die Grundlagen, beschreiben ein Vorgehen mit den notigen Tools und bringen einige Beispiele aus unserer Praxis. Wir behandeln ferner den praventiven Einsatz des Vorgehens im Entwicklungsprozess und definieren die Diagnostizierbarkeit (Diagnosibility) eines Softwaresystems als wichtige nichtfunktionale Eigenschaft.","n":0.134}}},{"i":482,"$":{"0":{"v":"Zero-Tolerance Construction","n":0.707}}},{"i":483,"$":{"0":{"v":"Managing Technical Debt in Enterprise Software Packages","n":0.378},"1":{"v":"We develop an evolutionary model and theory of software technical debt accumulation to facilitate a rigorous and balanced analysis of its benefits and costs in the context of a large commercial enterprise software package. Our theory focuses on the optimization problem involved in managing technical debt, and illustrates the different tradeoff patterns between software quality and customer satisfaction under early and late adopter scenarios at different lifecycle stages of the software package. We empirically verify our theory utilizing a ten year longitudinal data set drawn from 69 customer installations of the software package. We then utilize the empirical results to develop actionable policies for managing technical debt in enterprise software product adoption.","n":0.096}}},{"i":484,"$":{"0":{"v":"Establishing a framework for managing interest in technical debt","n":0.333}}},{"i":485,"$":{"0":{"v":"An approach to detecting duplicate bug reports using natural language and execution information","n":0.277},"1":{"v":"An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as duplicate and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. In this paper we present a new approach that further involves execution information. In our approach, when a new bug report arrives, its natural language information and execution information are compared with those of the existing bug reports. Then, a small number of existing bug reports are suggested to the triager as the most similar bug reports to the new bug report. Finally, the triager examines the suggested bug reports to determine whether the new bug report duplicates an existing bug report. We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67%-93% of duplicate bug reports in the Firefox bug repository, compared to 43%-72% using natural language information alone.","n":0.068}}},{"i":486,"$":{"0":{"v":"What Makes a Good Bug Report","n":0.408},"1":{"v":"In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.","n":0.072}}},{"i":487,"$":{"0":{"v":"Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows","n":0.277},"1":{"v":"We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter.   Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.","n":0.072}}},{"i":488,"$":{"0":{"v":"Developer prioritization in bug repositories","n":0.447},"1":{"v":"Developers build all the software artifacts in development. Existing work has studied the social behavior in software repositories. In one of the most important software repositories, a bug repository, developers create and update bug reports to support software development and maintenance. However, no prior work has considered the priorities of developers in bug repositories. In this paper, we address the problem of the developer prioritization, which aims to rank the contributions of developers. We mainly explore two aspects, namely modeling the developer prioritization in a bug repository and assisting predictive tasks with our model. First, we model how to assign the priorities of developers based on a social network technique. Three problems are investigated, including the developer rankings in products, the evolution over time, and the tolerance of noisy comments. Second, we consider leveraging the developer prioritization to improve three predicted tasks in bug repositories, i.e., bug triage, severity identification, and reopened bug prediction. We empirically investigate the performance of our model and its applications in bug repositories of Eclipse and Mozilla. The results indicate that the developer prioritization can provide the knowledge of developer priorities to assist software tasks, especially the task of bug triage.","n":0.071}}},{"i":489,"$":{"0":{"v":"TODO or to bug: exploring how task annotations play a role in the work practices of software developers","n":0.236},"1":{"v":"Software development is a highly collaborative activity that requires teams of developers to continually manage and coordinate their programming tasks. In this paper, we describe an empirical study that explored how task annotations embedded within the source code play a role in how software developers manage personal and team tasks. We present findings gathered by combining results from a survey of professional software developers, an analysis of code from open source projects, and interviews with software developers. Our findings help us describe how task annotations can be used to support a variety of activities fundamental to articulation work within software development. We describe how task management is negotiated between the more formal issue tracking systems and the informal annotations that programmers write within their source code. We report that annotations have different meanings and are dependent on individual, team and community use. We also present a number of issues related to managing annotations, which may have negative implications for maintenance. We conclude with insights into how these findings could be used to improve tool support and software process.","n":0.075}}},{"i":490,"$":{"0":{"v":"automatic bug triage using semi-supervised text classification","n":0.378},"1":{"v":"Abstract —In this paper, we propose a semi-supervised text classification approach for bug triage to avoid the deficiency of labeled bug reports in existing supervised approaches. This new approach combines naive Bayes classifier and expectation-maximization to take advantage of both labeled and unlabeled bug reports. This approach trains a classifier with a fraction of labeled bug reports. Then the approach iteratively labels numerous unlabeled bug reports and trains a new classifier with labels of all the bug reports. We also employ a weighted recommendation list to boost the performance by imposing the weights of multiple developers in training the classifier. Experimental results on bug reports of Eclipse show that our new approach outperforms existing supervised approaches in terms of classification accuracy. Keywords- automatic bug triage; expectation-maximization; semi-supervised text classification; weighted recommendation list I. I NTRODUCTION Most of large software projects employ a bug tracking system (bug repository) to manage bugs and developers. In software development and maintenance, a bug repository is a significant software repository for storing the bugs submitted by","n":0.076}}},{"i":491,"$":{"0":{"v":"Solving the Large Scale Next Release Problem with a Backbone-Based Multilevel Algorithm","n":0.289},"1":{"v":"The Next Release Problem (NRP) aims to optimize customer profits and requirements selection for the software releases. The research on the NRP is restricted by the growing scale of requirements. In this paper, we propose a Backbone-based Multilevel Algorithm (BMA) to address the large scale NRP. In contrast to direct solving approaches, the BMA employs multilevel reductions to downgrade the problem scale and multilevel refinements to construct the final optimal set of customers. In both reductions and refinements, the backbone is built to fix the common part of the optimal customers. Since it is intractable to extract the backbone in practice, the approximate backbone is employed for the instance reduction while the soft backbone is proposed to augment the backbone application. In the experiments, to cope with the lack of open large requirements databases, we propose a method to extract instances from open bug repositories. Experimental results on 15 classic instances and 24 realistic instances demonstrate that the BMA can achieve better solutions on the large scale NRP instances than direct solving approaches. Our work provides a reduction approach for solving large scale problems in search-based requirements engineering.","n":0.073}}},{"i":492,"$":{"0":{"v":"Reinforcing Software Quality Measure by Indirect Coupling Metrics","n":0.354}}},{"i":493,"$":{"0":{"v":"Review: information technology and organizational performance: an integrative model of it business value","n":0.277},"1":{"v":"Despite the importance to researchers, managers, and policy makers of how information technology (IT) contributes to organizational performance, there is uncertainty and debate about what we know and don't know. A review of the literature reveals that studies examining the association between information technology and organizational performance are divergent in how they conceptualize key constructs and their interrelationships. We develop a model of IT business value based on the resource-based view of the firm that integrates the various strands of research into a single framework. We apply the integrative model to synthesize what is known about IT business value and guide future research by developing propositions and suggesting a research agenda. A principal finding is that IT is valuable, but the extent and dimensions are dependent upon internal and external factors, including complementary organizational resources of the firm and its trading partners, as well as the competitive and macro environment. Our analysis provides a blueprint to guide future research and facilitate knowledge accumulation and creation concerning the organizational performance impacts of information technology.","n":0.076}}},{"i":494,"$":{"0":{"v":"The Value-Relevance of Intangibles: The Case of Software Capitalization","n":0.333},"1":{"v":"We examine in this study the relevance to investors of information on the capitalization of software development costs, as promulgated in 1985 by the Financial Accounting Standards Board in its Statement No. 86 (SFAS 86). We find that software capitalization is value-relevant to investors: The annually capitalized development costs are positively and significantly associated with stock returns and the cumulative software asset reported on the balance sheet is associated with stock prices. Furthermore, software capitalization figures are associated with subsequent reported earnings, indicating another dimension of relevance to investors. We also find that investors undervalue firms that expense all their software development costs. Finally, we find no support for the frequent argument that the judgment and subjectivity involved in software capitalization adversely affects the quality of reported earnings. We also investigate why the industry petitioned the FASB, in March 1996, to abolish SFAS 86. We document a significant shift in the mid-1990s in the impact of software capitalization on reported earnings and return-on-equity of software companies. Whereas in the early period of SFAS 86 application (mid- to late-1980s) software capitalization enhanced reported earnings considerably more than its detraction by the amortization of the software asset (since that asset was still small), during the early 1990s the gap between capitalization and amortization narrowed, and in 1995, the amortization?s negative impact on reported profitability roughly offset the positive impact of capitalization. This diminished impact of capitalization on reported performance may have been among the reasons underlying the petition to abolish SFAS 86. Finally, we find that analysts? earnings forecast errors are positively and significantly associated with the intensity of software capitalization.","n":0.061}}},{"i":495,"$":{"0":{"v":"Understanding and controlling software costs","n":0.447},"1":{"v":"A discussion is presented of the two primary ways of understanding software costs. The black-box or influence-function approach provides useful experimental and observational insights on the relative software productivity and quality leverage of various management, technical, environmental, and personnel options. The glass-box or cost distribution approach helps identify strategies for integrated software productivity and quality improvement programs using such structures as the value chain and the software productivity opportunity tree. The individual strategies for improving software productivity are identified. Issues related to software costs and controlling them are examined and discussed. It is pointed out that a good framework of techniques exists for controlling software budgets, schedules, and work completed, but that a great deal of further progress is needed to provide an overall set of planning and control techniques covering software product qualities and end-user system objectives. >","n":0.085}}},{"i":496,"$":{"0":{"v":"A Probabilistic Approach to Web Portal's Data Quality Evaluation","n":0.333},"1":{"v":"Advances in technology and the use of the Internet have favoured the emergence of a large number of Web applications, including Web Portals. Web portals provide the means to obtain a large amount of information therefore it is crucial that the information provided is of high quality. In recent years, several research projects have investigated Web Data Quality; however none has focused on data quality within the context of Web Portals. Therefore, the contribution of this research is to provide a framework centred on the point of view of data consumers, and that uses a probabilistic approach for Web portal's data quality evaluation. This paper shows the definition of operational model, based in our previous work.","n":0.093}}},{"i":497,"$":{"0":{"v":"Accounting Software Assets: A Valuation Model for Software","n":0.354},"1":{"v":"Enterprise Information Technology Systems are major corporate assets upon which corporate management and operations are heavily dependent. Current accounting standards for the treatment of these assets have not kept pace with advances in technology, such as models for creating and evolving IT. Capitalizable costs according to the standards are principally primary development costs and exclude system evolution. This weakness of the standards creates a disproportionate downward bias in the book value of software and in current earnings, as costs incurred for on‐going systems evolution is five to twenty times the costs incurred for first release. We present a quantitative valuation model for IT systems based upon engineering measurements of software that allow the fair value of software to be based on all costs incurred by the system. Costs are collected by an automatic tool and stored in an inventory system of enterprise software assets. To total costs is added the effect of each individual module's relativ...","n":0.08}}},{"i":498,"$":{"0":{"v":"Accounting for Expenditure on Software Development for Internal Use","n":0.333},"1":{"v":"The methods accepted by Australian, International, U.S. and U.K. Accounting Standards for the treatment of expenditure on software development are inconsistent, and permissive. A host of methods for recording capitalized software in terms of those standards is identified by reference to an illustrative case study. It is questionable whether many in-house developed software applications satisfy the professionally endorsed definition of ‘asset’. Moreover, even if accounting standards significantly reduce the range of options for capitalizing expenditure on software development, there would still be many values which could be assigned to capitalized software. That suggests that those ‘measures’ are not reliable, so that it would be inappropriate initially to recognize software expenditure as an ‘asset’. It is contended that expensing all outlays on software development as they are incurred (accompanied by reporting that expenditure as a line item in statements of financial performance, and expanded disclosures in notes) is likely to provide a clearer and more useful report on business operations than the alternative of capitalization, amortization and subsequent assessments of whether or not recorded values should be adjusted for ‘impairment’.","n":0.075}}},{"i":499,"$":{"0":{"v":"A review of quantitative IT value research","n":0.378}}},{"i":500,"$":{"0":{"v":"Software security in DevOps: synthesizing practitioners' perceptions and practices","n":0.333},"1":{"v":"In organizations that use DevOps practices, software changes can be deployed as fast as 500 times or more per day. Without adequate involvement of the security team, rapidly deployed software changes are more likely to contain vulnerabilities due to lack of adequate reviews. The goal of this paper is to aid software practitioners in integrating security and DevOps by summarizing experiences in utilizing security practices in a DevOps environment. We analyzed a selected set of Internet artifacts and surveyed representatives of nine organizations that are using DevOps to systematically explore experiences in utilizing security practices. We observe that the majority of the software practitioners have expressed the potential of common DevOps activities, such as automated monitoring, to improve the security of a system. Furthermore, organizations that integrate DevOps and security utilize additional security activities, such as security requirements analysis and performing security configurations. Additionally, these teams also have established collaboration between the security team and the development and operations teams.","n":0.079}}},{"i":501,"$":{"0":{"v":"What is social debt in software engineering","n":0.378},"1":{"v":"“Social debt” in software engineering informally refers to unforeseen project cost connected to a “suboptimal” development community. The causes of suboptimal development communities can be many, ranging from global distance to organisational barriers to wrong or uninformed socio-technical decisions (i.e., decisions that influence both social and technical aspects of software development). Much like technical debt, social debt impacts heavily on software development success. We argue that, to ensure quality software engineering, practitioners should be provided with mechanisms to detect and manage the social debt connected to their development communities. This paper defines and elaborates on social debt, pointing out relevant research paths. We illustrate social debt by comparison with technical debt and discuss common real-life scenarios that exhibit “sub-optimal” development communities.","n":0.091}}},{"i":502,"$":{"0":{"v":"A position study to investigate technical debt associated with security weaknesses","n":0.302},"1":{"v":"Context: Managing technical debt (TD) associated with potential security breaches found during design can lead to catching vulnerabilities (i.e., exploitable weaknesses) earlier in the software lifecycle; thus, anticipating TD principal and interest that can have decidedly negative impacts on businesses. Goal: To establish an approach to help assess TD associated with security weaknesses by leveraging the Common Weakness Enumeration (CWE) and its scoring mechanism, the Common Weakness Scoring System (CWSS). Method: We present a position study with a five-step approach employing the Quamoco quality model to operationalize the scoring of architectural CWEs. Results: We use static analysis to detect design level CWEs, calculate their CWSS scores, and provide a relative ranking of weaknesses that help practitioners identify the highest risks in an organization with a potential to impact TD. Conclusion: CWSS is a community agreed upon method that should be leveraged to help inform the ranking of security related TD items.","n":0.081}}},{"i":503,"$":{"0":{"v":"QATCH - An adaptive framework for software product quality assessment","n":0.316},"1":{"v":"Abstract   The subjectivity that underlies the notion of quality does not allow the design and development of a universally accepted mechanism for software quality assessment. This is why contemporary research is now focused on seeking mechanisms able to produce software quality models that can be easily adjusted to custom user needs. In this context, we introduce QATCH, an integrated framework that applies static analysis to benchmark repositories in order to generate software quality models tailored to stakeholder specifications. Fuzzy multi-criteria decision-making is employed in order to model the uncertainty imposed by experts’ judgments. These judgments can be expressed into linguistic values, which makes the process more intuitive. Furthermore, a robust software quality model, the base model, is generated by the system, which is used in the experiments for QATCH system verification. The paper provides an extensive analysis of QATCH and thoroughly discusses its validity and added value in the field of software quality through a number of individual experiments.","n":0.079}}},{"i":504,"$":{"0":{"v":"An industry perspective to comparing the SQALE and quamoco software quality models","n":0.289},"1":{"v":"Context: We investigate the different perceptions of quality provided by leading operational quality models when used to evaluate software systems from an industry perspective. Goal: To compare and evaluate the quality assessments of two competing quality models and to develop an extensible solution to meet the quality assurance measurement needs of an industry stakeholder -The Construction Engineering Research Laboratory (CERL). Method: In cooperation with our industry partner TechLink, we operationalize the Quamoco quality model and employ a multiple case study design comparing the results of Quamoco and SQALE, two implementations of well known quality models. The study is conducted across current versions of several open source software projects sampled from GitHub and commercial software for sustainment management systems implemented in the C# language from our industry partner. Each project represents a separate embedded unit of study in a given context -open source or commercial. We employ inter-rater agreement and correlation analysis to compare the results of both models, focusing on Maintainability, Reliability, and Security assessments. Results: Our observations suggest that there is a significant disconnect between the assessments of quality under both quality models. Conclusion: In order to support industry adoption, additional work is required to bring competing implementations of quality models into alignment. This exploratory case study helps us shed light into this problem.","n":0.068}}},{"i":505,"$":{"0":{"v":"Etude comparative de la distribution florale dans une portion des Alpes et des Jura","n":0.267}}},{"i":506,"$":{"0":{"v":"Investment Opportunities as Real Options: Getting Started on the Numbers","n":0.316}}},{"i":507,"$":{"0":{"v":"The Epsilon Transformation Language","n":0.5},"1":{"v":"Support for automated model transformation is essential for realizing a Model Driven Development (MDD) process. However, model transformation is only one of the many tools in a model engineering toolkit. To apply MDD in the large, automated support for a number of additional tasks such as model comparison, merging, validation and model-to-text transformation, is essential. While a number of successful model transformation languages have been currently proposed, the majority of them have been developed in isolation and as a result, they face consistency and integration difficulties with languages that support other model management tasks. We present the Epsilon Transformation Language (ETL), a hybrid model transformation language that has been developed atop the infrastructure provided by the Epsilon model management platform. By building atop Epsilon, ETL is seamlessly integrated with a number of other task-specific languages to help to realize composite model management workflows.","n":0.084}}},{"i":508,"$":{"0":{"v":"Evaluation of model transformation approaches for model refactoring","n":0.354},"1":{"v":"This paper provides a systematic evaluation framework for comparing model transformation approaches, based upon the ISO/IEC 9126-1 quality characteristics for software systems. We apply this framework to compare five transformation approaches (QVT-R, ATL, Kermeta, UML-RSDS and GrGen.NET) on a complex model refactoring case study: the amalgamation of apparent attribute clones in a class diagram. The case study highlights the problems with the specification and design of the refactoring category of model transformations, and provides a challenging example by which model transformation languages and approaches can be compared. We take into account a wide range of evaluation criteria aspects such as correctness, efficiency, flexibility, interoperability, re-usability and robustness, which have not been comprehensively covered by other comparative surveys of transformation approaches. The results show clear distinctions between the capabilities and suitabilities of different approaches to address the refactoring form of transformation problem.","n":0.084}}},{"i":509,"$":{"0":{"v":"Model-Transformation Design Patterns","n":0.577},"1":{"v":"This paper defines a catalogue of patterns for the specification and design of model transformations, and provides a systematic scheme and classification of these patterns, together with pattern application examples in leading model transformation languages such as ATL, QVT, GrGen.NET, and others. We consider patterns for improving transformation modularization and efficiency and for reducing data storage requirements. We define a metamodel-based formalization of model transformation design patterns, and measurement-based techniques to guide the selection of patterns. We also provide an evaluation of the effectiveness of transformation patterns on a range of different case studies.","n":0.103}}},{"i":510,"$":{"0":{"v":"Performance in model transformations: experiments with ATL and QVT","n":0.333},"1":{"v":"Model transformations are increasingly being incorporated in software development processes. However, as systems being developed with transformations grow in size and complexity, the performance of the transformations tends to degrade. In this paper we investigate the factors that have an impact on the execution performance of model transformations. We analyze the performance of three model transformation language engines, namely ATL, QVT Operational Mappings and QVT Relations. We implemented solutions to two transformation problems in these languages and compared the performance of these transformations. We extracted metric values from the transformations to systematically analyze how their characteristics influence transformation execution performance. We also implemented a solution to a transformation problem in ATL in three functionally equivalent ways, but with different language constructs to evaluate the effect of language constructs on transformation performance. The results of this paper enable a transformation designer to estimate beforehand the performance of a transformation, and to choose among implementation alternatives to achieve the best performance. In addition, transformation engine developers may find some of our results useful in order to tune their tools for better performance.","n":0.075}}},{"i":511,"$":{"0":{"v":"A Catalogue of Refactorings for Model-to-Model Transformations","n":0.378},"1":{"v":"In object-oriented programming, continuous refactorings are used as the main mechanism to increase the maintainability of the code base. Unfortunately, in the field of model transformations, such refac- toring support is so far missing. This paper tackles this limitation by adapting the notion of refactorings to model-to-model (M2M) transfor- mations. In particular, we present a dedicated catalogue of refactorings for improving the quality of M2M transformations. The refactorings have been explored by analyzing existing transformation examples defined in ATL. However, the refactorings are not specifically tailored to ATL, but applicable also to other M2M transformation languages.","n":0.102}}},{"i":512,"$":{"0":{"v":"SonarQube in Action","n":0.577},"1":{"v":"Summary SonarQube in Action shows developers how to use the SonarQube platform to help them continuously improve their source code. The book presents SonarQube's core Seven Axes of Quality: design/architecture, duplications, comments, unit tests, complexity, potential bugs, and coding rules. You'll find simple, easy-to-follow discussion and examples as you learn to integrate SonarQube into your development process. About the Technology SonarQube is a powerful open source tool for continuous inspection, a process that makes code quality analysis and reporting an integral part of the development lifecycle. Its unique dashboards, rule-based defect analysis, and tight build integration result in improved code quality without disruption to developer workflow. It supports many languages, including Java, C, C++, C#, PHP, and JavaScript. About the Book SonarQube in Action teaches you how to effectively use SonarQube following the continuous inspection model. This practical book systematically explores SonarQube's core Seven Axes of Quality (design, duplications, comments, unit tests, complexity, potential bugs, and coding rules). With well-chosen examples, it helps you learn to use SonarQube's review functionality and IDE integration to implement continuous inspection best practices in your own quality management process. The book's Java-based examples translate easily to other development languages. No prior experience with SonarQube or continuous delivery practice is assumed Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. What's InsideGather meaningful quality metrics Integrate with Ant, Maven, and Jenkins Write your own plugins Master the art of continuous inspectionAbout the Authors Ann Campbellb and Patroklos Papapetrou are experienced developers and team leaders. Both actively contribute to the SonarQube community. Table of ContentsPART 1 WHAT THE NUMBERS ARE TELLING YOU An introduction to SonarQubeIssues and coding standardsEnsuring that your code is doing things rightWorking with duplicate codeOptimizing source code documentationKeeping your source code files elegantImproving your application designPART 2 SETTLING IN WITH SONARQUBE Planning a strategy and expanding your insightContinuous Inspection with SonarQubeLetting SonarQube drive code reviewsIDE integrationPART 3 ADMINISTERING AND EXTENDING Security: users, groups, and roles Rule profile administration Making SonarQube fit your needsManaging your projectsWriting your own plugins","n":0.054}}},{"i":513,"$":{"0":{"v":"The Evolution of Technical Debt in the Apache Ecosystem","n":0.333},"1":{"v":"Software systems must evolve over time or become increasingly irrelevant says one of Lehman’s laws of software evolution. Many studies have been presented in the literature that investigate the evolution of software systems but few have focused on the evolution of technical debt. In this paper we study sixty-six Java open-source software projects from the Apache ecosystem focusing on the evolution of technical debt. We analyze the evolution of these systems over the past five years at the temporal granularity level of weekly snapshots. We calculate the trends of the technical debt time series but we also investigate the lower-level constituent components of this technical debt. We aggregate some of the information to the ecosystem level.","n":0.093}}},{"i":514,"$":{"0":{"v":"Evaluating maintainability with code metrics for model-to-model transformations","n":0.354},"1":{"v":"Using model-to-model transformations to generate analysis models or code from architecture models is sought to promote compliance and reuse of components. The maintainability of transformations is influenced by various characteristics - as with every programming language artifact. Code metrics are often used to estimate code maintainability. However, most of the established metrics do not apply to declarative transformation languages (such as QVT Relations) since they focus on imperative (e.g. object-oriented) coding styles. One way to characterize the maintainability of programs are code metrics. However, the vast majority of these metrics focus on imperative (e.g., object-oriented) coding styles and thus cannot be reused as-is for transformations written in declarative languages. In this paper we propose an initial set of quality metrics to evaluate transformations written in the declarative QVT Relations language. We apply the presented set of metrics to several reference transformations to demonstrate how to judge transformation maintainability based on our metrics.","n":0.081}}},{"i":515,"$":{"0":{"v":"A systematic approach to evaluating domain-specific modeling language environments for multi-agent systems","n":0.289},"1":{"v":"Multi-agent systems (MASs) include multiple interacting agents within an environment to provide a solution for complex systems that cannot be easily solved with individual agents or monolithic systems. However, the development of MASs is not trivial due to the various agent properties such as autonomy, responsiveness, and proactiveness, and the need for realization of the many different agent interactions. To support the development of MASs various domain-specific modeling languages (DSMLs) have been introduced that provide a declarative approach for modeling and supporting the generation of agent-based systems. To be effective, the proposed DSMLs need to meet the various stakeholder concerns and the related quality criteria for the corresponding MASs. Unfortunately, very often the evaluation of the DSML is completely missing or has been carried out in idiosyncratic approach. If the DSMLs are not well defined, then implicitly this will have an impact on the quality of the MASs. In this paper, we present an evaluation framework and systematic approach for assessing existing or newly defined DSMLs for MASs. The evaluation is specific for MAS DSMLs and targets both the language and the corresponding tools. To illustrate the evaluation approach, we first present SEA_ML, which is a model-driven MAS DSML for supporting the modeling and generation of agent-based systems. The evaluation of SEA_ML is based on a multi-case study research approach and provides both qualitative evaluation and quantitative analysis. We report on the lessons learned considering the adoption of the evaluation approach as well as the SEA_ML for supporting the generation of agent-based systems.","n":0.063}}},{"i":516,"$":{"0":{"v":"Optimization Patterns for OCL-Based Model Transformations","n":0.408},"1":{"v":"Writing queries and navigation expressions in OCL is an important part of the task of developing a model transformation definition. When such queries are complex and the size of the models is significant, performance issues cannot be neglected.\r\n\r\nIn this paper we present five patterns intended to optimize the performance of model transformations when OCL queries are involved. For each pattern we will give an example as well as several implementation alternatives. Experimental data gathered by running benchmarks is also shown to compare the alternatives.","n":0.109}}},{"i":517,"$":{"0":{"v":"Assessing and improving quality of QVTo model transformations","n":0.354},"1":{"v":"We investigate quality improvement in QVT operational mappings (QVTo) model transformations, one of the languages defined in the OMG standard on model-to-model transformations. Two research questions are addressed. First, how can we assess quality of QVTo model transformations? Second, how can we develop higher-quality QVTo transformations? To address the first question, we utilize a bottom---up approach, starting with a broad exploratory study including QVTo expert interviews, a review of existing material, and introspection. We then formalize QVTo transformation quality into a QVTo quality model. The quality model is validated through a survey of a broader group of QVTo developers. We find that although many quality properties recognized as important for QVTo do have counterparts in general purpose languages, a number of them are specific to QVTo or model transformation languages. To address the second research question, we leverage the quality model to identify developer support tooling for QVTo. We then implemented and evaluated one of the tools, namely a code test coverage tool. In designing the tool, code coverage criteria for QVTo model transformations are also identified. The primary contributions of this paper are a QVTo quality model relevant to QVTo practitioners and an open-source code coverage tool already usable by QVTo transformation developers. Secondary contributions are a bottom---up approach to building a quality model, a validation approach leveraging developer perceptions to evaluate quality properties, code test coverage criteria for QVTo, and numerous directions for future research and tooling related to QVTo quality.","n":0.064}}},{"i":518,"$":{"0":{"v":"Translation of QVT Relations into QVT Operational Mappings","n":0.354},"1":{"v":"Model transformations play a key role in Model-Driven Engineering solutions. To efficiently develop, specify, and manage model transformations, it is often necessary to use a combination of languages that stand for different transformation approaches. To provide a basis for such hybrid model transformation specification solutions, we developed and implemented a translation of the declarative QVT Relations into the imperative QVT Operational Mappings language.","n":0.126}}},{"i":519,"$":{"0":{"v":"Technical Debt in Model Transformation Specifications","n":0.408},"1":{"v":"Model transformations (MT), as with any other software artifact, may contain quality flaws. Even if a transformation is functionally correct, such flaws will impair maintenance activities such as enhancement and porting. The concept of technical debt (TD) models the impact of such flaws as a burden carried by the software which must either be settled in a ‘lump sum’ to eradicate the flaw, or paid in the ongoing additional costs of maintaining the software with the flaw. In this paper we investigate the characteristics of technical debt in model transformations, analysing a range of MT cases in different MT languages, and using measures of quality flaws or ‘bad smells’ for MT, adapted from code measures.","n":0.093}}},{"i":520,"$":{"0":{"v":"Applying static code analysis for domain-specific languages","n":0.378},"1":{"v":"The use of code quality control platforms for analysing source code is increasingly gaining attention in the developer community. These platforms are prepared to parse and check source code written in a variety of general-purpose programming languages. The emergence of domain-specific languages enables professionals from different areas to develop and describe problem solutions in their disciplines. Thus, source code quality analysis methods and tools can also be applied to software artefacts developed with a domain-specific language. To evaluate the quality of domain-specific language code, every software component required by the quality platform to parse and query the source code must be developed. This becomes a time-consuming and error-prone task, for which this paper describes a model-driven interoperability strategy that bridges the gap between the grammar formats of source code quality parsers and domain-specific text languages. This approach has been tested on the most widespread platforms for designing text-based languages and source code analysis. This interoperability approach has been evaluated on a number of specific contexts in different domain areas.","n":0.077}}},{"i":521,"$":{"0":{"v":"Influence of programming style in transformation bad smells: mining of ETL repositories","n":0.289},"1":{"v":"Bad smells affect maintainability and performance of model-to-model transformations. There are studies that define a set of transformation bad smells, and some of them propose techniques to recogni...","n":0.189}}},{"i":522,"$":{"0":{"v":"Pricing Cloud Compute Commodities: A Novel Financial Economic Model","n":0.333},"1":{"v":"In this study, we design, develop, and simulate a cloud resources pricing model that satisfies two important constraints: the dynamic ability of the model to provide a high satisfaction guarantee measured as Quality of Service (QoS) - from users perspectives, profitability constraints - from the cloud service providers perspectives We employ financial option theory and treat the cloud resources as underlying assets to capture the realistic value of the cloud compute commodities (C3). We then price the cloud resources using our model. We discuss the results for four different metrics that we introduce to guarantee the quality of service and price as follows: (a) Moore's law based depreciation of asset values, (b) new technology based volatility measures in capturing price changes, (c) a new financial option pricing based model combining the above two concepts, and (d) the effect of age of resources and depreciation of cloud resource on QoS. We show that the cloud parameters can be mapped to financial economic model and we discuss the results of cloud compute commodity pricing for various parameters, such as the age of the resource, quality of service, and contract period.","n":0.073}}},{"i":523,"$":{"0":{"v":"Cloud adoption: a goal-oriented requirements engineering approach","n":0.378},"1":{"v":"We motivate the need for a new requirements engineering methodology for systematically helping businesses and users to adopt cloud services and for mitigating risks in such transition. The methodology is grounded in goal oriented approaches for requirements engineering. We argue that Goal Oriented Requirements Engineering (GORE) is a promising paradigm to adopt for goals that are generic and flexible statements of users' requirements, which could be refined, elaborated, negotiated, mitigated for risks and analysed for economics considerations. We describe the steps of the proposed process and exemplify the use of the methodology through an example. The methodology can be used by small to large scale organisations to inform crucial decisions related to cloud adoption.","n":0.094}}},{"i":524,"$":{"0":{"v":"An algorithm for suffix stripping","n":0.447},"1":{"v":"The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length.","n":0.097}}},{"i":525,"$":{"0":{"v":"The Delphi Technique: Making Sense of Consensus","n":0.378},"1":{"v":"The Delphi technique is a widely used and accepted method for gathering data from respondents within their domain of expertise. The technique is designed as a group communication process which aims to achieve a convergence of opinion on a specific real-world issue. The Delphi process has been used in various fields of study such as program planning, needs assessment, policy determination, and resource utilization to develop a full range of alternatives, explore or expose underlying assumptions, as well as correlate judgments on a topic spanning a wide range of disciplines. The Delphi technique is well suited as a method for consensus-building by using a series of questionnaires delivered using multiple iterations to collect data from a panel of selected subjects. Subject selection, time frames for conducting and completing a study, the possibility of low response rates, and unintentionally guiding feedback from the respondent group are areas which should be considered when designing and implementing a Delphi study.","n":0.08}}},{"i":526,"$":{"0":{"v":"Feature location in source code: a taxonomy and survey","n":0.333},"1":{"v":"SUMMARY\r\nFeature location is the activity of identifying an initial location in the source code that implements functionality in a software system. Many feature location techniques have been introduced that automate some or all of this process, and a comprehensive overview of this large body of work would be beneficial to researchers and practitioners. This paper presents a systematic literature survey of feature location techniques. Eighty-nine articles from 25 venues have been reviewed and classified within the taxonomy in order to organize and structure existing work in the field of feature location. The paper also discusses open issues and defines future directions in the field of feature location. Copyright © 2011 John Wiley & Sons, Ltd.","n":0.093}}},{"i":527,"$":{"0":{"v":"The Goal Structuring Notation – A Safety Argument Notation","n":0.333},"1":{"v":"In Europe, over recent years, the responsibility for ensuring system safety has shifted onto the developers and operators to construct and present well reasoned arguments that their systems achieve acceptable levels of safety. These arguments (together with supporting evidence) are typically referred to as a “safety case”. This paper describes the role and purpose of a safety case. Safety arguments within safety cases are often poorly communicated. This paper presents a technique called GSN (Goal Structuring Notation) that is increasingly being used in safety-critical industries to improve the structure, rigor, and clarity of safety arguments. The paper also describes a number of extensions, based upon GSN, which can be used to assist the maintenance, construction, reuse and assessment of safety cases. The aim of this paper is to describe the current industrial use and research into GSN such that its applicability to other types of Assurance Case, in addition to safety cases, can also be considered.","n":0.08}}},{"i":528,"$":{"0":{"v":"Semantic clustering: Identifying topics in source code","n":0.378},"1":{"v":"Many of the existing approaches in Software Comprehension focus on program structure or external documentation. However, by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked. To understand software as a whole, we need to enrich software analysis with the developer knowledge hidden in the code naming. This paper proposes the use of information retrieval to exploit linguistic information found in source code, such as identifier names and comments. We introduce Semantic Clustering, a technique based on Latent Semantic Indexing and clustering to group source artifacts that use similar vocabulary. We call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code. We compare the topics to each other, identify links between them, provide automatically retrieved labels, and use a visualization to illustrate how they are distributed over the system. Our approach is language independent as it works at the level of identifier names. To validate our approach we applied it on several case studies, two of which we present in this paper. Note: Some of the visualizations presented make heavy use of colors. Please obtain a color copy of the article for better understanding.","n":0.071}}},{"i":529,"$":{"0":{"v":"Combining Formal Concept Analysis with Information Retrieval for Concept Location in Source Code","n":0.277},"1":{"v":"The paper addresses the problem of concept location in source code by presenting an approach which combines formal concept analysis (FCA) and latent semantic indexing (LSI). In the proposed approach, LSI is used to map the concepts expressed in queries written by the programmer to relevant parts of the source code, presented as a ranked list of search results. Given the ranked list of source code elements, our approach selects most relevant attributes from these documents and organizes the results in a concept lattice, generated via FCA. The approach is evaluated in a case study on concept location in the source code of eclipse, an industrial size integrated development environment. The results of the case study show that the proposed approach is effective in organizing different concepts and their relationships present in the subset of the search results. The proposed concept location method outperforms the simple ranking of the search results, reducing the programmers' effort.","n":0.08}}},{"i":530,"$":{"0":{"v":"An evaluation of argument patterns to reduce pitfalls of applying assurance case","n":0.289},"1":{"v":"In this paper, we will introduce some of the problem areas that Japanese software engineers are susceptible to during the creation of assurance cases, based on the author's educational experience with assurance cases. In addition, methods for dealing with these problem areas will also be explained. To mitigate these problems, it is expected to apply assurance case patterns that help engineers develop assurance cases by reusing those patterns. It is also shown an evaluation result of assurance case pattern application to develop an assurance case for a LAN device management system.","n":0.105}}},{"i":531,"$":{"0":{"v":"Towards Measurement of Confidence in Safety Cases","n":0.378},"1":{"v":"Safety cases capture a structured argument linking claims about the safety of a system to the evidence justifying those claims. However, arguments in safety cases tend to be predominantly qualitative. Partly, this is attributed to the lack of sufficient design and operational data necessary to measure the achievement of high-dependability goals, particularly for safety-critical functions implemented in software. The subjective nature of many forms of evidence, such as expert judgment and process maturity, also contributes to the overwhelming dependence on qualitative arguments. However, where data for quantitative measurements can be systematically collected, quantitative arguments provide benefits over qualitative arguments in assessing confidence in the safety case. In this paper, we propose a basis for developing and evaluating the confidence in integrated qualitative and quantitative safety arguments. We specify a safety argument using the Goal Structuring Notation (GSN), identify and quantify uncertainties therein, and use Bayesian Networks (BNs) as a means to reason about confidence in a probabilistic way. We illustrate our approach using a fragment of a safety case for an unmanned aircraft system (UAS).","n":0.076}}},{"i":532,"$":{"0":{"v":"The Use of Multilegged Arguments to Increase Confidence in Safety Claims for Software-Based Systems: A Study Based on a BBN Analysis of an Idealized Example","n":0.2},"1":{"v":"The work described here concerns the use of so-called multilegged arguments to support dependability claims about software-based systems. The informal justification for the use of multilegged arguments is similar to that used to support the use of multiversion software in pursuit of high reliability or safety. Just as a diverse 1-out-of-2 system might be expected to be more reliable than each of its two component versions, so might a two-legged argument be expected to give greater confidence in the correctness of a dependability claim (for example, a safety claim) than would either of the argument legs alone. Our intention here is to treat these argument structures formally, in particular, by presenting a formal probabilistic treatment of \"confidence,\" which will be used as a measure of efficacy. This will enable claims for the efficacy of the multilegged approach to be made quantitatively, answering questions such as, \"How much extra confidence about a system's safety will I have if I add a verification argument leg to an argument leg based upon statistical testing?\" For this initial study, we concentrate on a simplified and idealized example of a safety system in which interest centers upon a claim about the probability of failure on demand. Our approach is to build a \"Bayesian belief network\" (BBN) model of a two-legged argument and manipulate this analytically via parameters that define its node probability tables. The aim here is to obtain greater insight than what is afforded by the more usual BBN treatment, which involves merely numerical manipulation. We show that the addition of a diverse second argument leg can indeed increase confidence in a dependability claim; in a reasonably plausible example, the doubt in the claim is reduced to one-third of the doubt present in the original single leg. However, we also show that there can be some unexpected and counterintuitive subtleties here; for example, an entirely supportive second leg can sometimes undermine an original argument, resulting, overall, in less confidence than what came from this original argument. Our results are neutral on the issue of whether such difficulties will arise in real life $that is, when real experts judge real systems.","n":0.053}}},{"i":533,"$":{"0":{"v":"Using Data Fusion and Web Mining to Support Feature Location in Software","n":0.289},"1":{"v":"Data fusion is the process of integrating multiple sources of information such that their combination yields better results than if the data sources are used individually. This paper applies the idea of data fusion to feature location, the process of identifying the source code that implements specific functionality in software. A data fusion model for feature location is presented which defines new feature location techniques based on combining information from textual, dynamic, and web mining analyses applied to software. A novel contribution of the proposed model is the use of advanced web mining algorithms to analyze execution information during feature location. The results of an extensive evaluation indicate that the new feature location techniques based on web mining improve the effectiveness of existing approaches by as much as 62%.","n":0.088}}},{"i":534,"$":{"0":{"v":"Feature Identification: An Epidemiological Metaphor","n":0.447},"1":{"v":"Feature identification is a technique to identify the source code constructs activated when exercising one of the features of a program. We propose new statistical analyses of static and dynamic data to accurately identify features in large multithreaded object-oriented programs. We draw inspiration from epidemiology to improve previous approaches to feature identification and develop an epidemiological metaphor. We build our metaphor on our previous approach to feature identification, in which we use processor emulation, knowledge-based filtering, probabilistic ranking, and metamodeling. We carry out three case studies to assess the usefulness of our metaphor, using the \"save a bookmark\" feature of Web browsers as an illustration. In the first case study, we compare our approach with three previous approaches (a naive approach, a concept analysis-based approach, and our previous probabilistic approach) in identifying the feature in MOZILLA, a large, real-life, multithreaded object-oriented program. In the second case study, we compare the implementation of the feature in the FIREFOX and MOZILLA Web browsers. In the third case study, we identify the same feature in two more Web browsers, Chimera (in C) and ICEBrowser (in Java), and another feature in JHOTDRAW and XFIG, to highlight the generalizability of our metaphor","n":0.071}}},{"i":535,"$":{"0":{"v":"Configuring latent Dirichlet allocation based feature location","n":0.378},"1":{"v":"Feature location is a program comprehension activity, the goal of which is to identify source code entities that implement a functionality. Recent feature location techniques apply text retrieval models such as latent Dirichlet allocation (LDA) to corpora built from text embedded in source code. These techniques are highly configurable, and the literature offers little insight into how different configurations affect their performance. In this paper we present a study of an LDA based feature location technique (FLT) in which we measure the performance effects of using different configurations to index corpora and to retrieve 618 features from 6 open source Java systems. In particular, we measure the effects of the query, the text extractor configuration, and the LDA parameter values on the accuracy of the LDA based FLT. Our key findings are that exclusion of comments and literals from the corpus lowers accuracy and that heuristics for selecting LDA parameter values in the natural language context are suboptimal in the source code context. Based on the results of our case study, we offer specific recommendations for configuring the LDA based FLT.","n":0.074}}},{"i":536,"$":{"0":{"v":"Toward a Theory of Assurance Case Confidence","n":0.378},"1":{"v":"Abstract : Assurance cases provide an argument and evidence explaining why a claim about some system property holds. This report outlines a framework for justifying confidence in the truth of such an assurance case claim. The framework is based on the notion of eliminative induction - the principle (first put forward by Francis Bacon) that confidence in the truth of a hypothesis (or claim) increases as reasons for doubting its truth are identified and eliminated. Possible reasons for doubting the truth of a claim (defeaters) arise from analyzing an assurance case using defeasible reasoning concepts. Finally, the notion of Baconian probability provides a measure of confidence based on how many defeaters have been identified and eliminated.","n":0.093}}},{"i":537,"$":{"0":{"v":"Software Measurement: Establish - Extract - Evaluate - Execute","n":0.333},"1":{"v":"In this comprehensive introduction to software measurement, Ebert and Dumke detail knowledge and experiences about the subject in an easily understood, hands-on presentation. The book describes software measurement in theory and practice as well as provides guidance to all relevant measurement tools and online references. In addition, it presents hands-on experience from industry leaders and provides many examples and case studies from Global 100 companies. Besides the many practical hints and checklists, readers will also appreciate the large reference list, which includes links to metrics communities where project experiences are shared.","n":0.105}}},{"i":538,"$":{"0":{"v":"Measuring assurance case confidence using Baconian probabilities","n":0.378},"1":{"v":"The basis for assessing the validity of an assurance case is an active area of study. In this paper, we discuss how to assess confidence in a case by considering the doubts eliminated by the claims and evidence in a case. This is an application of eliminative induction and the notion of Baconian probability as put forward by L. Jonathan Cohen.","n":0.128}}},{"i":539,"$":{"0":{"v":"An empirical analysis of information retrieval based concept location techniques in software comprehension","n":0.277},"1":{"v":"Concept location, the problem of associating human oriented concepts with their counterpart solution domain concepts, is a fundamental problem that lies at the heart of software comprehension. Recent research has attempted to alleviate the impact of the concept location problem through the application of methods drawn from the information retrieval (IR) community. Here we present a new approach based on a complimentary IR method which also has a sound basis in cognitive theory. We compare our approach to related work through an experiment and present our conclusions. This research adapts and expands upon existing language modelling frameworks in IR for use in concept location, in software systems. In doing so it is novel in that it leverages implicit information available in system documentation. Surprisingly, empirical evaluation of this approach showed little performance benefit overall and several possible explanations are forwarded for this finding.","n":0.084}}},{"i":540,"$":{"0":{"v":"Representation of Confidence in Assurance Cases Using the Beta Distribution","n":0.316},"1":{"v":"Assurance cases are used to document an argument that a system -- such as a critical software system -- satisfies some desirable property (e.g., safety, security, or reliability). Demonstrating high confidence that the claims made based on an assurance case can be trusted is crucial to the success of the case. Researchers have proposed quantification of confidence as a Baconian probability ratio of eliminated concerns about the assurance case to the total number of identified concerns. In this paper, we extend their work by mapping this discrete ratio to a continuous probability distribution -- a beta distribution -- enabling different visualizations of the confidence in a claim. Further, the beta distribution allows us to quantify and visualize theuncertainty associated with the expressed confidence. Additionally, by transforming the assurance case into a reasoning structure, we show how confidence calculations can be performed using beta distributions.","n":0.083}}},{"i":541,"$":{"0":{"v":"Security Assurance Cases for Medical Cyber–Physical Systems","n":0.378},"1":{"v":"A series of public demonstrations of critical security vulnerabilities in medical cyber–physical systems (CPSs) such as infusion pumps and pacemakers has shown the susceptibility of medical devices to attacks by malignant agents. Medical-device manufacturers and regulators thus have an interest in establishing not only that a medical device is safe and effective (the traditional criteria for device acceptability) but that it is also secure. In this paper, the authors advocate the use of an argumentation framework for security based on assurance cases and define a candidate structure for security assurance cases along with a methodology for assurance-case-driven security engineering for medical CPSs. They also develop an example illustrating the approach. This approach, the authors believe, should replace the ad hoc analysis and argumentation approaches used today by both device developers and regulators.","n":0.087}}},{"i":542,"$":{"0":{"v":"On the interest of architectural technical debt: Uncovering the contagious debt phenomenon","n":0.289},"1":{"v":"A known problem in large software companies is to balance the prioritization of short-term and long-term business goals. As an example, architecture suboptimality (Architectural Technical Debt), incurred to deliver fast, might hinder future feature development. However, some technical debt generates more interest to be paid than other. We conducted a multi-phase, multiple-case embedded case study comprehending 9 sites at 6 large international software companies. We have investigated which architectural technical debt items generate more interest , how the interest occurs during software development and which costly extra-activities are triggered as a result. We presented a taxonomy of the most dangerous items identified during the qualitative investigation and a model of their effects that can be used for prioritization, for further investigation and as a quality model for extracting more precise and context-specific metrics. We found that some architectural technical debt items are contagious, causing the interest to be not only fixed, but potentially compound, which leads to the hidden growth of interest (possibly exponential). We found important factors to be monitored to refactor the debt before it becomes too costly. Instances of these phenomena need to be identified and stopped before the development reaches a crises.","n":0.071}}},{"i":543,"$":{"0":{"v":"A taxonomy of software types to facilitate search and evidence-based software engineering","n":0.289},"1":{"v":"Empirical software research could be improved if there was a systematic way to identify the types of software for which empirical evidence applies. This is because results are unlikely to be globally applicable, but are more likely to apply only in certain contexts such as the type of software on which the evidence has been tested. We present a software taxonomy that should help researchers to apply their research systematically to particular types of software. The taxonomy was generated using existing partial taxonomies and input from survey participants. If a taxonomy such as ours gains acceptance, it will facilitate comparison and appropriate application of research. In the paper, we present the benefits of such a taxonomy, the process we used to develop it, and the taxonomy itself.","n":0.089}}},{"i":544,"$":{"0":{"v":"An investigation of proposed techniques for quantifying confidence in assurance arguments","n":0.302},"1":{"v":"Abstract   The use of safety cases in certification raises the question of assurance argument sufficiency and the issue of confidence (or uncertainty) in the argument’s claims. Some researchers propose to model confidence quantitatively and to calculate confidence in argument conclusions. We know of little evidence to suggest that any proposed technique would deliver trustworthy results when implemented by system safety practitioners. Proponents do not usually assess the efficacy of their techniques through controlled experiment or historical study. Instead, they present an illustrative example where the calculation delivers a plausible result. In this paper, we review current proposals, claims made about them, and evidence advanced in favor of them. We then show that proposed techniques can deliver implausible results in some cases. We conclude that quantitative confidence techniques require further validation before they should be recommended as part of the basis for deciding whether an assurance argument justifies fielding a critical system.","n":0.081}}},{"i":545,"$":{"0":{"v":"Assessing the Overall Sufficiency of Safety Arguments","n":0.378},"1":{"v":"Safety cases offer a means for communicating information about the system safety among the system stakeholders. Recently, the requirement for a safety case has been considered by regulators for safety-critical systems. Adopting safety cases is necessarily dependent on the value added for regulatory authorities. In this work, we outline a structured approach for assessing the level of sufficiency of safety arguments. We use the notion of basic probability assignment to provide a measure of sufficiency and insufficiency for each argument node. We use the concept of belief combination to calculate the overall sufficiency and insufficiency of a safety argument based on the sufficiency and insufficiency of its nodes. The application of the proposed approach is illustrated by examples.","n":0.092}}},{"i":546,"$":{"0":{"v":"The essential synthesis of problem frames and assurance cases","n":0.333},"1":{"v":": Problem frames and assurance cases are two current research areas that can improve – and have improved – system dependability, in critical and noncritical systems alike. While these two techniques are effective separately, their synthesis is much more powerful. This paper describes the synthesis of these two techniques and the rationale behind the synthesis, the particular pieces that influence each other, and the beginning of a process to integrate the two in software system development. A detailed example of the application of the synthesis is also provided.","n":0.107}}},{"i":547,"$":{"0":{"v":"Reasoning About Confidence and Uncertainty in Assurance Cases: A Survey","n":0.316},"1":{"v":"Assurance cases are structured logical arguments supported by evidence that explain how systems, possibly software systems, satisfy desirable properties for safety, security or reliability. The confidence in both the logical reasoning and the underlying evidence is a factor that must be considered carefully when evaluating an assurance case; the developers must have confidence in their case before the system is delivered and the assurance case reviewer, such as a regulatory body, must have adequate confidence in the case before approving the system for use. A necessary aspect of gaining confidence in the assurance case is dealing with uncertainty, which may have several sources. Uncertainty, often impossible to eliminate, nevertheless undermines confidence and must therefore be sufficiently bounded. It can be broadly classified into two types, aleatory (statistical) and epistemic (systematic). This paper surveys how researchers have reasoned about uncertainty in assurance cases. We analyze existing literature to identify the type of uncertainty addressed and distinguish between qualitative and quantitative approaches for dealing with uncertainty.","n":0.078}}},{"i":548,"$":{"0":{"v":"Taking a page from the law books: Considering evidence weight in evaluating assurance case confidence","n":0.258},"1":{"v":"This brief report is a contribution to discussions of the notion of confidence in the context of assurance cases. In this work, we draw a parallel between the concepts of assurance case confidence and evidence weight in the legal domain, and explore the practical ramifications of this idea. We first establish what factors influence assurance case confidence, and propose a definition. Then, through a comparison with the legal domain (following the discussions of Jonathan Cohen, Keynes and Nance) we submit that confidence can be seen as composed of two distinct aspects, and we proceed to contend that it is beneficial to consider these aspects separately when performing an evaluation. One of the greatest advantages of doing so would be providing a separate measure for assurance case “ripeness” for review (to be used by assurance case developers, as well as regulators).","n":0.085}}},{"i":549,"$":{"0":{"v":"From Requirements to Features: An Exploratory Study of Feature-Oriented Refactoring","n":0.316},"1":{"v":"More and more frequently successful software systems need to evolve into families of systems, known as Software Product Lines (SPLs), to be able to cater to the different functionality requirements demanded by different customers while at the same time aiming to exploit as much common functionality as possible. As a first step, this evolution demands a clear understanding of how the functional requirements map into the features of the original system. Using this knowledge, features can be refactored so that they are reused for building the new systems of the evolved SPL. In this paper we present our experience in refactoring features based on the requirements specifications of a small and a medium size systems. Our work identified eight refactoring patterns that describe how to extract the elements of features which were subsequently implemented using Feature Oriented Software Development (FOSD) a novel modularization paradigm whose driving goal is to effectively modularize features for the development of variable systems. We argue that the identification of refactoring patterns are a stepping stone towards automating Feature-Oriented Refactoring, and present some open issues that should be addressed to that avail.","n":0.073}}},{"i":550,"$":{"0":{"v":"Combining lexical and structural information to reconstruct software layers","n":0.333},"1":{"v":"ContextThe architectures of existing software systems generally lack documentation or have often drifted from their initial design due to repetitive maintenance operations. To evolve such systems, it is mandatory to reconstruct and document their architectures. Many approaches were proposed to support the architecture recovery process but few of these consider the architectural style of the system under analysis. Moreover, most of existing approaches rely on structural dependencies between entities of the system and do not exploit the semantic information hidden in the source code of these entities. ObjectiveWe propose an approach that exploits both linguistic and structural information to recover the software architecture of Object Oriented (OO) systems. The focus of this paper is the recovery of architectures that comply with the layered style, which is widely used in software systems. MethodIn this work, we (i) recover the responsibilities of the system under study and (ii) assign these responsibilities to different abstraction layers. To do so, we use the linguistic information extracted from the source code to recover clusters corresponding to the responsibilities of the system. Then we assign these clusters to layers using the system's structural information and the layered style constraints. We formulate the recovery of the responsibilities and their assignment to layers as optimization problems that we solve using search-based algorithms. ResultsTo assess the effectiveness of our approach we conducted experiments on four open source systems. The so-obtained layering results yielded higher precision and recall than those generated using a structural-based layering approach. ConclusionOur hybrid lexical-structural approach is effective and shows potential for significant improvement over techniques based only on structural information.","n":0.061}}},{"i":551,"$":{"0":{"v":"Design and implementation of distributed expert systems","n":0.378},"1":{"v":"There is a rich body of work dedicated to expert systems. However, none of them focus on control strategies suitable for distributed environments. We describe a novel approach to design distributed expert systems that is able to control rule activation; it involves control strategies supporting selective inferencing with rules that propagate concurrently and incorporates a set of meta-rules that operate on a blackboard and that are expressed as an assurance case. It is implemented on top of ERESYE, a tool for the realization of expert systems that is written in Erlang. We describe an implementation of the approach in an industrial setting, using an example focusing on the feature identification problem, whose resolution is an important task for source code maintenance and evolution.","n":0.09}}},{"i":552,"$":{"0":{"v":"Automated feature discovery via sentence selection and source code summarization","n":0.316},"1":{"v":"Programs are, in essence, a collection of implemented features. Feature discovery in software engineering is the task of identifying key functionalities that a program implements. Manual feature discovery can be time consuming and expensive, leading to automatic feature discovery tools being developed. However, these approaches typically only describe features using lists of keywords, which can be difficult for readers who are not already familiar with the source code. An alternative to keyword lists is sentence selection, in which one sentence is chosen from among the sentences in a text document to describe that document. Sentence selection has been widely studied in the context of natural language summarization but is only beginning to be explored as a solution to feature discovery. In this paper, we compare four sentence selection strategies for the purpose of feature discovery. Two are off-the-shelf approaches, while two are adaptations we propose. We present our findings as guidelines and recommendations to designers of feature discovery tools. Copyright © 2016 John Wiley & Sons, Ltd.","n":0.077}}},{"i":553,"$":{"0":{"v":"System Assurance: Beyond Detecting Vulnerabilities","n":0.447},"1":{"v":"In this day of frequent acquisitions and perpetual application integrations, systems are often an amalgamation of multiple programming languages and runtime platforms using new and legacy content. Systems of such mixed origins are increasingly vulnerable to defects and subversion. \r\n\r\nSystem Assurance: Beyond Detecting Vulnerabilities addresses these critical issues. As a practical resource for security analysts and engineers tasked with system assurance, the book teaches you how to use the Object Management Group?s (OMG) expertise and unique standards to obtain accurate knowledge about your existing software and compose objective metrics for system assurance. OMG?s Assurance Ecosystem provides a common framework for discovering, integrating, analyzing, and distributing facts about your existing enterprise software. Its foundation is the standard protocol for exchanging system facts, defined as the OMG Knowledge Discovery Metamodel (KDM). In addition, the Semantics of Business Vocabularies and Business Rules (SBVR) defines a standard protocol for exchanging security policy rules and assurance patterns. Using these standards together, you will learn how to leverage the knowledge of the cybersecurity community and bring automation to protect your system.\r\n\r\n\r\n\r\nProvides end-to-end methodology for systematic, repeatable, and affordable System Assurance.\r\nIncludes an overview of OMG Software Assurance Ecosystem protocols that integrate risk, architecture and code analysis guided by the assurance argument.\r\nCase Study illustrating the steps of the System Assurance Methodology using automated tools.\r\n\r\nTable of Contents\r\n\r\n\r\nContents\r\n\r\n1. Why Hackers know more about our systems\r\n\r\n1.1 Operating in cyberspace involves risks\r\n\r\n1.2 Why Hackers are repeatadly successful\r\n\r\n1.2.1 What are the challenges in defending cybersystems?\r\n\r\n1.2.1.1 Difficulties in understanding and assessing risks\r\n\r\n1.2.1.2 Understanding Development Trends\r\n\r\n1.2.1.3 Comprehending Systems? Complexity\r\n\r\n1.2.1.4 Understanding Assessment Practices and their Limitations\r\n\r\n1.2.1.5 Vulnerability Scanning Technologies and their Issues\r\n\r\n1.3 Where do We Go from Here\r\n\r\n1.3.1 Systematic and repeatable defense at affordable cost\r\n\r\n1.3.2 The OMG Software Assurance Ecosystem\r\n\r\n1.3.3 Linguistic Modeling to manage the common vocabulary\r\n\r\n1.4 Who should read this book\r\n\r\n2 Chapter: Confidence as a Product\r\n\r\n2.1 Are you confident that there is no black cat in the dark room?\r\n\r\n2.2 The Nature of Assurance\r\n\r\n2.2.1 Engineering, Risk and Assurance\r\n\r\n2.2.2 Assurance Case (AC)\r\n\r\n2.2.2.1 Contents of an Assurance Case\r\n\r\n2.2.2.2 Structure of the Assurance Argument\r\n\r\n2.3 Overview of the Assurance Process\r\n\r\n2.3.1 Producing Confidence\r\n\r\n2.3.1.1 Economics of Confidence\r\n\r\n3 Chapter: How to Build Confidence \r\n3.1 Assurance in the System Lifecycle\r\n\r\n3.2 Activities of System Assurance Process\r\n\r\n3.2.1 Project Definition\r\n\r\n3.2.2 Project Preparation\r\n\r\n3.2.3 Assurance argument development\r\n\r\n3.2.4 Architecture Security Analysis\r\n\r\n3.2.4.1 Discover System Facts\r\n\r\n3.2.4.2 Threat identification\r\n\r\n3.2.4.3 Safeguard Identification\r\n\r\n3.2.4.4 Vulnerability detection\r\n\r\n3.2.4.5 Security Posture Analysis\r\n\r\n3.2.5 Evidence analysis\r\n\r\n3.2.6 Assurance Case Delivery\r\n\r\n4 Chapter: Knowledge of System as of Element in Cybersecurity argument \r\n\r\n4.1 What is system\r\n\r\n4.2 Boundaries of the system\r\n\r\n4.3 Resolution of the system description\r\n\r\n4.4 Conceptual commitment for system descriptions\r\n\r\n4.5 System architecture\r\n\r\n4.6 Example of an architecture framework\r\n\r\n4.7 Elements of System\r\n\r\n4.8 System Knowledge Involves Multiple Viewpoints\r\n\r\n4.9 Concept of operations (CONOP)\r\n\r\n4.10 Network Configuration\r\n\r\n4.11 System life cycle and assurance\r\n\r\n4.11.1 System life cycle stages\r\n\r\n4.11.2 Enabling Systems\r\n\r\n4.11.3 Supply Chain\r\n\r\n4.11.4 System life cycle processes\r\n\r\n4.11.5 The implications to the common vocabulary and the integrated system model\r\n\r\n5 Chapter: Knowledge of Risk as an Element of Cybersecurity argument \r\n\r\n5.1 Introduction\r\n\r\n5.2 Basic cybersecurity elements\r\n\r\n5.3 Common vocabulary for risk analysis\r\n\r\n5.3.1 Defining diScernable vocabulary for Assets\r\n\r\n5.3.2 Threats and hazards\r\n\r\n5.3.3 Defining dicernable vocabulary for Injury and Impact\r\n\r\n5.3.4 Defining dicernable vocabulary for threats\r\n\r\n5.3.5 Threat scenarios and attacks\r\n\r\n5.3.6 Defining dicernable vocabulary for vulnerabilities\r\n\r\n5.3.7 Defining dicernable vocabulary for safeguards\r\n\r\n5.3.8 Risk\r\n\r\n5.4 Systematic Threat Identification\r\n\r\n5.5 Assurance Strategies\r\n\r\n5.5.1 Injury Argument\r\n\r\n5.5.2 Entry point argument\r\n\r\n5.5.3 Threat argument\r\n\r\n5.5.4 Vulnerability argument\r\n\r\n5.5.5 Security requirement argument\r\n\r\n5.5.6 Assurance of the threat identification\r\n\r\n6 Chapter: Knowledge of Vulnerabilities as an Element of Cybersecurity Argument \r\n6.1 Vulnerability as part of system knowledege\r\n\r\n6.1.1 What is Vulnerability\r\n\r\n6.1.2 Vulnerability as Unit of Knowledge: The History of Vulnerability\r\n\r\n6.1.3 Vulnerabilities and the Phases of the System Life Cycle\r\n\r\n6.1.4 Enumeration of Vulnerabilities as a Knowledge Product\r\n\r\n6.1.5 Vulnerability Databases\r\n\r\n6.1.5.1 US-CERT\r\n\r\n6.1.5.2 Open Source Vulnerability Database (OSVDB)\r\n\r\n6.1.6 Vulnerability Life Cycle\r\n\r\n6.2 NIST Security Content Automation Protocol (SCAP) Ecosystem\r\n\r\n6.2.1 Overview of SCAP Ecosystem\r\n\r\n6.2.2 Information Exchanges under SCAP\r\n\r\n7 Chapter: Vulnerability Patterns as a New Assurance Content \r\n\r\n7.1 Beyond Current SCAP Ecosystem\r\n\r\n7.2 Vulnerability Patterns\r\n\r\n7.3 Software Fault Patterns\r\n\r\n7.3.1 Safeguard category of clusters and corresponding Software fault Patterns (SFPs)\r\n\r\n7.3.1.1 Authentication\r\n\r\n7.3.1.2 Access Control\r\n\r\n7.3.1.3 Privilege\r\n\r\n7.3.2 Direct Impact category of clusters and corresponding Software fault Patterns (SFPs)\r\n\r\n7.3.2.1 Information Leak\r\n\r\n7.3.2.2 Memory Management\r\n\r\n7.3.2.3 Memory Access\r\n\r\n7.3.2.4 Path Resolution\r\n\r\n7.3.2.5 Tainted Input\r\n\r\n8 Chapter: OMG Software Assurance Ecosystem \r\n\r\n8.1 Introduction\r\n\r\n8.2 OMG Assurance Ecosystem: towards collaborative cybersecurity\r\n\r\n9 Chapter: Common Fact Model for Assurance Content\r\n\r\n9.1 Assurance Content\r\n\r\n9.2 The Objectives\r\n\r\n9.3 Design criteria for information exchange protocols\r\n\r\n9.4 Tradeoffs\r\n\r\n9.5 Information Exchange Protocols\r\n\r\n9.6 The Nuts and Bolts of Fact Models\r\n\r\n9.6.1 Objects\r\n\r\n9.6.2 Noun Concepts\r\n\r\n9.6.3 Facts about existence of objects\r\n\r\n9.6.4 Individual concepts\r\n\r\n9.6.5 Relations between concepts\r\n\r\n9.6.6 Verb concepts\r\n\r\n9.6.7 Characteristics\r\n\r\n9.6.8 Situational concepts\r\n\r\n9.6.9 Viewpoints and views\r\n\r\n9.6.10 Information exchanges and assurance\r\n\r\n9.6.11 Fact-oriented Integration\r\n\r\n9.6.12 Automatic derivation of facts\r\n\r\n9.7 The representation of facts\r\n\r\n9.7.1 Representing facts in XML\r\n\r\n9.7.2 Representing facts and schemes in Prolog\r\n\r\n9.8 The common schema\r\n\r\n9.9 System assurance facts\r\n\r\n 10 Chapter: Linguistic Models\r\n\r\n10.1 Fact Models and Linguistic Models\r\n\r\n10.2 Background\r\n\r\n10.3 Overview of SBVR\r\n\r\n10.4 How to use SBVR\r\n\r\n10.4.1 Simple vocabulary\r\n\r\n10.4.2 Vocabulary Entries\r\n\r\n10.4.3 Statements\r\n\r\n10.4.4 Statements as formal definitions of new concepts\r\n\r\n10.4.4.1 Definition of a Noun Concept\r\n\r\n10.4.4.2 Definition of a Verb Concept\r\n\r\n10.4.4.3 The General Concept caption\r\n\r\n10.5 SBVR Vocabulary for describing Elementary Meanings\r\n\r\n10.6 SBVR Vocabulary for describing Representations\r\n\r\n10.7 SBVR Vocabulary for describing Extensions\r\n\r\n10.8 Reference schemes\r\n\r\n10.9 SBVR Semantic Formulations\r\n\r\n10.9.1 Defining new terms and facts types using SBVR\r\n\r\n11 Chapter: Standard Protocol for Exchanging System Facts \r\n11.1 Background\r\n\r\n11.2 Organization of the KDM vocabulary\r\n\r\n11.2.1 Infrastructure Layer\r\n\r\n11.2.2 Program Elements Layer\r\n\r\n11.2.3 Resource Layer\r\n\r\n11.2.4 Abstractions Layer\r\n\r\n11.3 The process of discovering system facts\r\n\r\n11.4 Discovering the baseline system facts\r\n\r\n11.4.1 Inventory views\r\n\r\n11.4.1.1 Inventory Viewpoint vocabulary in SBVR\r\n\r\n11.4.2 Build Views\r\n\r\n11.4.3 Data views\r\n\r\n11.4.4 UI views\r\n\r\n11.4.5 Code views\r\n\r\n11.4.5.1 Code views: Elements of Structure\r\n\r\n11.4.5.2 Code views: Elements of Behavior\r\n\r\n11.4.5.3 Micro KDM\r\n\r\n11.4.6 Platform views\r\n\r\n11.4.7 Event views\r\n\r\n11.5 Performing architecture analysis\r\n\r\n11.5.1 Structure Views\r\n\r\n11.5.2 Conceptual Views\r\n\r\n11.5.2.1 Linguistic Viewpoint\r\n\r\n11.5.2.2 Behavior Viewpoint\r\n\r\n12 Chapter: Case Study \r\n\r\n12.1 Introduction\r\n\r\n12.2 Background\r\n\r\n12.3 Concepts of operations\r\n\r\n12.3.1 Executive summary\r\n\r\n12.3.2 Purpose\r\n\r\n12.3.3 Locations\r\n\r\n12.3.4 Operational Authority\r\n\r\n12.3.5 System Architecture\r\n\r\n12.3.5.1 Clicks2Bricks Web server\r\n\r\n12.3.5.2 Database server\r\n\r\n12.3.5.3 SMTP server\r\n\r\n12.3.6 System Assumptions\r\n\r\n12.3.7 External dependencies\r\n\r\n12.3.8 Implementation Assumptions\r\n\r\n12.3.9 Interfaces with Other Systems\r\n\r\n12.3.10 Security assumptions\r\n\r\n12.3.11 External Security Notes\r\n\r\n12.3.12 Internal Security notes\r\n\r\n12.4 Business vocabulary and security policy for Clicks2Bricks in SBVR\r\n\r\n12.5 Building the integrated system model\r\n\r\n12.5.1 Building the baseline system model\r\n\r\n12.5.2 Enhancing the baseline model with the system architecture facts\r\n\r\n12.6 Mapping cybersecurity facts to system facts\r\n\r\n12.7 Assurance case","n":0.032}}},{"i":554,"$":{"0":{"v":"Leveraging artifact trees to evolve and reuse safety cases","n":0.333},"1":{"v":"Safety Assurance Cases (SACs) are increasingly used to guide and evaluate the safety of software-intensive systems. They are used to construct a hierarchically organized set of claims, arguments, and evidence in order to provide a structured argument that a system is safe for use. However, as the system evolves and grows in size, a SAC can be difficult to maintain. In this paper we utilize design science to develop a novel solution for identifying areas of a SAC that are affected by changes to the system. Moreover, we generate actionable recommendations for updating the SAC, including its underlying artifacts and trace links, in order to evolve an existing safety case for use in a new version of the system. Our approach, Safety Artifact Forest Analysis (SAFA), leverages traceability to automatically compare software artifacts from a previously approved or certified version with a new version of the system. We identify, visualize, and explain changes in a Delta Tree. We evaluate our approach using the Dronology system for monitoring and coordinating the actions of cooperating, small Unmanned Aerial Vehicles. Results from a user study show that SAFA helped users to identify changes that potentially impacted system safety and provided information that could be used to help maintain and evolve a SAC1.","n":0.069}}},{"i":555,"$":{"0":{"v":"Combining Bayesian belief networks and the goal structuring notation to support architectural reasoning about safety","n":0.258},"1":{"v":"There have been an increasing number of applications of Bayesian Belief Network (BBN) for predicting safety properties in an attempt to handle the obstacles of uncertainty and complexity present in modern software development. Yet there is little practical guidance on justifying the use of BBN models for the purpose of safety. In this paper, we propose a compositional and semi-automated approach to reasoning about safety properties of architectures. This approach consists of compositional failure analysis through applying the object-oriented BBN framework. We also show that producing sound safety arguments for BBN-based deviation analysis results can help understand the implications of analysis results and identify new safety problems. The feasibility of the proposed approach is demonstrated by means of a case study.","n":0.091}}},{"i":556,"$":{"0":{"v":"An analysis of safety evidence management with the Structured Assurance Case Metamodel","n":0.289},"1":{"v":"SACM (Structured Assurance Case Metamodel) is a standard for assurance case specification and exchange. It consists of an argumentation metamodel and an evidence metamodel for justifying that a system satisfies certain requirements. For assurance of safety-critical systems, SACM can be used to manage safety evidence and to specify safety cases. The standard is a promising initiative towards harmonizing and improving system assurance practices, but its suitability for safety evidence management needs to be further studied. To this end, this paper studies how SACM 1.1 supports this activity according to requirements from industry and from prior work. We have analysed the notion of evidence in SACM, its evidence lifecycle, the classes and associations of the evidence metamodel, and the link of this metamodel with the argumentation one. As a result, we have identified several improvement opportunities and extension possibilities in SACM. The notions of evidence and evidence assertion should be clarified, the overlaps between metamodel elements should be reduced, and a wider support to the lifecycle of the artefacts used as safety evidence could be provided. Addressing these aspects will allow SACM to better fit safety evidence management needs and practices, especially beyond the scope of a safety case. The results and the conclusions drawn are especially valuable for practitioners interested in SACM adoption and vendors interested in developing tool support for SACM-based safety evidence management. We present an analysis of how SACM supports safety evidence management.The analysis is based on requirements from industry and from prior work.We have identified nine improvement areas and four extension possibilities.Addressing these aspects will allow SACM to better fit safety evidence management.","n":0.061}}},{"i":557,"$":{"0":{"v":"Automated Feature Identification in Web Applications","n":0.408},"1":{"v":"Market-driven software intensive product development companies have been more and more experiencing the problem of feature expansion over time. Product managers face the challenge of identifying and locating the high value features in an application and weeding out the ones of low value from the next releases. Currently, there are few methods and tools that deal with feature identification and they address the problem only partially. Therefore, there is an urgent need of methods and tools that would enable systematic feature reduction to resolve issues resulting from feature creep. This paper presents an approach and an associated tool to automate feature identification for web applications. For empirical validation, a multiple case study was conducted using three well known web applications: Youtube, Google and BBC. The results indicate that there is a good potential for automating feature identification in web applications.","n":0.085}}},{"i":558,"$":{"0":{"v":"A workload characterization study of the 1998 World Cup Web site","n":0.302},"1":{"v":"This article presents a detailed workload characterization study of the 1998 World Cup Web site. Measurements from this site were collected over a three-month period. During this time the site received 1.35 billion requests, making this the largest Web workload analyzed to date. By examining this extremely busy site and through comparison with existing characterization studies, we are able to determine how Web server workloads are evolving. We find that improvements in the caching architecture of the World Wide Web are changing the workloads of Web servers, but major improvements to that architecture are still necessary. In particular, we uncover evidence that a better consistency mechanism is required for World Wide Web caches.","n":0.094}}},{"i":559,"$":{"0":{"v":"Multi-Tenant Cloud Service Composition Using Evolutionary Optimization","n":0.378},"1":{"v":"In Software as a Service (SaaS)cloud marketplace, several functionally equivalent services tend to be available with different Quality of Service (QoS)values. For processing end-users multi-dimensional QoS and functional requirements, the application engineers are required to choose suitable services and optimize the service composition plans for each category of users. However, existing approaches for dynamic services composition tend to support execution plans that search for service provisions of equivalent functionalities with varying QoS or cost constraints to meet the tenants' QoS requirements or to dynamically respond to changes in QoS. These approaches tend to ignore the fact that multi-tenant execution plans need to provide variant execution plans, each offering a customized plan for a given tenant with its functionality, QoS and cost requirements. Henceforth, the dynamic selection and composition of multi-tenant service composition is a NP-hard dynamic multiobjective optimization problem. To address these challenges, we propose a novel multi-tenant middleware for dynamic service composition in the SaaS cloud. In particular, we present new encoding representation and fitness functions that model the service selection and composition as an evolutionary search. We incorporate our approach with two Multi-Objective Evolutionary Algorithms (MOEA), i.e., MOEA/D-STM and NSGA-II, to perform a comparative study. The experiment results show that the MOEA/D-STM outperforms NSGA-II in terms of quality of solutions and computation time.","n":0.068}}},{"i":560,"$":{"0":{"v":"An Evaluation of ARFIMA (Autoregressive Fractional Integral Moving Average) Programs","n":0.316},"1":{"v":"Strong coupling between values at different times that exhibit properties of long range dependence, non-stationary, spiky signals cannot be processed by the conventional time series analysis. The autoregressive fractional integral moving average (ARFIMA) model, a fractional order signal processing technique, is the generalization of the conventional integer order models—autoregressive integral moving average (ARIMA) and autoregressive moving average (ARMA) model. Therefore, it has much wider applications since it could capture both short-range dependence and long range dependence. For now, several software programs have been developed to deal with ARFIMA processes. However, it is unfortunate to see that using different numerical tools for time series analysis usually gives quite different and sometimes radically different results. Users are often puzzled about which tool is suitable for a specific application. We performed a comprehensive survey and evaluation of available ARFIMA tools in the literature in the hope of benefiting researchers with different academic backgrounds. In this paper, four aspects of ARFIMA programs concerning simulation, fractional order difference filter, estimation and forecast are compared and evaluated, respectively, in various software platforms. Our informative comments can serve as useful selection guidelines.","n":0.074}}},{"i":561,"$":{"0":{"v":"Software Metrics : A Rigorous and Practical Approach","n":0.354},"1":{"v":"From the Publisher:\r\nThe Second Edition of Software Metrics provides an up-to-date, coherent, and rigorous framework for controlling, managing, and predicting software development processes. With an emphasis on real-world applications, Fenton and Pfleeger apply basic ideas in measurement theory to quantify software development resources, processes, and products. The book offers an accessible and comprehensive introduction to software metrics, now an essential component of software engineering for both classroom and industry. Software Metrics features extensive case studies from Hewlett Packard, IBM, the U.S. Department of Defense, Motorola, and others, in addition to worked examples and exercises. The Second Edition includes up-to-date material on process maturity and measurement, goal-question-metric, planning a metrics program, measurement in practice, experimentation, empirical studies, ISO9216, and metric tools.","n":0.091}}},{"i":562,"$":{"0":{"v":"Semantic similarity in a taxonomy: an information-based measure and its application to problems of ambiguity in natural language","n":0.236},"1":{"v":"This article presents a measure of semantic similarity in an IS-A taxonomy based on the notion of shared information content. Experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge-counting approach. The article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity, along with experimental results demonstrating their effectiveness.","n":0.125}}},{"i":563,"$":{"0":{"v":"The Strength of Weak Ties","n":0.447},"1":{"v":"Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups.","n":0.086}}},{"i":564,"$":{"0":{"v":"Focus Groups as Qualitative Research","n":0.447},"1":{"v":"Introduction Focus Groups as Qualitative Method The Uses of Focus Groups Planning and Research Design for Focus Groups Conducting and Analyzing Focus Groups Additional Possibilities Conclusions","n":0.196}}},{"i":565,"$":{"0":{"v":"Grounded Theory Research: Procedures, Canons and Evaluative Criteria","n":0.354},"1":{"v":"Using grounded theory as an example, this paper examines three methodological questions that are generally applicable to all qualitative methods. How should the usual scientific canons be reinterpreted for qualitative research? How should researchers report the procedures and canons used in their research? What evaluative criteria should be used in judging the research products? We propose that the criteria should be adapted to fit the procedures of the method. We demonstrate how this can be done for grounded theory and suggest criteria for evaluating studies following this approach. We argue that other qualitative researchers might be similarly specific about their procedures and evaluative criteria.","n":0.098}}},{"i":566,"$":{"0":{"v":"Software engineering economics","n":0.577},"1":{"v":"This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.","n":0.125}}},{"i":567,"$":{"0":{"v":"AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis","n":0.354},"1":{"v":"If patterns are good ideas that can be re-applied to new situations, AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis looks at what goes wrong in software development, time and time again. This entertaining and often enlightening text defines what seasoned developers have long suspected: despite advances in software engineering, most software projects still fail to meet expectations--and about a third are cancelled altogether. The authors of AntiPatterns draw on extensive industry experience, their own and others, to help define what's wrong with software development today. They outline reasons why problem patterns develop (such as sloth, avarice, and greed) and proceed to outline several dozen patterns that can give you headaches or worse. Their deadliest hit list begins with the Blob, where one object does most of the work in a project, and Continuous Obsolescence, where technology changes so quickly that developers can't keep up. Some of the more entertaining antipatterns include the Poltergeist (where do-nothing classes add unnecessary overhead), the Boat Anchor (a white elephant piece of hardware or software bought at great cost) and the Golden Hammer (a single technology that is used for every conceivable programming problem). The authors then proceed to define antipatterns oriented toward management problems with software (including Death by Planning and Project Mismanagement, along with several miniature antipatterns, that help define why so many software projects are late and overbudget). The authors use several big vendors' technologies as examples of today's antipatterns. Luckily, they suggest ways to overcome antipatterns and improve software productivity in \"refactored solutions\" that can overcome some of these obstacles. However, this is a realistic book, a mix of \"Dilbert\" and software engineering. A clever antidote to getting too optimistic about software development, AntiPatterns should be required reading for any manager facing a large-scale development project. --Richard Dragan","n":0.058}}},{"i":568,"$":{"0":{"v":"The influence of organizational structure on software quality: an empirical case study","n":0.289},"1":{"v":"Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness.","n":0.082}}},{"i":569,"$":{"0":{"v":"The Delphi method : techniques and applications","n":0.378}}},{"i":570,"$":{"0":{"v":"When to Use Focus Groups and Why","n":0.378}}},{"i":571,"$":{"0":{"v":"Theoretical Sensitivity: Advances in the Methodology of Grounded Theory","n":0.333}}},{"i":572,"$":{"0":{"v":"Socio-technical congruence: a framework for assessing the impact of technical and work dependencies on software development productivity","n":0.243},"1":{"v":"The identification and management of work dependencies is a fundamental challenge in software development organizations. This paper argues that modularization, the traditional technique intended to reduce interdependencies among components of a system, has serious limitations in the context of software development. We build on the idea of congruence, proposed in our prior work, to examine the relationship between the structure of technical and work dependencies and the impact of dependencies on software development productivity. Our empirical evaluation of the congruence framework showed that when developers' coordination patterns are congruent with their coordination needs, the resolution time of modification requests was significantly reduced. Furthermore, our analysis highlights the importance of identifying the \"right\" set of technical dependencies that drive the coordination requirements among software developers. Call and data dependencies appear to have far less impact than logical dependencies.","n":0.085}}},{"i":573,"$":{"0":{"v":"Predicting failures with developer networks and social network analysis","n":0.333},"1":{"v":"Software fails and fixing it is expensive. Research in failure prediction has been highly successful at modeling software failures. Few models, however, consider the key cause of failures in software: people. Understanding the structure of developer collaboration could explain a lot about the reliability of the final product. We examine this collaboration structure with the developer network derived from code churn information that can predict failures at the file level. We conducted a case study involving a mature Nortel networking product of over three million lines of code. Failure prediction models were developed using test and post-release failure data from two releases, then validated against a subsequent release. One model's prioritization revealed 58% of the failures in 20% of the files compared with the optimal prioritization that would have found 61% in 20% of the files, indicating that a significant correlation exists between file-based developer network metrics and failures.","n":0.082}}},{"i":574,"$":{"0":{"v":"Organizational social structures for software engineering","n":0.408},"1":{"v":"Software engineering evolved from a rigid process to a dynamic interplay of people (e.g., stakeholders or developers). Organizational and social literature call this interplay an Organizational Social Structure (OSS). Software practitioners still lack a systematic way to select, analyze, and support OSSs best fitting their problems (e.g., software development). We provide the state-of-the-art in OSSs, and discuss mechanisms to support OSS-related decisions in software engineering (e.g., choosing the OSS best fitting development scenarios). Our data supports two conclusions. First, software engineering focused on building software using project teams alone, yet these are one of thirteen OSS flavors from literature. Second, an emerging OSS should be further explored for software development: social networks. This article represents a first glimpse at OSS-aware software engineering, that is, to engineer software using OSSs best fit for the problem.","n":0.086}}},{"i":575,"$":{"0":{"v":"A Computer Network Model of Human Transactive Memory","n":0.354},"1":{"v":"Several of the design factors that must be considered in linking computers together into networks are also relevant to the ways in which individual human memory systems are linked into group memory systems. These factors include directory updating (learning who knows what in the group), information allocation (assigning memory items to group members), and retrieval coordination (planning how to find items in a way that takes advantage of who knows what). When these processes operate effectively in a group, the group's transactive memory is likely to be effective.","n":0.107}}},{"i":576,"$":{"0":{"v":"AwareCon: Situation Aware Context Communication","n":0.447},"1":{"v":"Ubicomp environments impose tough constraints on networks, including immediate communication, low energy consumption, minimal maintenance and administration. With the AwareCon network, we address these challenges by prescribing an integrated architecture that differs from classical networking, as it features an awareness of the surrounding situation and context. In various settings, where AwareCon was implemented on tiny battery driven devices, we show that applications and usability of devices benefit from this approach.","n":0.12}}},{"i":577,"$":{"0":{"v":"Social Debt in Software Engineering: Insights from Industry","n":0.354},"1":{"v":"Social debt is analogous to technical debt in many ways: it represents the state of software development organisations as the result of “accumulated” decisions. In the case of social debt, decisions are about people and their interactions. Our objective was to study the causality around social debt in practice. In so doing, we conducted exploratory qualitative research in a large software company. We found many forces together causing social debt; we represented them in a framework, and captured anti-patterns that led to the debt in the first place. Finally, we elicited best practices that technicians adopted to pay back some of the accumulated debt. We learned that social debt is strongly correlated with technical debt and both forces should be reckoned with together during the software process.","n":0.089}}},{"i":578,"$":{"0":{"v":"Socio-technical developer networks: should we trust our measurements?","n":0.354},"1":{"v":"Software development teams must be properly structured to provide effectiv collaboration to produce quality software. Over the last several years, social network analysis (SNA) has emerged as a popular method for studying the collaboration and organization of people working in large software development teams. Researchers have been modeling networks of developers based on socio-technical connections found in software development artifacts. Using these developer networks, researchers have proposed several SNA metrics that can predict software quality factors and describe the team structure. But do SNA metrics measure what they purport to measure? The objective of this research is to investigate if SNA metrics represent socio-technical relationships by examining if developer networks can be corroborated with developer perceptions. To measure developer perceptions, we developed an online survey that is personalized to each developer of a development team based on that developer's SNA metrics. Developers answered questions about other members of the team, such as identifying their collaborators and the project experts. A total of 124 developers responded to our survey from three popular open source projects: the Linux kernel, the PHP programming language, and the Wireshark network protocol analyzer. Our results indicate that connections in the developer network are statistically associated with the collaborators whom the developers named. Our results substantiate that SNA metrics represent socio-technical relationships in open source development projects, while also clarifying how the developer network can be interpreted by researchers and practitioners.","n":0.065}}},{"i":579,"$":{"0":{"v":"Working Digital Money into a Cash Economy: The Collaborative Work of Loan Payment","n":0.277},"1":{"v":"This paper examines how different forms of money, specifically digital versus cash, impact on the work of an organisation and its customers. In doing so it contributes to the body of literature exploring how the social meanings of money impact on practice. We describe the findings of an ethnographic study examining loan collection workflows, where bank loans given to auto-rickshaw drivers to purchase their auto-rickshaw are overseen and managed by an intermediary. We found that making the mobile money service usable for the drivers took considerable work and was largely achieved because it was embedded in the wider, trusted, loan payment ecosystem. Although Airtel Money promises anytime, anywhere payments, payment remains time and place bound for the drivers. It is tempting to take a transactional approach to payments, and indeed we initially approached the problem of enabling frequent payments as one of payment mechanism. However, in practice payments are embedded in sets of social relations and a socio-technical ecosystem. It takes considerable collaborative work, and a fair amount of flexibility, to enable these financially vulnerable drivers to pay off their loans, and reducing the issue to one of payment mechanism alone does not tell the full story.","n":0.071}}},{"i":580,"$":{"0":{"v":"From developer networks to verified communities: a fine-grained approach","n":0.333},"1":{"v":"Effective software engineering demands a coordinated effort. Unfortunately, a comprehensive view on developer coordination is rarely available to support software-engineering decisions, despite the significant implications on software quality, software architecture, and developer productivity. We present a fine-grained, verifiable, and fully automated approach to capture a view on developer coordination, based on commit information and source-code structure, mined from version-control systems. We apply methodology from network analysis and machine learning to identify developer communities automatically. Compared to previous work, our approach is fine-grained, and identifies statistically significant communities using order-statistics and a community-verification technique based on graph conductance. To demonstrate the scalability and generality of our approach, we analyze ten open-source projects with complex and active histories, written in various programming languages. By surveying 53 open-source developers from the ten projects, we validate the authenticity of inferred community structure with respect to reality. Our results indicate that developers of open-source projects form statistically significant community structures and this particular view on collaboration largely coincides with developers' perceptions of real-world collaboration.","n":0.077}}},{"i":581,"$":{"0":{"v":"Agile in Distress: Architecture to the Rescue","n":0.378},"1":{"v":"For large-scale software-development endeavors, agility is enabled by architecture, and vice versa. The iterative, risk-driven life cycle inherent in agile approaches allows developers to focus early on key architectural decisions, spread these decisions over time, and validate architectural solutions early. Conversely, an early focus on architecture allows a large agile project to define an implementation structure that drives an organization into small teams, some focusing on common elements and their key relationships and some working more autonomously on features. Architects in agile software development typically work on three distinct but interdependent structures: architecture of the system, the structure of the development organization, and the production infrastructure. Architectural work supports the implementation of high-priority business features without risking excessive redesign later or requiring heavy coordination between teams. Architectural tactics provide a framework for identifying key concerns and guide the alignment of these three structures throughout the development life cycle.","n":0.082}}},{"i":582,"$":{"0":{"v":"The Architect's Role in Community Shepherding","n":0.408},"1":{"v":"Software architects don't just design architecture components or champion architecture qualities; they often must guide and harmonize the entire community of project stakeholders. The community-shepherding aspects of the architect's role have been gaining attention, given the increasing importance of complex \"organizational rewiring\" scenarios such as DevOps, open source strategies, transitions to agile development, and corporate acquisitions. In these scenarios, architects would benefit by having effective models to align communities with architectures. This article discusses the \"smells\" indicating that a community isn't functioning efficiently, offers a set of mitigations for those smells, and provides an overview of community types.","n":0.101}}},{"i":583,"$":{"0":{"v":"Uncovering Latent Social Communities in Software Development","n":0.378},"1":{"v":"Software development is increasingly carried out by developer communities in a global setting. One way to prepare for development success is to uncover and harmonize these communities to exploit their collective, collaborative potential. A proposed decision tree can help practitioners do this.","n":0.154}}},{"i":584,"$":{"0":{"v":"Cognitive interdependence in close relationships","n":0.447},"1":{"v":"This chapter is concerned with the thinking processes of the intimate dyad. So, although we will focus from time to time on the thinking processes of the individual—as they influence and are influenced by the relationship with another person—our prime interest is in thinking as it occurs at the dyadic level. This may be dangerous territory for inquiry. After all, this topic resembles one that has, for many years now, represented something of a “black hole” in the social sciences—the study of the group mind. For good reasons, the early practice of drawing an analogy between the mind of the individual and the cognitive operations of the group has long been avoided, and references to the group mind in contemporary literature have dwindled to a smattering of wisecracks.","n":0.088}}},{"i":585,"$":{"0":{"v":"The lonesome architect","n":0.577},"1":{"v":"Although the benefits are well-known and undisputed, sharing architectural knowledge is not something architects automatically do. In an attempt to better understand what architects really do and what kind of support they need for sharing knowledge, we have conducted large-scale survey research. The results of our study indicate that architects can be characterized as rather lonesome decision makers who mainly consume, but neglect documenting and actively sharing architectural knowledge. Acknowledging this nature of architects suggests ways to develop more effective support for architectural knowledge sharing.","n":0.108}}},{"i":586,"$":{"0":{"v":"The Dark Side of Software Engineering: Evil on Computing Projects","n":0.316},"1":{"v":"FOREWORD ( Linda Rising). INTRODUCTION. I.1 What's the Dark Side? I.1.1 Why the Dark Side? I.1.2 Who Cares About the Dark Side? I.1.3 How Dark is the Dark Side? I.1.4 What Else is on the Dark Side? I.1.5 Ethics and the Dark Side. I.1.6 Personal Anecdotes About the Dark Side. Reference. PART 1: DARK SIDE ISSUES. CHAPTER 1 SUBVERSION. 1.1 Introductory Case Studies and Anecdotes. 1.1.1 A Faculty Feedback System. 1.1.2 An Unusual Cooperative Effort. 1.1.3 Lack of Cooperation due to Self Interest. 1.1.4 An Evil Teammate. 1.1.5 Thwarting the Evil Union. 1.2 The Survey: Impact of Subversive Stakeholders On Software Projects. 1.2.1 Introduction. 1.2.2 The Survey. 1.2.3 The Survey Findings. 1.2.4 Conclusions. 1.2.5 Impact on Practice. 1.2.6 Impact on Research. 1.2.7 Limitations. 1.2.8 Challenges. 1.2.9 Acknowledgments. 1.3 Selected Responses. 1.3.1 Sample Answers to the Question: \"What Were the Motivations and Goals of the Subversive Stakeholders?\" 1.3.2 Sample Answers to the Question \"How Were the Subversive Attacks Discovered?\" 1.3.3 Sample Answers to the Question \"How Can Projects be Defended Against Subversive Stakeholders?\" 1.4 A Follow-Up to the Survey: Some Hypotheses and Related Survey Findings. References. CHAPTER 2 LYING. 2.1 Introductory Case Studies and Anecdotes. 2.2 Incidents of Lying: The Survey. 2.2.1 The Survey Results. 2.2.2 General Scope. 2.2.3 An Overview of the Problem. 2.2.4 Clarifi cation of Terms. 2.2.5 Discussion. 2.2.6 Conclusions. 2.2.7 Limitations. 2.3 Qualitative Survey Responses on Lying. 2.4 What Can Be Done About Lying? 2.5 The Questionnaire Used in the Survey. References. CHAPTER 3 HACKING. 3.1 Case Studies of Attacks and Biographies of Hackers. 3.2 Cyber Terrorism and Government-Sponsored Hacking. 3.3 The Hacker Subculture. 3.3.1 Why They Are Called \"Hackers\". 3.3.2 Motivation of Hackers. 3.3.3 Hacker Slang. 3.3.4 Hacker Ethics. 3.3.5 Public Opinion about Hackers. 3.4 How a Hacker Is Identified. 3.5 Time Line of a Typical Malware Attack. 3.6 Hacker Economy: How Does a Hacker Make Money? 3.7 Social Engineering. 3.7.1 Social Engineering Examples and Case Studies. 3.7.2 Tactics of Social Engineering. 3.8 A Lingering Question. 3.9 Late-Breaking News. CHAPTER 4 THEFT OF INFORMATION. 4.1 Introduction. 4.2 Case Studies. 4.2.1 Data Theft. 4.2.2 Source Code Theft. 4.3 How Do the Victims Find Out That Their Secrets Are Stolen? 4.4 Intellectual Property Protection. 4.4.1 Trade Secret Protection. 4.4.2 Copyright Protection. 4.4.3 Patent Protection. 4.4.4 Steganography. 4.5 Open Versus Closed Source. CHAPTER 5 ESPIONAGE. 5.1 Introduction. 5.2 What Is Espionage? 5.3 Case Studies. 5.3.1 Sweden Versus Russia. 5.3.2 Shekhar Verma. 5.3.3 Lineage III. 5.3.4 GM versus VW: Jose Ignacio Lopez. 5.3.5 British Midland Tools. 5.3.6 Solid Oak Software. 5.3.7 Proctor & Gamble versus Unilever. 5.3.8 News Corp Versus Vivendi. 5.3.9 Spying: Was A TI Chip Really Stolen by a French Spy? 5.3.10 Confi cker. 5.4 Cyber Warfare. Reference. CHAPTER 6 DISGRUNTLED EMPLOYEES AND SABOTAGE. 6.1 Introduction and Background. 6.2 Disgruntled Employee Data Issues. 6.2.1 Data Tampering. 6.2.2 Data Destruction. 6.2.3 Data Made Public. 6.2.4 Theft Via Data. 6.3 Disgruntled Employee Software Issues. 6.3.1 Software Destruction. 6.4 Disgruntled Employee System Issues. 6.5 What to Do About Disgruntled Employee Acts. 6.6 Sabotage. References. CHAPTER 7 WHISTLE-BLOWING. 7.1 A Hypothetical Scenario. 7.2 Whistle-Blowing and Software Engineering. 7.3 More Case Studies and Anecdotes. 7.3.1 Jeffrey Wigand and Brown and Williamson Tobacco. 7.3.2 A Longitudinal Study of Whistle-Blowing. 7.3.3 An Even More Pessimistic View. 7.3.4 Academic Whistle-Blowing. 7.3.5 The Sum Total of Whistle-Blowing. References. APPENDIX TO CHAPTER 7 PRACTICAL IMPLICATIONS OF THE RESEARCH INTO WHISTLE-BLOWING. References. PART 2: VIEWPOINTS ON DARK SIDE ISSUES. Introduction. CHAPTER 8 OPINIONS, PREDICTIONS, AND BELIEFS. 8.1 Automated Crime (Donn B. Parker). Information Sources. 8.2 Let's Play Make Believe (Karl E. Wiegers). Reference. 8.3 Dark, Light, or Just Another Shade of Grey? (Les Hatton). 8.4 Rational Software Developers as Pathological Code Hackers (Norman Fenton). CHAPTER 9 PERSONAL ANECDOTES. 9.1 An Offi cer and a Gentleman Confronts the Dark Side (Grady Booch). 9.2 Less Carrot and More Stick (June Verner). References. 9.3 \"Them and Us\": Dispatches from the Virtual Software Team Trenches (Valentine Casey). 9.4 What is it to Lie on a Software Project? (Robert N. Britcher). 9.5 \"Merciless Control Instrument\" and the Mysterious Missing Fax (A. H. (anonymous)). 9.6 Forest of Arden (David Alan Grier). 9.7 Hard-Headed Hardware Hit Man (Will Tracz). 9.8 A Lighthearted Anecdote (Eugene Farmer). CONCLUSIONS. INDEX.","n":0.038}}},{"i":587,"$":{"0":{"v":"Interviewing Techniques","n":0.707}}},{"i":588,"$":{"0":{"v":"Interactive churn metrics: socio-technical variants of code churn","n":0.354},"1":{"v":"A central part of software quality is finding bugs. One method of finding bugs is by measuring important aspects of the software product and the development process. In recent history, researchers have discovered evidence of a \"code churn\" effect whereby the degree to which a given source code file has changed over time is correlated with faults and vulnerabilities. Computing the code churn metric comes from counting source code differences in version control repositories. However, code churn does not take into account a critical factor of any software development team: the human factor, specifically who is making the changes. In this paper, we introduce a new class of human-centered metrics, \"interactive churn metrics\" as variants of code churn. Using the git blame tool, we identify the most recent developer who changed a given line of code in a file prior to a given revision. Then, for each line changed in a given revision, determined if the revision author was changing his or her own code (\"self churn\"), or the author was changing code last modified by somebody else (\"interactive churn\"). We derive and present several metrics from this concept. Finally, we conducted an empirical analysis of these metrics on the PHP programming language and its post-release vulnerabilities. We found that our interactive churn metrics are statistically correlated with post-release vulnerabilities and only weakly correlated with code churn metrics and source lines of code. The results indicate that interactive churn metrics are associated with software quality and are different from the code churn and source lines of code.","n":0.062}}},{"i":589,"$":{"0":{"v":"When Software Architecture Leads to Social Debt","n":0.378},"1":{"v":"Social and technical debt both represent the state of software development organizations as a result of accumulated decisions. In the case of social debt, decisions (and connected debt) weigh on people and their socio-technical interactions/characteristics. Digging deeper into social debt with an industrial case-study, we found that software architecture, the prince of development artefacts, plays a major role in causing social debt. This paper discusses a key circumstance wherefore social debt is connected to software architectures and what can be done and measured in response, as observed in our case-study. Also, we introduce DAHLIA, that is \"Debt-Aimed Architecture-Level Incommunicability Analysis\" - a framework to elicit some of the causes behind social debt for further analysis.","n":0.093}}},{"i":590,"$":{"0":{"v":"General methods for software architecture recovery: a potential approach and its evaluation","n":0.289},"1":{"v":"Software architecture is a critical artefact in the software lifecycle. It is a system blueprint for construction, it aids in planning teaming and division of work, and it aids in reasoning about system properties. But architecture documentation is seldom created and, even when it is initially created, it is seldom maintained. For these reasons organisations often feel the need to recover legacy architectures, for example, as part of planning for evolution or cloud migration. But there is no existing general architecture recovery approach nor tool that can be applied to any type of system, under any condition. We will show that one way of achieving such generality is to apply systematic code inspection following a Grounded Theory (GT) approach. Though relatively costly and human-intensive, a GT-based approach has several merits, for example: (a) it is general by design; (b) it can be partially automated; (c) it yields evidence-based results rooted of the system being examined. This article presents one theoretical formulation of a general architecture recovery method–called REM–and reports on the evaluation of REM in the context of a large architecture recovery campaign performed for the European Space Agency. Our results illustrate some intriguing properties and opportunities of GT-based architecture recovery approaches and point out lessons learned and venues for further research.","n":0.069}}},{"i":591,"$":{"0":{"v":"Going Beyond the Data: Empirical Validation Leading to Grounded Theory","n":0.316},"1":{"v":"The purpose of this study is two-fold. First, a validation study on Construct-TM is conducted to show that modeling the actual and cognitive knowledge networks of a group can produce agent interactions within the model that correlate significantly with the communication network obtained from empirical data. Second, empirically grounded theory is produced by combining empirical data with simulation experiments run on empirically validated models.","n":0.125}}},{"i":592,"$":{"0":{"v":"Who is the expert? combining intention and knowledge of online discussants in collaborative RE tasks","n":0.258},"1":{"v":"Large, distributed software development projects rely on the collaboration of culturally heterogeneous and geographically distributed stakeholders. Software requirements, as well as solution ideas are elicited in distributed processes, which increasingly use online forums and mailing lists, in which stakeholders mainly use free or semi-structured natural language text. The identification of contributors of key information about a given topic --called experts, in both the software domain and code-- and in particular an automated support for retrieving information from available online resources, are becoming of crucial importance. In this paper, we address the problem of expert finding in mailing-list discussions, and propose an approach which combines content- and intent-based information extraction for ranking online discussants with respect to their expertise in the discussed topics. We illustrate its application on an example.","n":0.088}}},{"i":593,"$":{"0":{"v":"The influence of human aspects on software process improvement: Qualitative research findings and comparison to previous studies","n":0.243},"1":{"v":"Background: Understanding how to successfully deal with human and technical aspects involved in Software Process Improvement (SPI) programs is a challenging issue. Technical aspects are not enough to guarantee the success of activities. Human factors (e.g., experiences, opinions and perceptions) impact the effectiveness of SPI programs. Aim: this paper aims to improve our current understanding on how human aspects can influence SPI programs from the point of view of the professionals involved in such efforts. We compare our findings with the results of previous studies in order to analyze the different contexts in which human aspects influence SPI. Method: We conducted a qualitative study in the context of small companies involved in SPI programs in Amazonas State in Brazil based on the Brazilian Maturity Model - MPS.BR. We used semi-structural interviews with software engineers. Results: We identified findings about eleven of the fourteen investigated human aspects. We identified new characteristics that motivate actors to get involved in SPI programs. Finally, while previous research indicated that the process of individual decision making as a negative aspect, we identified this as a positive aspect. Conclusions: Despite the differences in companies' size and maturity models, our findings corroborate many of the important results of previous case studies. The consistency of the presented results with previous research in different organizations, teams, and countries suggest that these results are true for software engineering in general. However, there were differences related to the motivating factors and the perception of the individual decision making. In our study, we observed that the personality, perception and employee selection aspects were also significant to the SPI program's success. This has not been re","n":0.061}}},{"i":594,"$":{"0":{"v":"Virtual Teamwork: Mastering the Art and Practice of Online Learning and Corporate Collaboration","n":0.277},"1":{"v":"\"This book, by Robert Ubell and his excellent team of collaborators, adds an important dimension to effective teaching and learning in online environments. It addresses how interaction and collaboration online can be effectively harnessed in virtual teams. It is an important contribution to the larger field of Internet-based education.\" Frank Mayadas, Alfred P. Sloan Foundation How to create and manage highly successful teams online With the advent of the global economy and high-speed Internet, online collaboration is fast becoming the norm in education and industry. This book takes online collaboration to the next level, showing how you can bolster online learning and business performance with the innovative use of virtual teams. Written by a team of experts headed by online learning pioneer Robert Ubell, Virtual Teamwork covers best practices for online instruction and team learning, reveals proven techniques for managing enterprise and global virtual teams, and helps you choose the best communication tools for the job. Educators, project managers, and anyone involved in teaching online courses or creating online programs will find a wealth of tips and techniques for building and managing successful virtual teams, including guidance for: Integrating team instruction in the virtual classroom Using best techniques for team interaction across borders and time zones Structuring cost-effective, competitive projects that work Leveraging leadership, mentoring, and conflict management in virtual teams Conducting testing, grading, and peer- and self-assessment online Managing corporate, global, and engineering virtual teams Choosing the right technologies for effective collaboration","n":0.064}}},{"i":595,"$":{"0":{"v":"Managing technical debt in practice: an industrial report","n":0.354},"1":{"v":"The Technical Debt (TD) metaphor has been used as a way to manage and communicate long-term consequences that some decisions may cause. However the state of the art in TD has not culminated yet in rigorous analysis models for large-scale projects. This work analyses an industrial project, from the perspective of its decisions and related events, so that we can better characterize the existence of TD and show the evolution of its parameters. The project in study had a life cycle of six years (2005-2011) and its data for analysis was collected from emails, documents, CVS logs, code files and interviews with developers and project managers. From this analysis, we identified the factors that had influence on the project decisions and their impact on the system along the time. Furthermore, we were able to extract a set of lessons associated with the characterization of TD in projects of this port.","n":0.082}}},{"i":596,"$":{"0":{"v":"Architecting in Networked Organizations","n":0.5},"1":{"v":"The context of software architecting increasingly reflects webs of IT companies pooling resources together for software development. What results is a networked organization, populated by heterogeneous development communities connected via internet. How does this scenario change the process of software architecting? Pivoting around this research question, this paper presents architecture concerns relevant in such networked development scenarios. Supporting these concerns is critical to understand the impact of architecture on organizational change and vice versa. To this aim, we introduce a viewpoint, its supporting tool and evaluate both through a case-study.","n":0.105}}},{"i":597,"$":{"0":{"v":"Dynamic networked organizations for software engineering","n":0.408},"1":{"v":"Current practice in software engineering suggests a radical change in perspective: where once stood fixed teams of people following a development plan, now stand just-in-time Dynamic Networked Organizations (DyNOs), adopting a common flexible strategy for development, rather than a plan. This shift in perspective has gone relatively unnoticed by current software engineering research. This paper offers a glimpse at what processes and instruments lie beyond “current” software engineering research, where studying emergent DyNOs, their creation and steering becomes critical. To understand the underpinnings of this evolution, we explored a simple yet vivid scenario from real-life industrial practice. Using scenario analysis we elicited a number of social and organizational requirements in working with DyNOs. Also, comparing our evidence with literature, we made some key observations. First, managing DyNOs makes organizational requirements a first-class entity for development success. Second, research in software engineering should be invested in understanding and governing the DyNOs behind the software lifecycle.","n":0.081}}},{"i":598,"$":{"0":{"v":"Collective programming: making end-user programming (more) social","n":0.378},"1":{"v":"The do-it-yourself Web 2.0 culture is quickly creating and sharing more end-user produced content. Gradually moving from static content, such as pictures and text, to interactive content, such as end-user programmed games, the artifacts created and shared have become significantly more sophisticated. The next frontier to make end-user programming more social is to move beyond the current create, upload, share, download, and repeat Web 2.0 models. Collective Programming is a framework that fuses 100% Web-native end-user programming tools with real-time communication mechanisms into a cloud-based multi end-user programming environment. A prototype built, called CyberCollage, enables groups of students to work on game design projects together: they can play multi-user games, change game worlds in real-time, and engage in virtual pair programming.","n":0.091}}},{"i":599,"$":{"0":{"v":"Grounded Theory Applications in Reviewing Knowledge Management Literature","n":0.354},"1":{"v":"Literature reviews are complex, time-consuming undertakings. Most research methodologies are deductive and require the researcher to commence with a review to inform the study and formulate the research question. On the other hand the grounded theory method is inductive, ending with a theory rather than beginning with a hypothesis, and literature in these studies is typically used as a comparator for the emerging theory. This paper proposes that a theory of contemporary thought in one particular subject, knowledge management, may be developed using a grounded theory method of reviewing its literature.","n":0.105}}},{"i":600,"$":{"0":{"v":"The Frictionless Development Environment Scorecard","n":0.447},"1":{"v":"The environment in which we work as developers can make a tremendous difference on our productivity and well-being. Yet it's easy to get trapped in an unproductive setup by inertia, and thus suffer death by a thousand cuts. A scorecard we can use to evaluate and fix the environment we work in covers the setup of our workstation and working environment, our ability to access remote hosts, general-purpose tools, editing, debugging, application development, and the specific problem at hand. Some fixes involve tweaks in our setup, and others might require us to install new tools, learn new skills, and negotiate with our managers. They're all worthwhile investments. The Web extra at http://youtu.be/AhMETa0tvkw is an audio podcast of author Diomidis Spinellis reading his Tools of the Trade column, in which he discusses how a scorecard can help evaluate and fix the environment you work in by covering the setup of your workstation and working environment, your ability to access remote hosts, general-purpose tools, editing, debugging, application development, and the specific problem at hand.","n":0.076}}},{"i":601,"$":{"0":{"v":"Transactive Memory System, Communication Quality, and Knowledge Sharing in Distributed Teams: An Empirical Examination in Open Source Software Project Teams","n":0.224},"1":{"v":"This study develops a research model explaining knowledge sharing among open source software (OSS) developers based on the theories of transactive memory system (TMS) and compensatory adaptation. Specifically, this study proposes that the TMS should be positively associated with knowledge sharing and the quality of communication among OSS developers. The quality of communication is positively associated with knowledge sharing. Furthermore, this study suggests that communication quality partially mediate the positive relationship between TMS and knowledge sharing. By collecting data from 120 OSS project teams, this study empirically confirms all the hypotheses.","n":0.105}}},{"i":602,"$":{"0":{"v":"An Empirical Study on the Functional Mechanism among Team Trust, Team Commitment and Team Innovation in Chinese High Technology Innovation Teams.","n":0.218}}},{"i":603,"$":{"0":{"v":"Guidelines on Security and Privacy in Public Cloud Computing","n":0.333},"1":{"v":"NIST Special Publication 800-144 - Cloud computing can and does mean different things to different people. The common characteristics most interpretations share are on-demand scalability of highly available and reliable pooled computing resources, secure access to metered services from nearly anywhere, and displacement of data and services from inside to outside the organization. While aspects of these characteristics have been realized to a certain extent, cloud computing remains a work in progress. This publication provides an overview of the security and privacy challenges pertinent to public cloud computing and points out considerations organizations should take when outsourcing data, applications, and infrastructure to a public cloud environment.~","n":0.097}}},{"i":604,"$":{"0":{"v":"Goal-oriented requirements engineering: a guided tour","n":0.408},"1":{"v":"Goals capture, at different levels of abstraction, the various objectives the system under consideration should achieve. Goal-oriented requirements engineering is concerned with the use of goals for eliciting, elaborating, structuring, specifying, analyzing, negotiating, documenting, and modifying requirements. This area has received increasing attention. The paper reviews various research efforts undertaken along this line of research. The arguments in favor of goal orientation are first briefly discussed. The paper then compares the main approaches to goal modeling, goal specification and goal-based reasoning in the many activities of the requirements engineering process. To make the discussion more concrete, a real case study is used to suggest what a goal-oriented requirements engineering method may look like. Experience, with such approaches and tool support are briefly discussed as well.","n":0.089}}},{"i":605,"$":{"0":{"v":"Information security policy compliance: an empirical study of rationality-based beliefs and information security awareness","n":0.267},"1":{"v":"Many organizations recognize that their employees, who are often considered the weakest link in information security, can also be great assets in the effort to reduce risk related to information security. Since employees who comply with the information security rules and regulations of the organization are the key to strengthening information security, understanding compliance behavior is crucial for organizations that want to leverage their human capital.\r\n\r\nThis research identifies the antecedents of employee compliance with the information security policy (ISP) of an organization. Specifically, we investigate the rationality-based factors that drive an employee to comply with requirements of the ISP with regard to protecting the organization's information and technology resources. Drawing on the theory of planned behavior, we posit that, along with normative belief and self-efficacy, an employee's attitude toward compliance determines intention to comply with the ISP. As a key contribution, we posit that an employee's attitude is influenced by benefit of compliance, cost of compliance, and cost of noncompliance, which are beliefs about the overall assessment of consequences of compliance or noncompliance. We then postulate that these beliefs are shaped by the employee's outcome beliefs concerning the events that follow compliance or noncompliance: benefit of compliance is shaped by intrinsic benefit, safety of resources, and rewards, while cost of compliance is shaped by work impediment; and cost of noncompliance is shaped by intrinsic cost, vulnerability of resources, and sanctions. We also investigate the impact of information security awareness (ISA) on outcome beliefs and an employee's attitude toward compliance with the ISP.\r\n\r\nOur results show that an employee's intention to comply with the ISP is significantly influenced by attitude, normative beliefs, and self-efficacy to comply. Outcome beliefs significantly affect beliefs about overall assessment of consequences, and they, in turn, significantly affect an employee's attitude. Furthermore, ISA positively affects both attitude and outcome beliefs. As the importance of employees' following their organizations' information security rules and regulations increases, our study sheds light on the role of ISA and compliance-related beliefs in an organization's efforts to encourage compliance.","n":0.055}}},{"i":606,"$":{"0":{"v":"Security Requirements Engineering: A Framework for Representation and Analysis","n":0.333},"1":{"v":"This paper presents a framework for security requirements elicitation and analysis. The framework is based on constructing a context for the system, representing security requirements as constraints, and developing satisfaction arguments for the security requirements. The system context is described using a problem-oriented notation, then is validated against the security requirements through construction of a satisfaction argument. The satisfaction argument consists of two parts: a formal argument that the system can meet its security requirements and a structured informal argument supporting the assumptions expressed in the formal argument. The construction of the satisfaction argument may fail, revealing either that the security requirement cannot be satisfied in the context or that the context does not contain sufficient information to develop the argument. In this case, designers and architects are asked to provide additional design information to resolve the problems. We evaluate the framework by applying it to a security requirements analysis within an air traffic control technology evaluation project.","n":0.08}}},{"i":607,"$":{"0":{"v":"Handling obstacles in goal-oriented requirements engineering","n":0.408},"1":{"v":"Requirements engineering is concerned with the elicitation of high-level goals to be achieved by the envisioned system, the refinement of such goals and their operationalization into specifications of services and constraints and the assignment of responsibilities for the resulting requirements to agents such as humans, devices and software. Requirements engineering processes often result in goals, requirements, and assumptions about agent behavior that are too ideal; some of them are likely not to be satisfied from time to time in the running system due to unexpected agent behavior. The lack of anticipation of exceptional behaviors results in unrealistic, unachievable, and/or incomplete requirements. As a consequence, the software developed from those requirements will not be robust enough and will inevitably result in poor performance or failures, sometimes with critical consequences on the environment. This paper presents formal techniques for reasoning about obstacles to the satisfaction of goals, requirements, and assumptions elaborated in the requirements engineering process. The techniques are based on a temporal logic formalization of goals and domain properties; they are integrated into an existing method for goal-oriented requirements elaboration with the aim of deriving more realistic, complete, and robust requirements specifications. A key principle is to handle exceptions at requirements engineering time and at the goal level, so that more freedom is left for resolving them in a satisfactory way. The various techniques proposed are illustrated and assessed in the context of a real safety-critical system.","n":0.065}}},{"i":608,"$":{"0":{"v":"Towards Regulatory Compliance: Extracting Rights and Obligations to Align Requirements with Regulations","n":0.289},"1":{"v":"In the United States, federal and state regulations prescribe stakeholder rights and obligations that must be satisfied by the requirements for software systems. These regulations are typically wrought with ambiguities, making the process of deriving system requirements ad hoc and error prone. In highly regulated domains such as healthcare, there is a need for more comprehensive standards that can be used to assure that system requirements conform to regulations. To address this need, we expound upon a process called Semantic Parameterization previously used to derive rights and obligations from privacy goals. In this work, we apply the process to the Privacy Rule from the U.S. Health Insurance Portability and Accountability Act (HIPAA). We present our methodology for extracting and prioritizing rights and obligations from regulations and show how semantic models can be used to clarify ambiguities through focused elicitation and to balance rights with obligations. The results of our analysis can aid requirements engineers, standards organizations, compliance officers, and stakeholders in assuring systems conform to policy and satisfy requirements.","n":0.077}}},{"i":609,"$":{"0":{"v":"Modeling security requirements through ownership, permission and delegation","n":0.354},"1":{"v":"Security requirements engineering is emerging as a branch of software engineering, spurred by the realization that security must be dealt with early on during the requirements phase. Methodologies in this field are challenging, as they must take into account subtle notions such as trust (or lack thereof), delegation, and permission; they must also model entire organizations and not only systems-to-be. In our previous work we introduced Secure Tropos, a formal framework for modeling and analyzing security requirements. Secure Tropos is founded on three main notions: ownership, trust, and delegation. In this paper, we refine Secure Tropos introducing the notions of at-least delegation and trust of execution; also, at-most delegation and trust of permission. We also propose monitoring as a security design pattern intended to overcome the problem of lack of trust between actors. The paper presents a semantic for these notions, and describes an implemented formal reasoning tool based on Datalog.","n":0.081}}},{"i":610,"$":{"0":{"v":"Privacy APIs: access control techniques to analyze and verify legal privacy policies","n":0.289},"1":{"v":"There is a growing interest in establishing rules to regulate the privacy of citizens in the treatment of sensitive personal data such as medical and financial records. Such rules must be respected by software used in these sectors. The regulatory statements are somewhat informal and must be interpreted carefully in the software interface to private data. This paper describes techniques to formalize regulatory privacy rules and how to exploit this formalization to analyze the rules automatically. Our formalism, which we call privacy APIs, is an extension of access control matrix operations to include (1) operations for notification and logging and (2) constructs that ease the mapping between legal and formal language. We validate the expressive power of privacy APIs by encoding the 2000 and 2003 HIPAA consent rules in our system. This formalization is then encoded into Promela and we validate the usefulness of the formalism by using the SPIN model checker to verify properties that distinguish the two versions of HIPAA.","n":0.079}}},{"i":611,"$":{"0":{"v":"Choice and Chance: A Conceptual Model of Paths to Information Security Compromise","n":0.289},"1":{"v":"No longer the exclusive domain of technology experts, information security is now a management issue. Through a grounded approach using interviews, observations, and secondary data, we advance a model of the information security compromise process from the perspective of the attacked organization. We distinguish between deliberate and opportunistic paths of compromise through the Internet, labeled choice and chance, and include the role of countermeasures, the Internet presence of the firm, and the attractiveness of the firm for information security compromise. Further, using one year of alert data from intrusion detection devices, we find empirical support for the key contributions of the model. We discuss the implications of the model for the emerging research stream on information security in the information systems literature.","n":0.091}}},{"i":612,"$":{"0":{"v":"Systematic Elaboration of Scalability Requirements through Goal-Obstacle Analysis","n":0.354},"1":{"v":"Scalability is a critical concern for many software systems. Despite the recognized importance of considering scalability from the earliest stages of development, there is currently little support for reasoning about scalability at the requirements level. This paper presents a goal-oriented approach for eliciting, modeling, and reasoning about scalability requirements. The approach consists of systematically identifying scalability-related obstacles to the satisfaction of goals, assessing the likelihood and severity of these obstacles, and generating new goals to deal with them. The result is a consolidated set of requirements in which important scalability concerns are anticipated through the precise, quantified specification of scaling assumptions and scalability goals. The paper presents results from applying the approach to a complex, large-scale financial fraud detection system.","n":0.091}}},{"i":613,"$":{"0":{"v":"Improved practical support for large-scale requirements prioritising","n":0.378},"1":{"v":"An efficient, accurate and practical process for prioritising requirements is of great importance in commercial software developments. This article improves an existing cost-value approach in which stakeholders compare all unique pairs of candidate requirements according to their value and their cost of implementation. Techniques for reducing the required number of comparisons are suggested, thus making the process more efficient. An initial approach for managing requirements interdependencies is proposed. A support tool for the improved process has been developed to make the process more practical in commercial developments. The improved process and its support tool have been applied and evaluated in an industrial project at Ericsson Radio Systems AB. The results indicate a pressing need for mature processes for prioritising requirements, and the work presented here is an important step in that direction.","n":0.087}}},{"i":614,"$":{"0":{"v":"Using Obstacles for Systematically Modeling, Analysing, and Mitigating Risks in Cloud Adoption","n":0.289}}},{"i":615,"$":{"0":{"v":"Value-based argumentation for justifying compliance","n":0.447},"1":{"v":"Compliance is often achieved 'by design' through a coherent system of controls consisting of information systems and procedures. This system-based control requires a new approach to auditing in which companies must demonstrate to the regulator that they are 'in control'. They must determine the relevance of a regulation for their business, justify which set of control measures they have taken to comply with it, and demonstrate that the control measures are operationally effective. In this paper we show how value-based argumentation theory can be applied to the compliance domain. Corporate values motivate the selection of control measures (actions) which aim to fulfil control objectives, i.e. adopted norms (goals). In particular, we show how to formalize the audit dialogue in which companies justify their compliance decisions to regulators using value-based argunlentation. The approach is illustrated by a case study of the safety and security measures adopted in the context of EU customs regulation.","n":0.081}}},{"i":616,"$":{"0":{"v":"Risk-Aware Web Service Allocation in the Cloud Using Portfolio Theory","n":0.316},"1":{"v":"In this paper, we view the cloud as market place for trading instances of web services, which can be bought or leased by web applications. Applications can buy diversity by selecting web services from multiple cloud sellers in a cloud-based market. We argue that by diversifying the selection, we can improve the dependability of the application and reduce risks associated with service level agreements violations. We propose a novel dynamic adaptive search based software engineering approach, which uses portfolio theory to construct a diversify portfolio of web service instances, traded from multiple cloud providers. The approach systematically evaluates the Quality of Service and risks of the portfolio, compare it to the optimal traded portfolio at a given time. It can then dynamically decide on a new portfolio and adapt the application accordingly. We use a hypothetical scenario to demonstrate the effective use of the portfolio-based optimization.","n":0.083}}},{"i":617,"$":{"0":{"v":"Researching Evolution in Industrial Plant Automation: Scenarios and Documentation of the Pick and Place Unit","n":0.258}}},{"i":618,"$":{"0":{"v":"Evolution styles: foundations and models for software architecture evolution","n":0.333},"1":{"v":"As new market opportunities, technologies, platforms, and frameworks become available, systems require large-scale and systematic architectural restructuring to accommodate them. Today's architects have few techniques to help them plan this architecture evolution. In particular, they have little assistance in planning alternative evolution paths, trading off various aspects of the different paths, or knowing best practices for particular domains. In this paper, we describe an approach for planning and reasoning about architecture evolution. Our approach focuses on providing architects with the means to model prospective evolution paths and supporting analysis to select among these candidate paths. To demonstrate the usefulness of our approach, we show how it can be applied to an actual architecture evolution. In addition, we present some theoretical results about our evolution path constraint specification language.","n":0.088}}},{"i":619,"$":{"0":{"v":"To Pay or Not to Pay Technical Debt","n":0.354},"1":{"v":"Ward Cunningham coined the term technical debt as a metaphor for the trade-off between writing clean code at higher cost and delayed de livery, and writing messy code cheap and fast at the cost of higher maintenance efforts once it's shipped. Joshua Kerievsky extended the metaphor to architecture and design. Technical debt is similar to financial debt: it supports quick development at the cost of compound interest to be paid later. The longer we wait to garden our design and code, the larger the amount of interest. Discussions of the metaphor have distinguished different types of technical debt and how and when to best pay them off. Most agree that, sooner or later, technical debt will come due. But is this assumption universally true? If it's better to pay interest, what factors influence the decision to service the debt? And if we decide to retire it, what approach should we take?","n":0.081}}},{"i":620,"$":{"0":{"v":"Usability experiments to evaluate UML/SysML-based Model driven Software Engineering Notations for logic control in Manufacturing Automation","n":0.25},"1":{"v":"Many industrial companies and researchers are looking for more efficient model driven engineering approaches (MDE) in software engineering of manufacturing automation systems (MS) especially for logic control programming, but are uncertain about the applicability and effort needed to implement those approaches in comparison to classical Programmable Logic Controller (PLC) programming with IEC 61131-3. The paper summarizes results of usability experiments evaluating UML and SysML as software engineering notations for a MDE applied in the domain of manufacturing systems. Modeling MS needs to cover the domain specific characteristics, i.e. hybrid process, real time requirements and communication requirements. In addition the paper presents factors, constraint and practical experience for the development of further usability experiments. The paper gives examples of notational expressiveness and weaknesses of UML and SysML. The appendix delivers detailed master models, representing the correct best suited model, and evaluation schemes of the experiment, which is helpful if setting up own empirical experiments.","n":0.082}}},{"i":621,"$":{"0":{"v":"Technical debt in Automated Production Systems","n":0.408},"1":{"v":"The term technical debt borrowed from financial debt describes the long-term negative effects of sub-optimal solutions to achieve short-term benefits. It has been widely studied so far in pure software systems. However, there is a lack of studies on technical debt in technical systems, which contain mechanical, electrical and software parts. Automated Production Systems are such technical systems. In this position paper, we introduce technical debt for Automated Production Systems and give examples from the different disciplines. Based on that description, we outline future research directions on technical debt in this field.","n":0.104}}},{"i":622,"$":{"0":{"v":"Characterizing Implicit Communal Components as Technical Debt in Automotive Software Systems","n":0.302},"1":{"v":"Automotive software systems are often characterized by a set of features that are implemented through a network of communicating components. It is common practice to implement or adapt features by an ad hoc (re) use of signals that originate from components of another feature. Thereby, over time some components become so-called implicit communal components. These components increase the necessary efforts for several development activities because they introduce feature dependencies. Refactoring implicit communal components reduces these efforts but also costs refactoring effort. In this paper, we provide empirical evidence that implicit communal components exist in industrial automotive systems. For two cases, we show that less than 10% of the components are responsible for more than 90% of the feature dependencies. Secondly, we propose a refactoring approach for implicit communal components, which makes them explicit by moving them to a dedicated platform component layer. Finally, we characterize implicit communal components as technical debt, which is a metaphor for suboptimal solutions having short-term benefits but causing a long-term negative impact. With this metaphor, we describe the trade-off between accepting the negative effects of implicit communal components and spending the necessary refactoring costs.","n":0.073}}},{"i":623,"$":{"0":{"v":"Maintenance effort estimation with KAMP4aPS for cross-disciplinary automated PLC-based Production Systems - a collaborative approach","n":0.258},"1":{"v":"Abstract   Automated production systems (aPSs) are often in operation for several decades. Due to a multiplicity of reasons, these assets have to be maintained and modified over the time multiple times and with respect to multiple engineering domains. An increased economic pressure demands to perform these tasks in an optimized way. Therefore, it is necessary to estimate change effects with respect to multidisciplinary interdependences, required surrounding non-functional tasks and the effort and costs included in each step. This paper outlines available cost estimation methods for PLC-based automation and Information Systems (ISs). We introduce Karlsruhe Architectural Maintainability Prediction for aPS (KAMP4aPS), an approach to estimate the necessary maintenance tasks to be performed and their related costs for the domain of aPSs by extending KAMP, which is limited to change propagation analysis on ISs. KAMP requires a metamodel to derive these tasks automatically. Unfortunately, a domain spanning metamodel is missing for aPSs. Hence, we need to develop a part of the metamodel derived from an AutomationML description for the chosen demonstrator at first. Finally, we apply and compare different estimation methods and KAMP4aPS to analyze the exchange of a fieldbus system as exemplary change scenario on a lab size plant to demonstrate the benefits of our discipline-spanning approach.","n":0.07}}},{"i":624,"$":{"0":{"v":"Technical Debt: A Research Roadmap Report on the Eighth Workshop on Managing Technical Debt (MTD 2016)","n":0.25},"1":{"v":"We report here on the Eighth International Workshop on Managing Technical Debt, collocated with the International Conference on Software Maintenance and Evolution (ICSME 2016). The technical debt research community continues to expand through collaborations of industry, tool vendors, and academia. The major themes of discussion this year indicate convergence on a common definition on technical debt and its elements which drive the maturation of a research roadmap, demonstrating that managing technical debt is a mainstream topic in software engineering research bringing empirical analysis, data science, software design and architecture analysis and automation among other challenges together.","n":0.102}}},{"i":625,"$":{"0":{"v":"An investigation of technical debt in automatic production systems","n":0.333},"1":{"v":"Technical Debt is a recent concept, borrowed from the financial domain. It has been recently used in software development to describe technical sub-optimal solutions that have short-term benefits but long-term extra-costs. However, no body of literature investigates how Automatic Production Systems companies deal with Technical Debt. We investigated how Technical Debt is known, how much it hurts and how is managed in an automatic production systems company. Results from one in-depth investigation show that the automatic production systems company spend quite a lot of resources because of Technical Debt, both in the extra-costs (interest) and in its management. The company presents moderate awareness of what Technical Debt is and how much is present in its systems. However, the tracking level is quite low. We, therefore, claim that Technical Debt needs more research in this domain, as it is a source of substantial extracosts and the current practices to manage it are not suitable.","n":0.081}}},{"i":626,"$":{"0":{"v":"Adapting the concept of technical debt to software of automated Production Systems focusing on fault handling, mode of operation and safety aspects","n":0.213},"1":{"v":"Abstract   Technical Debt is a well-known and beneficial concept in software engineering, but almost unknown in the domain of automated Production Systems. There, software is always related to automation as well as mechanical hardware and safety issues need to be especially considered. Therefore, the concepts of TD need adaptation for software in aPS. This paper focusses on safety aspects and related modes of operation as well as fault handling, which were already identified as challenges for software architecture in aPS. Four industrial use cases as well as safety norms and three software guidelines provided from industrial aPS companies are taken as a basis for the development of checklists to avoid TD on the one hand and enhancing TD classifications for aPS software on the other hand. The identified TD aspects were validated using results from a questionnaire including more than 70 German industrial companies from this domain.","n":0.082}}},{"i":627,"$":{"0":{"v":"Feature-oriented development in industrial automation software ecosystems: Development scenarios and tool support","n":0.289},"1":{"v":"Due to increased market demands for highly customized and machine-specific solutions in manufacturing, industrial software systems are often developed as software product lines (SPL) and organized as software ecosystems (SECO) with internal and external developers composing individual solutions based on a common technological platform. In such settings, software development usually occurs in a multistage process: system variants initially derived from a platform are adapted and extended to meet specific requirements. This common approach, however, results in significant challenges for software development and maintenance. In this paper we review key challenges we have been observing when investigating our industrial partner's software ecosystems. We then present a feature-oriented development approach we have been developing to tackle those. Our approach is backed with static analysis methods to deal with system variants and versions created in software maintenance.","n":0.086}}},{"i":628,"$":{"0":{"v":"Technical debt at the crossroads of research and practice","n":0.333},"1":{"v":"Increasingly, software developers and managers use the metaphor of technical debt to communicate key trade-offs related to release and quality issues. We report here on the Fifth International Work...","n":0.186}}},{"i":629,"$":{"0":{"v":"The WEKA data mining software: an update","n":0.378},"1":{"v":"More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.","n":0.097}}},{"i":630,"$":{"0":{"v":"Neural Networks: A Comprehensive Foundation","n":0.447},"1":{"v":"From the Publisher:\r\nThis book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.","n":0.099}}},{"i":631,"$":{"0":{"v":"MOA: Massive Online Analysis","n":0.5},"1":{"v":"Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Naive Bayes classifiers at the leaves. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis, and is released under the GNU GPL license.","n":0.115}}},{"i":632,"$":{"0":{"v":"Mining high-speed data streams","n":0.5},"1":{"v":"Many organizations today have more than very large databases; they have databases that grow without limit at a rate of several million records per day. Mining these continuous data streams brings unique opportunities, but also new challenges. This paper describes and evaluates VFDT, an anytime system that builds decision trees using constant memory and constant time per example. VFDT can incorporate tens of thousands of examples per second using off-the-shelf hardware. It uses Hoeffding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. We study VFDT's properties and demonstrate its utility through an extensive set of experiments on synthetic data. We apply VFDT to mining the continuous stream of Web access data from the whole University of Washington main campus.","n":0.089}}},{"i":633,"$":{"0":{"v":"Self-adaptive software: Landscape and research challenges","n":0.408},"1":{"v":"Software systems dealing with distributed applications in changing environments normally require human supervision to continue operation in all conditions. These (re-)configuring, troubleshooting, and in general maintenance tasks lead to costly and time-consuming procedures during the operating phase. These problems are primarily due to the open-loop structure often followed in software development. Therefore, there is a high demand for management complexity reduction, management automation, robustness, and achieving all of the desired quality requirements within a reasonable cost and time range during operation. Self-adaptive software is a response to these demands; it is a closed-loop system with a feedback loop aiming to adjust itself to changes during its operation. These changes may stem from the software system's self (internal causes, e.g., failure) or context (external events, e.g., increasing requests from users). Such a system is required to monitor itself and its context, detect significant changes, decide how to react, and act to execute such decisions. These processes depend on adaptation properties (called self-* properties), domain characteristics (context information or models), and preferences of stakeholders. Noting these requirements, it is widely believed that new models and frameworks are needed to design self-adaptive software. This survey article presents a taxonomy, based on concerns of adaptation, that is, how, what, when and where, towards providing a unified view of this emerging area. Moreover, as adaptive systems are encountered in many disciplines, it is imperative to learn from the theories and models developed in these other areas. This survey article presents a landscape of research in self-adaptive software by highlighting relevant disciplines and some prominent research projects. This landscape helps to identify the underlying research gaps and elaborates on the corresponding challenges.","n":0.06}}},{"i":634,"$":{"0":{"v":"Estimating continuous distributions in Bayesian classifiers","n":0.408},"1":{"v":"When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models.","n":0.089}}},{"i":635,"$":{"0":{"v":"Software Engineering for Self-Adaptive Systems : A Second Research Roadmap","n":0.316},"1":{"v":"The goal of this roadmap paper is to summarize the state-of-the-art and identify research challenges when developing, deploying and managing self-adaptive software systems. Instead of dealing with a wide range of topics associated with the field, we focus on four essential topics of self-adaptation: design space for self-adaptive solutions, software engineering processes for self-adaptive systems, from centralized to decentralized control, and practical run-time verification & validation for self-adaptive systems. For each topic, we present an overview, suggest future directions, and focus on selected challenges. This paper complements and extends a previous roadmap on software engineering for self-adaptive systems published in 2009 covering a different set of topics, and reflecting in part on the previous paper. This roadmap is one of the many results of the Dagstuhl Seminar 10431 on Software Engineering for Self-Adaptive Systems, which took place in October 2010.","n":0.085}}},{"i":636,"$":{"0":{"v":"DDD: A New Ensemble Approach for Dealing with Concept Drift","n":0.316},"1":{"v":"Online learning algorithms often have to operate in the presence of concept drifts. A recent study revealed that different diversity levels in an ensemble of learning machines are required in order to maintain high generalization on both old and new concepts. Inspired by this study and based on a further study of diversity with different strategies to deal with drifts, we propose a new online ensemble learning approach called Diversity for Dealing with Drifts (DDD). DDD maintains ensembles with different diversity levels and is able to attain better accuracy than other approaches. Furthermore, it is very robust, outperforming other drift handling approaches in terms of accuracy when there are false positive drift detections. In all the experimental comparisons we have carried out, DDD always performed at least as well as other drift handling approaches under various conditions, with very few exceptions.","n":0.084}}},{"i":637,"$":{"0":{"v":"Systematic review: A systematic review of effect size in software engineering experiments","n":0.289},"1":{"v":"An effect size quantifies the effects of an experimental treatment. Conclusions drawn from hypothesis testing results might be erroneous if effect sizes are not judged in addition to statistical significance. This paper reports a systematic review of 92 controlled experiments published in 12 major software engineering journals and conference proceedings in the decade 1993-2002. The review investigates the practice of effect size reporting, summarizes standardized effect sizes detected in the experiments, discusses the results and gives advice for improvements. Standardized and/or unstandardized effect sizes were reported in 29% of the experiments. Interpretations of the effect sizes in terms of practical importance were not discussed beyond references to standard conventions. The standardized effect sizes computed from the reviewed experiments were equal to observations in psychology studies and slightly larger than standard conventions in behavioral science.","n":0.086}}},{"i":638,"$":{"0":{"v":"An Adaptive and Intelligent SLA Negotiation System for Web Services","n":0.316},"1":{"v":"The effective use of services to compose business processes in services computing demands that the Quality of Services (QoS) meet consumers' expectations. Automated web-based negotiation of Service Level Agreements (SLA) can help define the QoS requirements of critical service-based processes. We propose a novel trusted Negotiation Broker (NB) framework that performs adaptive and intelligent bilateral bargaining of SLAs between a service provider and a service consumer based on each party's high-level business requirements. We define mathematical models to map business-level requirements to low-level parameters of the decision function, which obscures the complexity of the system from the parties. We also define an algorithm for adapting the decision functions during an ongoing negotiation to comply with an opponent's offers or with updated consumer preferences. The NB uses intelligent agents to conduct the negotiation locally by selecting the most appropriate time-based decision functions. The negotiation outcomes are validated by extensive experimental study for Exponential, Polynomial, and Sigmoid time-based decision functions using simulations on our prototype framework. Results are compared in terms of a total utility value of the negotiating parties to demonstrate the efficiency of our proposed approach.","n":0.073}}},{"i":639,"$":{"0":{"v":"A Learning-Based Framework for Engineering Feature-Oriented Self-Adaptive Software Systems","n":0.333},"1":{"v":"Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation.","n":0.071}}},{"i":640,"$":{"0":{"v":"Batch-incremental versus instance-incremental learning in dynamic and evolving data","n":0.333},"1":{"v":"Many real world problems involve the challenging context of data streams, where classifiers must be incremental: able to learn from a theoretically-infinite stream of examples using limited time and memory, while being able to predict at any point. Two approaches dominate the literature: batch-incremental methods that gather examples in batches to train models; and instance-incremental methods that learn from each example as it arrives. Typically, papers in the literature choose one of these approaches, but provide insufficient evidence or references to justify their choice. We provide a first in-depth analysis comparing both approaches, including how they adapt to concept drift, and an extensive empirical study to compare several different versions of each approach. Our results reveal the respective advantages and disadvantages of the methods, which we discuss in detail.","n":0.088}}},{"i":641,"$":{"0":{"v":"Dynamic decision networks for decision-making in self-adaptive systems: a case study","n":0.302},"1":{"v":"Bayesian decision theory is increasingly applied to support decision-making processes under environmental variability and uncertainty. Researchers from application areas like psychology and biomedicine have applied these techniques successfully. However, in the area of software engineering and specifically in the area of self-adaptive systems (SASs), little progress has been made in the application of Bayesian decision theory. We believe that techniques based on Bayesian Networks (BNs) are useful for systems that dynamically adapt themselves at runtime to a changing environment, which is usually uncertain. In this paper, we discuss the case for the use of BNs, specifically Dynamic Decision Networks (DDNs), to support the decision-making of self-adaptive systems. We present how such a probabilistic model can be used to support the decision-making in SASs and justify its applicability. We have applied our DDN-based approach to the case of an adaptive remote data mirroring system. We discuss results, implications and potential benefits of the DDN to enhance the development and operation of self-adaptive systems, by providing mechanisms to cope with uncertainty and automatically make the best decision.","n":0.076}}},{"i":642,"$":{"0":{"v":"PowerAPI: A Software Library to Monitor the Energy Consumed at the Process-Level","n":0.289},"1":{"v":"Energy consumption by information and communication technologies (ICT) has been growing rapidly over recent years. Comparable to the civil aviation domain, the research community now considers ICT energy consumption as a major concern. Several studies report that energy consumption is an issue during all steps of a computer's life, from hardware assemblage, to usage, and dismantling. Research in the area of Green IT has proposed various approaches to save energy at the hardware and software levels. In the context of software, this challenge requires identification of new development methodologies that can help reduce the energy footprint. To tackle this challenge, we propose PowerAPI, a tool to quantify this energy consumption, by providing an application programming interface (API) that monitors, in real-time, the energy consumed at the granularity of a system process.","n":0.087}}},{"i":643,"$":{"0":{"v":"Service-Level Agreements for Electronic Services","n":0.447},"1":{"v":"The potential of communication networks and middleware to enable the composition of services across organizational boundaries remains incompletely realized. In this paper, we argue that this is in part due to outsourcing risks and describe the possible contribution of Service-Level Agreements (SLAs) to mitigating these risks. For SLAs to be effective, it should be difficult to disregard their original provisions in the event of a dispute between the parties. Properties of understandability, precision, and monitorability ensure that the original intent of an SLA can be recovered and compared to trustworthy accounts of service behavior to resolve disputes fairly and without ambiguity. We describe the design and evaluation of a domain-specific language for SLAs that tend to exhibit these properties and discuss the impact of monitorability requirements on service-provision practices.","n":0.088}}},{"i":644,"$":{"0":{"v":"Self-Adaptive Trade-off Decision Making for Autoscaling Cloud-Based Services","n":0.354},"1":{"v":"Elasticity in the cloud is often achieved by on-demand autoscaling. In such context, the goal is to optimize the Quality of Service (QoS) and cost objectives for the cloud-based services. However, the difficulty lies in the facts that these objectives, e.g., throughput and cost, can be naturally conflicted; and the QoS of cloud-based services often interfere due to the shared infrastructure in cloud. Consequently, dynamic and effective trade-off decision making of autoscaling in the cloud is necessary, yet challenging. In particular, it is even harder to achieve well-compromised trade-offs, where the decision largely improves the majority of the objectives; while causing relatively small degradations to others. In this paper, we present a self-adaptive decision making approach for autoscaling in the cloud. It is capable to adaptively produce autoscaling decisions that lead to well-compromised trade-offs without heavy human intervention. We leverage on ant colony inspired multi-objective optimization for searching and optimizing the trade-offs decisions, the result is then filtered by compromise-dominance, a mechanism that extracts the decisions with balanced improvements in the trade-offs. We experimentally compare our approach to four state-of-the-arts autoscaling approaches: rule, heuristic, randomized and multi-objective genetic algorithm based solutions. The results reveal the effectiveness of our approach over the others, including better quality of trade-offs and significantly smaller violation of the requirements.","n":0.068}}},{"i":645,"$":{"0":{"v":"Plato: a genetic algorithm approach to run-time reconfiguration in autonomic computing systems","n":0.289},"1":{"v":"Increasingly, applications need to be able to self-reconfigure in response to changing requirements and environmental conditions. Autonomic computing has been proposed as a means for automating software maintenance tasks. As the complexity of adaptive and autonomic systems grows, designing and managing the set of reconfiguration rules becomes increasingly challenging and may produce inconsistencies. This paper proposes an approach to leverage genetic algorithms in the decision-making process of an autonomic system. This approach enables a system to dynamically evolve target reconfigurations at run time that balance tradeoffs between functional and non-functional requirements in response to changing requirements and environmental conditions. A key feature of this approach is incorporating system and environmental monitoring information into the genetic algorithm such that specific changes in the environment automatically drive the evolutionary process towards new viable solutions. We have applied this genetic-algorithm based approach to the dynamic reconfiguration of a collection of remote data mirrors, demonstrating an effective decision-making method for diffusing data and minimizing operational costs while maximizing data reliability and network performance, even in the presence of link failures.","n":0.075}}},{"i":646,"$":{"0":{"v":"Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications","n":0.277},"1":{"v":"Mobile applications require to self-adapt their behavior to context changes.We propose a DSPL approach to manage variability at runtime.Configurations are generated using multiobjective evolutionary algorithms.We apply a fix operator to generate only valid configurations at runtime.We demonstrate that this approach is suitable for mobile environments. Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system's execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments.","n":0.064}}},{"i":647,"$":{"0":{"v":"FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software","n":0.333},"1":{"v":"Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.","n":0.065}}},{"i":648,"$":{"0":{"v":"Model predictive control for software systems with CobRA","n":0.354},"1":{"v":"Self-adaptive software systems monitor their operation and adapt when their requirements fail due to unexpected phenomena in their environment. This paper examines the case where the environment changes dynamically over time and the chosen adaptation has to take into account such changes. In control theory, this type of adaptation is known as Model Predictive Control and comes with a well-developed theory and myriads of successful applications. The paper focuses on modelling the dynamic relationship between requirements and possible adaptations. It then proposes a controller that exploits this relationship to optimize the satisfaction of requirements relative to a cost-function. This is accomplished through a model-based framework for designing self- adaptive software systems that can guarantee a certain level of requirements satisfaction over time, by dynamically composing adaptation strategies when necessary. The proposed framework is illustrated and evaluated through a simulation of the Meeting-Scheduling System exemplar.","n":0.083}}},{"i":649,"$":{"0":{"v":"Keep it SIMPLEX: satisfying multiple goals with guarantees in control-based self-adaptive systems","n":0.289},"1":{"v":"An increasingly important concern of software engineers is handling uncertainties at design time, such as environment dynamics that may be difficult to predict or requirements that may change during operation. The idea of self-adaptation is to handle such uncertainties at runtime, when the knowledge becomes available. As more systems with strict requirements require self-adaptation, providing guarantees for adaptation has become a high-priority. Providing such guarantees with traditional architecture-based approaches has shown to be challenging. In response, researchers have studied the application of control theory to realize self-adaptation. However, existing control-theoretic approaches applied to adapt software systems have primarily focused on satisfying only a single adaptation goal at a time, which is often too restrictive for real applications. In this paper, we present Simplex Control Adaptation, SimCA, a new approach to self-adaptation that satisfies multiple goals, while being optimal with respect to an additional goal. SimCA offers robustness to measurement inaccuracy and environmental disturbances, and provides guarantees. We evaluate SimCA for two systems with strict requirements that have to deal with uncertainties: an underwater vehicle system used for oceanic surveillance, and a tele-assistance system for health care support.","n":0.073}}},{"i":650,"$":{"0":{"v":"Improving Architecture-Based Self-Adaptation through Resource Prediction","n":0.408},"1":{"v":"An increasingly important concern for modern systems design is how best to incorporate self-adaptation into systems so as to improve their ability to dynamically respond to faults, resource variation, and changing user needs. One promising approach is to use architectural models as a basis for monitoring, problem detection, and repair selection. While this approach has been shown to yield positive results, current systems use a reactive approach: they respond to problems only when they occur. In this paper we argue that self-adaptation can be improved by adopting an anticipatory approach in which predictions are used to inform adaptation strategies. We show how such an approach can be incorporated into an architecture-based adaptation framework and demonstrate the benefits of the approach.","n":0.091}}},{"i":651,"$":{"0":{"v":"A Two-Phase Online Prediction Approach for Accurate and Timely Adaptation Decision","n":0.302},"1":{"v":"A Service-Based Application (SBA) is built by defining a workflow that composes and coordinates different Web services available via the Internet. In the context of on-demand SBA execution, suitable services are selected and integrated at runtime to meet different non-functional requirements (such as price and execution time). In such dynamic and distributed environment, an important issue is to guarantee the end-to-end Quality of Service (QoS). As a consequence, SBA provider is required to monitor each running SBA instance, analyze its runtime execution states, then identify proper adaptation plans if necessary, and finally apply the relative countermeasures. One of the main challenges is to accurately trigger the adaptation process as early as possible. In this paper, we present a two-phase decision approach that can accurately analyze the adaptation needs for on-demand SBA execution model. Our approach is based on the online prediction techniques: an adaptation decision is determined by predicting an upcoming end-to-end QoS degradation through two-phase evaluations. Firstly, the end-to-end QoS is estimated at runtime based on monitoring techniques; if a QoS degradation is tent to happen, in the second phase, both static and adaptive strategies are introduced to assess whether it is the best timing to draw the final adaptation decision. Our approach is evaluated and validated by a series of realistic simulations.","n":0.068}}},{"i":652,"$":{"0":{"v":"Automatically hardening a self-adaptive system against uncertainty","n":0.378},"1":{"v":"A self-adaptive system (SAS) can reconfigure to adapt to potentially adverse conditions that can manifest in the environment at run time. However, the SAS may not have been explicitly developed with such conditions in mind, thereby requiring additional configuration states or updates to the requirements specification for the SAS to provide assurance that it continually satisfies its requirements and delivers acceptable behavior. By discovering both adverse environmental conditions and the SAS configuration states that can mitigate those conditions at design time, an SAS can be hardened against uncertainty prior to deployment, effectively extending its lifetime. This paper introduces two search-based techniques, Ragnarok and Valkyrie, for hardening an SAS against uncertainty. Ragnarok automatically discovers adverse conditions that negatively impact an SAS by searching for environmental conditions that explicitly cause requirements violations. Valkyrie then searches for SAS configurations that improve requirements satisficement throughout execution in response to discovered adverse environmental conditions. Together, these techniques can be used to improve the design and implementation of an SAS. We apply each technique to an industry-provided remote data mirroring application that can self-reconfigure in response to unknown or adverse conditions, such as network message delays, network link failures, and sensor noise.","n":0.071}}},{"i":653,"$":{"0":{"v":"Solving the next adaptation problem with prometheus","n":0.378},"1":{"v":"Dealing with multiple requirement failures is an essential capability for self-adaptive software systems. This capability becomes more challenging in the presence of conflicting goals. This paper is concerned with the next adaptation problem: the problem of finding the best next adaptation in the presence of multiple failures. ‘Best’ here means that the adaptation chosen optimizes a given set of objective functions, such as the cost of adaptation or the degree of failure for system requirements. The paper proposes a formal framework for defining the next adaptation problem, assuming that we can specify quantitatively the constraints that hold between indicators that measure the degree of failure of each requirement and control parameters. These constraints, along with one or several objective functions, are translated into a constrained multi-objective optimization problem that can be solved by using an OMT/SMT (Optimization Modulo Theories/Satisfiability Modulo Theories) solver, such as OptiMathSAT. The proposed framework is illustrated with the Meeting Scheduler exemplar and a second, e-shop case study.","n":0.079}}},{"i":654,"$":{"0":{"v":"Statistical detection of QoS violations based on CUSUM control charts","n":0.316},"1":{"v":"Currently software systems operate in highly dynamic contexts, and consequently they have to adapt their behavior in response to changes in their contexts or/and requirements. Existing approaches trigger adaptations after detecting violations in quality of service (QoS) requirements by just comparing observed QoS values to predefined thresholds without any statistical confidence or certainty. These threshold-based adaptation approaches may perform unnecessary adaptations, which can lead to severe shortcomings such as follow-up failures or increased costs. In this paper we introduce a statistical approach based on CUSUM control charts called AuDeQAV - Automated Detection of QoS Attributes Violations. This approach estimates at runtime a current status of the running system, and monitors its QoS attributes and provides early detection of violations in its requirements with a defined level of confidence. This enables timely intervention preventing undesired consequences from the violation or from inappropriate remediation. We validated our approach using a series of experiments and response time datasets from real-world web services.","n":0.079}}},{"i":655,"$":{"0":{"v":"Considering Transient Effects of Self-Adaptations in Model-Driven Performance Analyses","n":0.333},"1":{"v":"Model-driven performance engineering allows software architects to reason on performance characteristics of a software system in early design phases. In recent years, model-driven analysis techniques have been developed to evaluate performance characteristics of self-adaptive software systems. These techniques aim to reason on the ability of a self-adaptive software system to fulfill performance requirements in transient phases. A transient phase is the interval in which the behavior of the system changes, e.g., due to a burst in user requests. However, the effectiveness and efficiency with which a system is able to adapt depends not only on the time when it triggers adaptation actions but also on the time at which they are completed. Executing an adaptation action can cause additional stress on the adapted system. This can further impede the performance of the system in the transient phase. Model-driven analyses of self-adaptive software do not consider these transient effects. This paper outlines an approach for evaluating transient effects in model-driven analyses of self-adaptive software systems. The evaluation applied our approach to a horizontally scaling media hosting application in three experiments. By considering the delay in booting new Virtual Machines (VMs), we were able to improve the accuracy of predicted response times. The second and third experiment demonstrated that the increased accuracy enables an early detection and resolution of design deficiencies of self-adaptive software systems.","n":0.067}}},{"i":656,"$":{"0":{"v":"Case Study Research: Design and Methods","n":0.408},"1":{"v":"Foreword, by Donald T. Campbell Preface 1. INTRODUCTION: How to Know Whether and When to Use Case Studies as a Research Method The Case Study as a Research Method Comparing Case Studies With Other Research Methods in the Social Sciences Different Kinds of Case Studies, But a Common Definition Summary 2. DESIGNING CASE STUDIES: Identifying Your Case(s) and Establishing the Logic of Your Case Study General Approach to Designing Case Studies Criteria for Judging the Quality of Research Designs Case Study Designs Modest Advice in Selecting Case Study Designs 3. PREPARING TO COLLECT CASE STUDY EVIDENCE: What You Need to Do Before Starting to Collect Case Study Data The Case Study Investigator: Desired Skills Preparation and Training for a Specific Case Study The Case Study Protocol Screening the Candidate \"Cases\" for Your Case Study The Pilot Case Study Summary 4. COLLECTING CASE STUDY EVIDENCE: The Principles You Should Follow in Working With Six Sources of Evidence Six Sources of Evidence Three Principles of Data Collection Summary 5. ANALYZING CASE STUDY EVIDENCE: How to Start Your Analysis, Your Analytic Choices, and How They Work An Analytic Strategy: More Than Familiarity With Analytic Tools Five Analytic Techniques Pressing for a High-Quality Analysis Summary 6. REPORTING CASE STUDIES: How and What to Compose Targeting Case Study Reports Case Study Reprots as Part of Larger, Mixed Methods Studies Illustrative Structures for Case Study Compositions Procedures in Doing a Case Study Report What Makes an Examplary Case Study? References Author Index Subject Index About the Author","n":0.063}}},{"i":657,"$":{"0":{"v":"SMOTE: synthetic minority over-sampling technique","n":0.447},"1":{"v":"An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \"normal\" examples with only a small percentage of \"abnormal\" or \"interesting\" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy.","n":0.068}}},{"i":658,"$":{"0":{"v":"Statistical Power Analysis for the Behavioral Sciences (2nd ed.)","n":0.333}}},{"i":659,"$":{"0":{"v":"Evaluation: from Precision, Recall and F-measure to ROC, Informedness, Markedness and Correlation","n":0.289},"1":{"v":"Commonly used evaluation measures including Recall, Precision, F-Factor and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case. .","n":0.087}}},{"i":660,"$":{"0":{"v":"DECOR: A Method for the Specification and Detection of Code and Design Smells","n":0.277},"1":{"v":"Code and design smells are poor solutions to recurring implementation and design problems. They may hinder the evolution of a system by making it hard for software engineers to carry out changes. We propose three contributions to the research field related to code and design smells: (1) DECOR, a method that embodies and defines all the steps necessary for the specification and detection of code and design smells, (2) DETEX, a detection technique that instantiates this method, and (3) an empirical validation in terms of precision and recall of DETEX. The originality of DETEX stems from the ability for software engineers to specify smells at a high level of abstraction using a consistent vocabulary and domain-specific language for automatically generating detection algorithms. Using DETEX, we specify four well-known design smells: the antipatterns Blob, Functional Decomposition, Spaghetti Code, and Swiss Army Knife, and their 15 underlying code smells, and we automatically generate their detection algorithms. We apply and validate the detection algorithms in terms of precision and recall on XERCES v2.7.0, and discuss the precision of these algorithms on 11 open-source systems.","n":0.074}}},{"i":661,"$":{"0":{"v":"Understanding variable importances in forests of randomized trees","n":0.354},"1":{"v":"Despite growing interest and practical use in various scientific areas, variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view. In this work we characterize the Mean Decrease Impurity (MDI) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions. We derive a three-level decomposition of the information jointly provided by all input variables about the output in terms of i) the MDI importance of each input variable, ii) the degree of interaction of a given input variable with the other input variables, iii) the different interaction terms of a given degree. We then show that this MDI importance of a variable is equal to zero if and only if the variable is irrelevant and that the MDI importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables. We illustrate these properties on a simple example and discuss how they may change in the case of non-totally randomized trees such as Random Forests and Extra-Trees.","n":0.075}}},{"i":662,"$":{"0":{"v":"Learning a Metric for Code Readability","n":0.408},"1":{"v":"In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from 120 human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80 percent effective and better than a human, on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with three measures of software quality: code changes, automated defect reports, and defect log messages. We measure these correlations on over 2.2 million lines of code, as well as longitudinally, over many releases of selected projects. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggest that comments, in and of themselves, are less important than simple blank lines to local judgments of readability.","n":0.081}}},{"i":663,"$":{"0":{"v":"Evaluating static analysis defect warnings on production software","n":0.354},"1":{"v":"Static analysis tools for software defect detection are becoming widely used in practice. However, there is little public information regarding the experimental evaluation of the accuracy and value of the warnings these tools report. In this paper, we discuss the warnings found by FindBugs, a static analysis tool that finds defects in Java programs. We discuss the kinds of warnings generated and the classification of warnings into false positives, trivial bugs and serious bugs. We also provide some insight into why static analysis tools often detect true but trivial bugs, and some information about defect warnings across the development lifetime of software release. We report data on the defect warnings in Sun's Java 6 JRE, in Sun's Glassfish JEE server, and in portions of Google's Java codebase. Finally, we report on some experiences from incorporating static analysis into the software development process at Google.","n":0.083}}},{"i":664,"$":{"0":{"v":"Comparing and experimenting machine learning techniques for code smell detection","n":0.316},"1":{"v":"Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy (>96 %), and only a hundred training examples are needed to reach at least 95 % accuracy.","n":0.076}}},{"i":665,"$":{"0":{"v":"Analyzing the State of Static Analysis: A Large-Scale Evaluation in Open Source Software","n":0.277},"1":{"v":"The use of automatic static analysis has been a software engineering best practice for decades. However, we still do not know a lot about its use in real-world software projects: How prevalent is the use of Automated Static Analysis Tools (ASATs) such as FindBugs and JSHint? How do developers use these tools, and how does their use evolve over time? We research these questions in two studies on nine different ASATs for Java, JavaScript, Ruby, and Python with a population of 122 and 168,214 open-source projects. To compare warnings across the ASATs, we introduce the General Defect Classification (GDC) and provide a grounded-theory-derived mapping of 1,825 ASAT-specific warnings to 16 top-level GDC classes. Our results show that ASAT use is widespread, but not ubiquitous, and that projects typically do not enforce a strict policy on ASAT use. Most ASAT configurations deviate slightly from the default, but hardly any introduce new custom analyses. Only a very small set of default ASAT analyses is widely changed. Finally, most ASAT configurations, once introduced, never change. If they do, the changes are small and have a tendency to occur within one day of the configuration's initial introduction.","n":0.072}}},{"i":666,"$":{"0":{"v":"Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt","n":0.316},"1":{"v":"The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-admitted technical debt), and that the most common types of self-admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-admitted technical debt, significantly outperforming the current state-of-the-art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset.","n":0.063}}},{"i":667,"$":{"0":{"v":"A Bayesian Approach for the Detection of Code and Design Smells","n":0.302},"1":{"v":"The presence of code and design smells can have a severe impact on the quality of a program. Consequently, their detection and correction have drawn the attention of both researchers and practitioners who have proposed various approaches to detect code and design smells in programs. However, none of these approaches handle the inherent uncertainty of the detection process. We propose a Bayesian approach to manage this uncertainty. First, we present a systematic process to convert existing state-of-the-art detection rules into a probabilistic model. We illustrate this process by generating a model to detect occurrences of the Blob antipattern. Second, we present results of the validation of the model: we built this model on two open-source programs, GanttProject v1.10.2 and Xerces v2.7.0, and measured its accuracy. Third, we compare our model with another approach to show that it returns the same candidate classes while ordering them to minimise the quality analysts' effort. Finally, we show that when past detection results are available, our model can be calibrated using machine learning techniques to offer an improved, context-specific detection.","n":0.075}}},{"i":668,"$":{"0":{"v":"Product metrics for automatic identification of \"bad smell\" design problems in Java source-code","n":0.277},"1":{"v":"Refactoring can have a direct influence on reducing the cost of software maintenance through changing the internal structure of the source-code to improve the overall design that helps the present and future programmers evolve and understand a system. Bad smells are a set of design problems with refactoring identified as a solution. Locating these bad smells has been described as more a human intuition than an exact science. This paper addresses the issue of identifying the characteristics of a bad smell through the use of a set of software metrics. Then by using a pre-defined set of interpretation rules to interpret the software metric results applied to Java source-code, the software engineer can be provided with significant guidance as to the location of bad smells. These issues are addressed in a number of ways. Firstly, a precise definition of bad smells is given from the informal descriptions given by the originators Fowler and Beck. The characteristics of the bad smells have been used to define a set of measurements and interpretation rules for a subset of the bad smells. A prototype tool has been implemented to enable the evaluation of the interpretation rules in two case studies.","n":0.071}}},{"i":669,"$":{"0":{"v":"Comparison of the predicted and observed secondary structure of T4 phage lysozyme.","n":0.289},"1":{"v":"Predictions of the secondary structure of T4 phage lysozyme, made by a number of investigators on the basis of the amino acid sequence, are compared with the structure of the protein determined experimentally by X-ray crystallography. Within the amino terminal half of the molecule the locations of helices predicted by a number of methods agree moderately well with the observed structure, however within the carboxyl half of the molecule the overall agreement is poor. For eleven different helix predictions, the coefficients giving the correlation between prediction and observation range from 0.14 to 0.42. The accuracy of the predictions for both beta-sheet regions and for turns are generally lower than for the helices, and in a number of instances the agreement between prediction and observation is no better than would be expected for a random selection of residues. The structural predictions for T4 phage lysozyme are much less successful than was the case for adenylate kinase (Schulz et al. (1974) Nature 250, 140-142). No one method of prediction is clearly superior to all others, and although empirical predictions based on larger numbers of known protein structure tend to be more accurate than those based on a limited sample, the improvement in accuracy is not dramatic, suggesting that the accuracy of current empirical predictive methods will not be substantially increased simply by the inclusion of more data from additional protein structure determinations.","n":0.066}}},{"i":670,"$":{"0":{"v":"Detecting defects in object-oriented designs: using reading techniques to increase software quality","n":0.289},"1":{"v":"Inspections can be used to identify defects in software artifacts. In this way, inspection methods help to improve software quality, especially when used early in software development. Inspections of software design may be especially crucial since design defects (problems of correctness and completeness with respect to the requirements, internal consistency, or other quality attributes) can directly affect the quality of, and effort required for, the implementation.  We have created a set of “reading techniques” (so called because they help a reviewer to “read” a design artifact for the purpose of finding relevant information) that gives specific and practical guidance for identifying defects in Object-Oriented designs. Each reading technique in the family focuses the reviewer on some aspect of the design, with the goal that an inspection team applying the entire family should achieve a high degree of coverage of the design defects.  In this paper, we present an overview of this new set of reading techniques. We discuss how some elements of these techniques are based on empirical results concerning an analogous set of reading techniques that supports defect detection in requirements documents. We present an initial empirical study that was run to assess the feasibility of these new techniques, and discuss the changes made to the latest version of the techniques based on the results of this study.","n":0.068}}},{"i":671,"$":{"0":{"v":"JDeodorant: identification and application of extract class refactorings","n":0.354},"1":{"v":"Evolutionary changes in object-oriented systems can result in large, complex classes, known as \"God Classes\". In this paper, we present a tool, developed as part of the JDeodorant Eclipse plugin, that can recognize opportunities for extracting cohesive classes from \"God Classes\" and automatically apply the refactoring chosen by the developer.","n":0.141}}},{"i":672,"$":{"0":{"v":"An XML-based lightweight C++ fact extractor","n":0.408},"1":{"v":"A lightweight fact extractor is presented that utilizes XML tools, such as XPath and XSLT to extract static information from C++ source code programs. The source code is first converted into an XML representation, srcML, to facilitate the use of a wide variety of XML tools. The method is deemed lightweight because only a partial parsing of the source is done. Additionally, the technique is quite robust and can be applied to incomplete and noncompilable source code. The trade off to this approach is that queries on some low level details cannot be directly addressed. This approach is applied to a fact extractor benchmark as comparison with other, heavier weight, fact extractors. Fact extractors are widely used to support understanding tasks associated with maintenance, reverse engineering and various other software engineering tasks.","n":0.087}}},{"i":673,"$":{"0":{"v":"The Effectiveness of Automated Static Analysis Tools for Fault Detection and Refactoring Prediction","n":0.277},"1":{"v":"Many automated static analysis (ASA) tools have been developed in recent years for detecting software anomalies. The aim of these tools is to help developers to eliminate software defects at early stages and produce more reliable software at a lower cost. Determining the effectiveness of ASA tools requires empirical evaluation. This study evaluates coding concerns reported by three ASA tools on two open source software (OSS) projects with respect to two types of modifications performed in the studied software CVS repositories: corrections of faults that caused failures, and refactoring modifications. The results show that fewer than 3% of the detected faults correspond to the coding concerns reported by the ASA tools. ASA tools were more effective in identifying refactoring modifications and corresponded to about 71% of them. More than 96% of the coding concerns were false positives that do not relate to any fault or refactoring modification.","n":0.082}}},{"i":674,"$":{"0":{"v":"Static correspondence and correlation between field defects and warnings reported by a bug finding tool","n":0.258},"1":{"v":"Despite the interest and the increasing number of static analysis tools for detecting defects in software systems, there is still no consensus on the actual gains that such tools introduce in software development projects. Therefore, this article reports a study carried out to evaluate the degree of correspondence and correlation between post-release defects (i.e., field defects) and warnings issued by FindBugs, a bug finding tool widely used in Java systems. The study aimed to evaluate two types of relations: static correspondence (when warnings contribute to find the static program locations changed to remove field defects) and statistical correlation (when warnings serve as early indicators for future field defects). As a result, we have concluded that there is no static correspondence between field defects and warnings. However, statistical tests showed that there is a moderate level of correlation between warnings and such kinds of software defects.","n":0.083}}},{"i":675,"$":{"0":{"v":"Support vector machines for anti-pattern detection","n":0.408},"1":{"v":"Developers may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and--or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns in a whole software system may be infeasible because of the required parsing time and of the subsequent needed manual validation. Detecting anti-patterns on subsets of a system could reduce costs, effort, and resources. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently some limitations: they require extensive knowledge of anti-patterns, they have limited precision and recall, and they cannot be applied on subsets of systems. To overcome these limitations, we introduce SVMDetect, a novel approach to detect anti-patterns, based on a machine learning technique---support vector machines. Indeed, through an empirical study involving three subject systems and four anti-patterns, we showed that the accuracy of SVMDetect is greater than of DETEX when detecting anti-patterns occurrences on a set of classes. Concerning, the whole system, SVMDetect is able to find more anti-patterns occurrences than DETEX.","n":0.076}}},{"i":676,"$":{"0":{"v":"Understanding database schema evolution","n":0.5},"1":{"v":"Database reverse engineering (DRE) has traditionally been carried out by considering three main information sources: (1) the database schema, (2) the stored data, and (3) the application programs. Not all of these information sources are always available, or of sufficient quality to inform the DRE process. For example, getting access to real-world data is often extremely problematic for information systems that maintain private data. In recent years, the analysis of the evolution history of software programs have gained an increasing role in reverse engineering in general, but comparatively little such research has been carried out in the context of database reverse engineering. The goal of this paper is to contribute to narrowing this gap and exploring the use of the database evolution history as an additional information source to aid database schema reverse engineering. We present a tool-supported method for analyzing the evolution history of legacy databases, and we report on a large-scale case study of reverse engineering a complex information system and curate it as a benchmark for future research efforts within the community. We present a tool-supported method to analyze the history of a database schema.The method makes use of mining software repositories (MSR) techniques.We report on the application of the method to a large-scale case study.","n":0.069}}},{"i":677,"$":{"0":{"v":"Establishing Referential Integrity in Legacy Information Systems - Reality Bites!","n":0.316},"1":{"v":"Most modern relational DBMS have the ability to monitor and enforce referential integrity constraints (RICs). In contrast to new applications, however, heavily evolved legacy information systems may not make use of this important feature, if their design predates its availability. The detection of RICs in legacy systems has been a long-term research topic in the DB reengineering community and a variety of different methods have been proposed, analyzing schema, application code and data. However, empirical evidence on their application for reengineering large-scale industrial systems is scarce and all too often \"problems\" (case studies) are carefully selected to fit a particular \"solution\" (method), rather than the other way around. This paper takes a different approach. We analyze in detail the issues posed in reengineering a complex, mission-critical information system to support RICs. In our analysis, we find that many of the assumptions typically made in DB reengineering methods do not readily apply. Based on our findings, we design a process and tools for detecting RICs in context of our real-world problem and provide preliminary results on their effectiveness.","n":0.075}}},{"i":678,"$":{"0":{"v":"Discovering the Objectual Meaning of Foreign Key Constraints in Enterprise Applications","n":0.302},"1":{"v":"The software industry is increasingly confronted with the issues of understanding and maintaining a special type of object-oriented systems, namely enterprise applications. A specific concern for these applications is to understand the persistent data (usually stored in a RDBMS), and its manipulation in the object-oriented source code. While foreign keys are an important means for indicating relations within the persistent data, oftentimes, by looking solely at the database schema, it is impossible to determine the exact nature of these relations. This paper proposes a novel approach for determining a refined understanding of the relations among the persistent data, by correlating the information about foreign keys extracted from the database schema with the way the data are used in the source code. By analyzing two enterprise systems we found that the proposed approach helps specifying a significant number of foreign key constraints in terms of their objectual meaning (e.g., if they denote an inheritance or an aggregation relation). Thus, the approach contributes to enhancing, in an automated manner, the understanding of a system's database schema by tying it to the source code that uses it.","n":0.074}}},{"i":679,"$":{"0":{"v":"2010 CWE/SANS Top 25 Most Dangerous Software Errors","n":0.354}}},{"i":680,"$":{"0":{"v":"CQML Scheme: A Classification Scheme for Comprehensive Quality Model Landscapes","n":0.316},"1":{"v":"Managing quality during the development, operation, and maintenance of software(-intensive) systems and services is a challenging task. Although many organizations need to define, control, measure, and improve various quality aspects of their development artifacts and processes, nearly no guidance is available on how to select, adapt, define, combine, use, and evolve quality models. Catalogs of models as well as selection and tailoring processes are widely missing. A first step towards better support for selecting and adapting quality models can be seen in a classification of existing quality models, especially with respect to their suitability for different purposes and contexts. This article presents so-called comprehensive quality model landscapes (CQMLs), which provide such a classification scheme and help to get an overview of existing models and their relationships. The article focuses on the description and justification of essential concepts needed to define quality models and landscapes. In addition, the article describes typical usage scenarios, illustrates the concept with examples, and sketches open questions and future work.","n":0.078}}},{"i":681,"$":{"0":{"v":"Large Scale Distributed Deep Networks","n":0.447},"1":{"v":"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.","n":0.068}}},{"i":682,"$":{"0":{"v":"A Practical Guide to Support Vector Classication","n":0.378},"1":{"v":"Support vector machine (SVM) is a popular technique for classication. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signicant steps. In this guide, we propose a simple procedure, which usually gives reasonable results.","n":0.152}}},{"i":683,"$":{"0":{"v":"“Why Should I Trust You?”: Explaining the Predictions of Any Classifier","n":0.302}}},{"i":684,"$":{"0":{"v":"Practical Bayesian Optimization of Machine Learning Algorithms","n":0.378},"1":{"v":"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.","n":0.074}}},{"i":685,"$":{"0":{"v":"Man is to computer programmer as woman is to homemaker? debiasing word embeddings","n":0.277},"1":{"v":"The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.","n":0.07}}},{"i":686,"$":{"0":{"v":"Equality of opportunity in supervised learning","n":0.408},"1":{"v":"We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.","n":0.102}}},{"i":687,"$":{"0":{"v":"Ad click prediction: a view from the trenches","n":0.354},"1":{"v":"Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates.   We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.","n":0.071}}},{"i":688,"$":{"0":{"v":"The Daikon system for dynamic detection of likely invariants","n":0.333},"1":{"v":"Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x=a), non-zero (x 0), being in a range (a@?x@?b), linear relationships (y=ax+b), ordering (x@?y), functions from a library (x=fn(y)), containment (x@?y), sortedness (xissorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C++, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikon's output has been used for generating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csail.mit.edu/daikon/.","n":0.069}}},{"i":689,"$":{"0":{"v":"Google Vizier: A Service for Black-Box Optimization","n":0.378},"1":{"v":"Any sufficiently complex system acts as a black box when it becomes easier to experiment with than to understand. Hence, black-box optimization has become increasingly important as systems have become more complex. In this paper we describe Google Vizier, a Google-internal service for performing black-box optimization that has become the de facto parameter tuning engine at Google. Google Vizier is used to optimize many of our machine learning models and other systems, and also provides core capabilities to Google's Cloud Machine Learning HyperTune subsystem. We discuss our requirements, infrastructure design, underlying algorithms, and advanced features such as transfer learning and automated early stopping that the service provides.","n":0.097}}},{"i":690,"$":{"0":{"v":"Machine Learning: The High Interest Credit Card of Technical Debt","n":0.316},"1":{"v":"Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns. 1 Machine Learning and Complex Systems Real world software engineers are often faced with the challenge of moving quickly to ship new products or services, which can lead to a dilemma between speed of execution and quality of engineering. The concept of technical debt was first introduced by Ward Cunningham in 1992 as a way to help quantify the cost of such decisions. Like incurring fiscal debt, there are often sound strategic reasons to take on technical debt. Not all debt is necessarily bad, but technical debt does tend to compound. Deferring the work to pay it off results in increasing costs, system brittleness, and reduced rates of innovation. Traditional methods of paying off technical debt include refactoring, increasing coverage of unit tests, deleting dead code, reducing dependencies, tightening APIs, and improving documentation [4]. The goal of these activities is not to add new functionality, but to make it easier to add future improvements, be cheaper to maintain, and reduce the likelihood of bugs. One of the basic arguments in this paper is that machine learning packages have all the basic code complexity issues as normal code, but also have a larger system-level complexity that can create hidden debt. Thus, refactoring these libraries, adding better unit tests, and associated activity is time well spent but does not necessarily address debt at a systems level. In this paper, we focus on the system-level interaction between machine learning code and larger systems as an area where hidden technical debt may rapidly accumulate. At a system-level, a machine learning model may subtly erode abstraction boundaries. It may be tempting to re-use input signals in ways that create unintended tight coupling of otherwise disjoint systems. Machine learning packages may often be treated as black boxes, resulting in large masses of “glue code” or calibration layers that can lock in assumptions. Changes in the external world may make models or input signals change behavior in unintended ways, ratcheting up maintenance cost and the burden of any debt. Even monitoring that the system as a whole is operating as intended may be difficult without careful design.","n":0.047}}},{"i":691,"$":{"0":{"v":"Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization","n":0.354},"1":{"v":"How can we take advantage of opportunities for experimental parallelization in exploration-exploitation tradeoffs? In many experimental scenarios, it is often desirable to execute experiments simultaneously or in batches, rather than only performing one at a time. Additionally, observations may be both noisy and expensive. We introduce Gaussian Process Batch Upper Confidence Bound (GP-BUCB), an upper confidence bound-based algorithm, which models the reward function as a sample from a Gaussian process and which can select batches of experiments to run in parallel. We prove a general regret bound for GP-BUCB, as well as the surprising result that for some common kernels, the asymptotic average regret can be made independent of the batch size. The GP-BUCB algorithm is also applicable in the related case of a delay between initiation of an experiment and observation of its results, for which the same regret bounds hold. We also introduce Gaussian Process Adaptive Upper Confidence Bound (GP-AUCB), a variant of GP-BUCB which can exploit parallelism in an adaptive manner. We evaluate GP-BUCB and GP-AUCB on several simulated and real data sets. These experiments show that GP-BUCB and GP-AUCB are competitive with state-of-the-art heuristics.","n":0.073}}},{"i":692,"$":{"0":{"v":"TFX: A TensorFlow-Based Production-Scale Machine Learning Platform","n":0.378},"1":{"v":"Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components---a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt.   We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions.   We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2% increase in app installs resulting from improved data and model analysis.","n":0.071}}},{"i":693,"$":{"0":{"v":"Testing scientific software: A systematic literature review","n":0.378},"1":{"v":"Context: Scientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code. Objective: This study aims to identify specific challenges, proposed solutions, and unsolved problems faced when testing scientific software. Method: We conducted a systematic literature survey to identify and analyze relevant literature. We identified 62 studies that provided relevant information about testing scientific software. Results: We found that challenges faced when testing scientific software fall into two main categories: (1) testing challenges that occur due to characteristics of scientific software such as oracle problems and (2) testing challenges that occur due to cultural differences between scientists and the software engineering community such as viewing the code and the model that it implements as inseparable entities. In addition, we identified methods to potentially overcome these challenges and their limitations. Finally we describe unsolved challenges and how software engineering researchers and practitioners can help to overcome them. Conclusions: Scientific software presents special challenges for testing. Specifically, cultural differences between scientist developers and software engineers, along with the characteristics of the scientific software make testing more difficult. Existing techniques such as code clone detection can help to improve the testing process. Software engineers should consider special challenges posed by scientific software such as oracle problems when developing testing techniques.","n":0.064}}},{"i":694,"$":{"0":{"v":"An Approach to Software Testing of Machine Learning Applications","n":0.333},"1":{"v":"Some machine learning applications are intended to learn properties of data sets where the correct answers are not already known to human users. It is challenging to test such ML software, because there is no reliable test oracle. We describe a software testing approach aimed at addressing this problem. We present our findings from testing implementations of two different ML ranking algorithms: Support Vector Machines and MartiRank.","n":0.122}}},{"i":695,"$":{"0":{"v":"TensorFlow Debugger: Debugging Dataflow Graphs for Machine Learning","n":0.354}}},{"i":696,"$":{"0":{"v":"Software clone detection: A systematic review","n":0.408},"1":{"v":"Abstract   Context  Reusing software by means of copy and paste is a frequent activity in software development. The duplicated code is known as a  software clone  and the activity is known as  code cloning . Software clones may lead to bug propagation and serious maintenance problems.    Objective  This study reports an extensive systematic literature review of software clones in general and software clone detection in particular.    Method  We used the standard systematic literature review method based on a comprehensive set of 213 articles from a total of 2039 articles published in 11 leading journals and 37 premier conferences and workshops.    Results  Existing literature about software clones is classified broadly into different categories. The importance of semantic clone detection and model based clone detection led to different classifications. Empirical evaluation of clone detection tools/techniques is presented. Clone management, its benefits and cross cutting nature is reported. Number of studies pertaining to nine different types of clones is reported. Thirteen intermediate representations and 24 match detection techniques are reported.    Conclusion  We call for an increased awareness of the potential benefits of software clone management, and identify the need to develop semantic and model clone detection techniques. Recommendations are given for future research.","n":0.071}}},{"i":697,"$":{"0":{"v":"An exploratory study of the impact of antipatterns on class change- and fault-proneness","n":0.277},"1":{"v":"Antipatterns are poor design choices that are conjectured to make object-oriented systems harder to maintain. We investigate the impact of antipatterns on classes in object-oriented systems by studying the relation between the presence of antipatterns and the change- and fault-proneness of the classes. We detect 13 antipatterns in 54 releases of ArgoUML, Eclipse, Mylyn, and Rhino, and analyse (1) to what extent classes participating in antipatterns have higher odds to change or to be subject to fault-fixing than other classes, (2) to what extent these odds (if higher) are due to the sizes of the classes or to the presence of antipatterns, and (3) what kinds of changes affect classes participating in antipatterns. We show that, in almost all releases of the four systems, classes participating in antipatterns are more change-and fault-prone than others. We also show that size alone cannot explain the higher odds of classes with antipatterns to underwent a (fault-fixing) change than other classes. Finally, we show that structural changes affect more classes with antipatterns than others. We provide qualitative explanations of the increase of change- and fault-proneness in classes participating in antipatterns using release notes and bug reports. The obtained results justify a posteriori previous work on the specification and detection of antipatterns and could help to better focus quality assurance and testing activities.","n":0.068}}},{"i":698,"$":{"0":{"v":"Java quality assurance by detecting code smells","n":0.378},"1":{"v":"Software inspection is a known technique for improving software quality. It involves carefully examining the code, the design, and the documentation of software and checking these for aspects that are known to be potentially problematic based on past experience. Code smells are a metaphor to describe patterns that are generally associated with bad design and bad programming practices. Originally, code smells are used to find the places in software that could benefit from refactoring. In this paper we investigate how the quality of code can be automatically assessed by checking for the presence of code smells and how this approach can contribute to automatic code inspection. We present an approach for the automatic detection and visualization of code smells and discuss how this approach can be used in the design of a software inspection tool. We illustrate the feasibility of our approach with the development of jCOSMO, a prototype code smell browser that detects and visualizes code smells in JAVA source code. Finally, we show how this tool was applied in a case study.","n":0.076}}},{"i":699,"$":{"0":{"v":"Detecting bad smells in source code using change history information","n":0.316},"1":{"v":"Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately.   We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis.","n":0.075}}},{"i":700,"$":{"0":{"v":"An interactive ambient visualization for code smells","n":0.378},"1":{"v":"Code smells are characteristics of software that indicate that code may have a design problem. Code smells have been proposed as a way for programmers to recognize the need for restructuring their software. Because code smells can go unnoticed while programmers are working, tools called smell detectors have been developed to alert programmers to the presence of smells in their code, and to help them understand the cause of those smells. In this paper, we propose a novel smell detector called Stench Blossom that provides an interactive ambient visualization designed to first give programmers a quick, high-level overview of the smells in their code, and then, if they wish, to help in understanding the sources of those code smells. We also describe a laboratory experiment with 12 programmers that tests several hypotheses about our tool. Our findings suggest that programmers can use our tool effectively to identify smells and to make refactoring judgements. This is partly because the tool serves as a memory aid, and partly because it is more reliable and easier to use than heuristics for analyzing smells.","n":0.075}}},{"i":701,"$":{"0":{"v":"JDeodorant: Identification and Removal of Type-Checking Bad Smells","n":0.354},"1":{"v":"In this demonstration, we present an Eclipse plug-in that automatically identifies type-checking bad smells in Java source code, and resolves them by applying the \"replace conditional with polymorphism\" or \"replace type code with state/strategy \" refactorings. To the best of our knowledge there is a lack of tools that identify type-checking bad smells. Moreover, none of the state-of-the-art IDEs support the refactorings that resolve such kind of bad smells.","n":0.12}}},{"i":702,"$":{"0":{"v":"JDeodorant: Identification and Removal of Feature Envy Bad Smells","n":0.333},"1":{"v":"In this demonstration we present an Eclipse plug-in that identifies feature envy bad smells in Java projects and resolves them by applying the appropriate move method refactorings. The main contribution is the ability to pre-evaluate the impact of all possible move refactorings on design quality and apply the most effective one.","n":0.14}}},{"i":703,"$":{"0":{"v":"Code Smell Detection: Towards a Machine Learning-Based Approach","n":0.354},"1":{"v":"Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Usually the detection techniques are based on the computation of different kinds of metrics, and other aspects related to the domain of the system under analysis, its size and other design features are not taken into account. In this paper we propose an approach we are studying based on machine learning techniques. We outline some common problems faced for smells detection and we describe the different steps of our approach and the algorithms we use for the classification.","n":0.1}}},{"i":704,"$":{"0":{"v":"Towards assessing software architecture quality by exploiting code smell relations","n":0.316},"1":{"v":"We can evaluate software architecture quality using a plethora of metrics proposed in the literature, but interpreting and exploiting in the right way these metrics is not always a simple task. This is true for both fixing the right metric threshold values and determining the actions to be taken to improve the quality of the system. Instead of metrics, we can detect code or architectural anomalies that give us useful hints on the possible architecture degradation. In this paper, we focus our attention on the detection of code smells and in particular on their relations and co-occurrences, with the aim to evaluate technical debt in an architectural context. We start from the assumption that certain patterns of code anomalies tend to be better indicators of architectural degradation than simple metrics evaluation.","n":0.087}}},{"i":705,"$":{"0":{"v":"Automatic metric thresholds derivation for code smell detection","n":0.354},"1":{"v":"Code smells are archetypes of design shortcomings in the code that can potentially cause problems during maintenance. One known approach for detecting code smells is via detection rules: a combination of different object-oriented metrics with pre-defined threshold values. The usage of inadequate thresholds when using this approach could lead to either having too few observations (too many false negatives) or too many observations (too many false positives). Furthermore, without a clear methodology for deriving thresholds, one is left with those suggested in literature (or by the tool vendors), which may not necessarily be suitable to the context of analysis. In this paper, we propose a data-driven (i.e., benchmark-based) method to derive threshold values for code metrics, which can be used for implementing detection rules for code smells. Our method is transparent, repeatable and enables the extraction of thresholds that respect the statistical properties of the metric in question (such as scale and distribution). Thus, our approach enables the calibration of code smell detection rules by selecting relevant systems as benchmark data. To illustrate our approach, we generated a benchmark dataset based on 74 systems of the Qualitas Corpus, and extracted the thresholds for five smell detection rules.","n":0.071}}},{"i":706,"$":{"0":{"v":"A Robust Multi-objective Approach for Software Refactoring under Uncertainty","n":0.333},"1":{"v":"Refactoring large systems involves several sources of uncertainty related to the severity levels of code smells to be corrected and the importance of the classes in which the smells are located. Due to the dynamic nature of software development, these values cannot be accurately determined in practice, leading to refactoring sequences that lack robustness. To address this problem, we introduced a multi-objective robust model, based on NSGA-II, for the software refactoring problem that tries to find the best trade-off between quality and robustness. We evaluated our approach using six open source systems and demonstrated that it is significantly better than state-of-the-art refactoring approaches in terms of robustness in 100% of experiments based on a variety of real-world scenarios. Our suggested refactoring solutions were found to be comparable in terms of quality to those suggested by existing approaches and to carry an acceptable robustness price. Our results also revealed an interesting feature about the trade-off between quality and robustness that demonstrates the practical value of taking robustness into account in software refactoring.","n":0.076}}},{"i":707,"$":{"0":{"v":"CodeCrawler: an information visualization tool for program comprehension","n":0.354},"1":{"v":"C ode C rawler  (in the remainder of the text CC) is a language independent, interactive, information visualization tool. It is mainly targeted at visualizing object-oriented software, and has been successfully validated in several industrial case studies over the past few years. CC adheres to lightweight principles: it implements and visualizes polymetric views, visualizations of software enriched with information such as software metrics and other source code semantics. CC is built on top of Moose, an extensible language independent reengineering environment that implements the FAMIX metamodel. In its last implementation, CC has become a general-purpose information visualization tool.","n":0.101}}},{"i":708,"$":{"0":{"v":"SMURF: A SVM-based Incremental Anti-pattern Detection Approach","n":0.378},"1":{"v":"In current, typical software development projects, hundreds of developers work asynchronously in space and time and may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and -- or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns incrementally and on subsets of a system could reduce costs, effort, and resources by allowing practitioners to identify and take into account occurrences of anti-patterns as they find them during their development and maintenance activities. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently four limitations: (1) they require extensive knowledge of anti-patterns, (2) they have limited precision and recall, (3) they are not incremental, and (4) they cannot be applied on subsets of systems. To overcome these limitations, we introduce SMURF, a novel approach to detect anti-patterns, based on a machine learning technique -- support vector machines -- and taking into account practitioners' feedback. Indeed, through an empirical study involving three systems and four anti-patterns, we showed that the accuracy of SMURF is greater than that of DETEX and BDTEX when detecting anti-patterns occurrences. We also showed that SMURF can be applied in both intra-system and inter-system configurations. Finally, we reported that SMURF accuracy improves when using practitioners' feedback.","n":0.068}}},{"i":709,"$":{"0":{"v":"Poster: Filtering Code Smells Detection Results","n":0.408},"1":{"v":"Many tools for code smell detection have been devel- oped, providing often different results. This is due to the informal definition of code smells and to the subjective interpretation of them. Usually, aspects related to the domain, size, and design of the system are not taken into account when detecting and analyzing smells. These aspects can be used to filter out the noise and achieve more relevant results. In this paper, we propose different filters that we have identified for five code smells. We provide two kind of filters, Strong and Weak Filters, that can be integrated as part of a detection approach.","n":0.099}}},{"i":710,"$":{"0":{"v":"Detecting Bad Smells in Object Oriented Design Using Design Change Propagation Probability Matrix","n":0.277},"1":{"v":"Object oriented software systems are subject to frequent modifications either during development (iterative, agile software development) or software evolution. For such systems which have large number of classes, detection of design defects is a complex task. Bad smells are used to identify design defects in object oriented software design. Identification of bad smells allows us to apply appropriate refactorings to improve the quality of design. In existing bad smell detection systems, bad smells are generally detected using human intuition, and recently, people started developing quantitative methods. As human intuition is subjective, the quantitative methods to detect bad smells are effective as they do not include subjectivity (bias) and allows for automation. This paper proposes a quantitative method. The proposed quantitative method makes use of the concept design change propagation probability matrix (DCPP matrix) to detect two important bad smells. The first one is shotgun surgery bad smell and the other one is divergent change bad smell. Two of the advantages of the proposed quantitative method are: Detecting shotgun surgery and divergent change bad smells require that the design change propagation between artifacts that are connected directly and indirectly should be considered quantitatively. The proposed method considers this aspect quantitatively. The second advantage is, the method is amicable for automation. Using this proposed method, with typical example designs, the bad smells shotgun surgery and divergent change are detected. Appropriate refactorings are suggested for the detected bad smells. Different advantages of the proposed quantitative method are presented. A broader framework in which this quantitative method is applied is given.","n":0.062}}},{"i":711,"$":{"0":{"v":"Filtering clones for individual user based on machine learning analysis","n":0.316},"1":{"v":"Results from code clone detectors may contain plentiful useless code clones, and judging whether a code clone is useful varies from user to user based on different purposes of them. We are planing a system to study the judgment of each individual user by applying machine learning algorithms on code clones. We describe the reason why individual judgment should be respected and how in this paper.","n":0.123}}},{"i":712,"$":{"0":{"v":"Bad-smell prediction from software design model using machine learning techniques","n":0.316},"1":{"v":"Bad-smell prediction significantly impacts on software quality. It is beneficial if bad-smell prediction can be performed as early as possible in the development life cycle. We present methodology for predicting bad-smells from software design model. We collect 7 data sets from the previous literatures which offer 27 design model metrics and 7 bad-smells. They are learnt and tested to predict bad-smells using seven machine learning algorithms. We use cross-validation for assessing the performance and for preventing over-fitting. Statistical significance tests are used to evaluate and compare the prediction performance. We conclude that our methodology have proximity to actual values.","n":0.101}}},{"i":713,"$":{"0":{"v":"Filtering code smells detection results","n":0.447},"1":{"v":"Many tools for code smell detection have been developed, providing often different results. This is due to the informal definition of code smells and to the subjective interpretation of them. Usually, aspects related to the domain, size, and design of the system are not taken into account when detecting and analyzing smells. These aspects can be used to filter out the noise and achieve more relevant results. In this paper, we propose different filters that we have identified for five code smells. We provide two kind of filters, Strong and Weak Filters, that can be integrated as part of a detection approach.","n":0.099}}},{"i":714,"$":{"0":{"v":"A Duplicated Code Refactoring Advisor","n":0.447},"1":{"v":"Refactoring is one of the key practices in Extreme Programming and other agile methods. Duplicated code is one of the most pervasive and pungent smells to remove from source code through refactoring. Duplicated code has been largely studied in the literature, and different types of duplications, or “clones”, have been identified. Some studies analyzed in details the problems caused by clones in the code, others outlined also the difficulties in removing clones, and the cases in which it could be better not removing them. The refactoring cost for removing clones can be very high, also due to the different choices on the possible refactoring steps. In this paper, we describe our approach and tool developed with the aim to suggest the best refactorings to remove clones in Java code. Our approach is based on the classification of the clones in terms of their location in a class hierarchy, and allows to choose among a restricted set of refactorings, which are then evaluated using multiple criteria. We provide a validation of the effectiveness of the approach.","n":0.076}}},{"i":715,"$":{"0":{"v":"Code Smell Detecting Tool and Code Smell-Structure Bug Relationship","n":0.333},"1":{"v":"This paper proposes an approach for detecting the so- called bad smells in software known as Code Smell. In considering software bad smells, object-oriented software metrics were used to detect the source code whereby Eclipse Plugins were developed for detecting in which location of Java source code the bad smell appeared so that software refactoring could then take place. The detected source code was classified into 7 types: Large Class, Long Method, Parallel Inheritance Hierarchy, Long Parameter List, Lazy Class, Switch Statement, and Data Class. This work conducted analysis by using 323 java classes to ascertain the relationship between the code smell and structural defects of software by using the data mining techniques of Naive Bayes and Association Rules. The result of the Naive Bayes test showed that the Lazy Class caused structural defects in DLS, DE, and Se. Also, Data Class caused structural defects in UwF, DE, and Se, while Long Method, Large Class, Data Class, and Switch Statement caused structural defects in UwF and Se. Finally, Parallel Inheritance Hierarchy caused structural defects in Se. However, Long Parameter List caused no structural defects whatsoever. The results of the Association Rules test found that the Lazy Class code smell caused structural defects in DLS and DE, which corresponded to the results of the Naive Bayes test.","n":0.068}}},{"i":716,"$":{"0":{"v":"Technical debt from the stakeholder perspective","n":0.408},"1":{"v":"The concept of technical debt provides an excellent tool for describing technology gaps in terms any stakeholder can understand. The technical debt metaphor was pioneered by the software development community and describes technical challenges in that context very well. However, establishing a definitional framework which describes issues affecting quality more broadly will better align to stakeholder perspectives. Building on the existing concept in this way will enable technology stakeholders by providing a centralized technical debt model. The metaphor can then be used to consistently describe quality challenges anywhere within the technical environment. This paper lays the foundation for this conceptual model by proposing a definitional framework that describes how technology gaps affect all aspects of quality.","n":0.093}}},{"i":717,"$":{"0":{"v":"Tool Support for Expert-Centred Code Assessments","n":0.408},"1":{"v":"There is empirical evidence that the code quality of software has an important impact on the external, i.e., user perceptible, software quality. Currently a large number of source code metrics exist that seem to ease the evaluation of code quality. Nevertheless, studies show that the theoretical foundations are weak and promising approaches for the automatic assessment of code quality are to be considered with great caution. We therefore came to the conclusion that the metric values and other findings provided by various static code analysis tools can only be used in the context of an expert-centred assessment of internal software quality. In order to be able to carry out code quality assessments in a timely and efficient manner it is inevitable to have additional tool support. For that purpose we developed the eclipsed based tool software product quality reporter (SPQR) that supports expert-centred evaluation of source code - from the formulation of project-specific quality models up to the generation of preliminary code quality reports. The application of SPQR already proved its usefulness in various code assessment projects around the world.","n":0.075}}},{"i":718,"$":{"0":{"v":"Benchmarking-oriented analysis of source code quality: experiences with the QBench approach","n":0.302},"1":{"v":"Static code analysis tools provide valuable input for experts to judge the internal quality of software. Nevertheless this approach is time consuming and therefore (semi-)automatic approaches would be desirable. In the QBench [14] project a promising benchmarking oriented approach for calculating a quality rating was developed. We tried to apply the methods and tools of the QBench project in order to investigate the strengths and weaknesses of the approach. In summary, the calculation of the quality rating does not lead to satisfying results. We therefore developed a number of alternative calculation methods and compared them to the results of the original calculation. Some of these calculation variants lead to a better characterization of the software quality as the original QBench algorithm. Furthermore we detected some technical weaknesses in the provided toolset and considerably enhanced this toolset for calculating the quality rating to make it better applicable.","n":0.083}}},{"i":719,"$":{"0":{"v":"METHODOLOGICAL FIT IN MANAGEMENT FIELD RESEARCH.","n":0.408},"1":{"v":"Methodological fit, an implicitly valued attribute of high-quality field research in organizations, has received little attention in the management literature. Fit refers to internal consistency among elements of a research project—research question, prior work, research design, and theoretical contribution. We introduce a contingency framework that relates prior work to the design of a research project, paying particular attention to the question of when to mix qualitative and quantitative data in a single research paper. We discuss implications of the framework for educating new field researchers.","n":0.108}}},{"i":720,"$":{"0":{"v":"Introduction to Action Research","n":0.5}}},{"i":721,"$":{"0":{"v":"Essential Scrum: A Practical Guide to the Most Popular Agile Process","n":0.302},"1":{"v":"A Practical Guide to the Most Popular Agile Process The Single-Source, Comprehensive Guide to Scrum for All Team Members, Managers, and Executives If you want to use Scrum to develop innovative products and services that delight your customers, Essential Scrum is the complete, single-source reference youve been searching for. Leading Scrum coach and trainer Kenny Rubin illuminates the values, principles, and practices of Scrum, and describes flexible, proven approaches that can help you implement it far more effectively. Whether you are new to Scrum or years into your use, this book will introduce, clarify, and deepen your Scrum knowledge at the team, product, and portfolio levels. Drawing from Rubins experience helping hundreds of organizations succeed with Scrum, this book provides easy-to-digest descriptions enhanced by more than two hundred illustrations based on an entirely new visual icon language for describing Scrums roles, artifacts, and activities. Essential Scrum will provide every team member, manager, and executive with a common understanding of Scrum, a shared vocabulary they can use in applying it, and practical knowledge for deriving maximum value from it.","n":0.075}}},{"i":722,"$":{"0":{"v":"Introduction to Action Research: Social Research for Social Change","n":0.333},"1":{"v":"What Is Action Research? Introduction: Action Research, Diversity, and Democracy A History of Action Research Action Research Cases From Practice: The Stories of Stongfjorden, Mondragon, and Programs for Employment and Workplace Systems at Cornell University Science, Epistemology, and Practice in Action Research An Epistemological Foundation for Action Research Scientific Method and Action Research Social Science Research Techniques, Work Forms, and Research Strategies in Action Research Knowledge Generation in Action Research: The Dialectics of Local Knowledge and Research-based Knowledge The Friendly Outsider: From AR as a Research Strategy to the Skills Needed to Become an Action Researcher Varieties of Action Research Praxis: Liberating Human Potential Pragmatic Action Research Power, Liberation, Adult Education, Feminism, and Social Reform Educational Action Research Participatory evaluation Rapid Rural Appraisal, Participatory Rural Appraisal, and Participatory Learning and Analysis Human Inquiry, Collaborative Inquiry, Cooperative Inquiry, Action Inquiry, Self-reflective Inquiry, and Mapping the Varieties of Action Research Action Science and Organizational Learning Action Research, Higher Education, and Democracy Educating Action Researchers Action Research, Participation, and Democratization","n":0.077}}},{"i":723,"$":{"0":{"v":"An Assessment of the Scientific Merits of Action Research.","n":0.333},"1":{"v":"December 1978, volume 23 This article describes the deficiencies of positivist science for generating knowledge for use in solving problems that members of organizations face. Action research is introduced as a method for correcting these deficiencies. When action research is tested against the criteria of positivist science, action research is found not to meet its critical tests. The appropriateness of positivist science is questioned as a basis for judging the scientific merits of action research. Action research can base its legitimacy as science in philosophical traditions that are different from those which legitimate positivist science. Criteria and methods of science appropriate to action research are offered.","n":0.097}}},{"i":724,"$":{"0":{"v":"Visualizing and Managing Technical Debt in Agile Development: An Experience Report","n":0.302},"1":{"v":"This paper reports the experience of an architecture team of a software development department with 25 agile teams in supporting technical decisions regarding technical practices. The main motivation to use technical debt metaphor was its acknowledged potential in driving software development and maintenance decisions, especially those long term maintenance tradeoffs which are usually less visible to developers and decision makers in general. We propose the use of a \"technical debt board\" with main technical debt categories to manage and visualize the high-level debt, combined with tools to measure it at low-level (software metrics and other kind of static analysis). We have found that our approach improved the teams’ awareness about the technical debt, stimulated a beneficial competition between teams towards the debt payment and enhanced the communication regarding technical decisions.","n":0.088}}},{"i":725,"$":{"0":{"v":"Flexibility in research designs in empirical software engineering","n":0.354},"1":{"v":"Problem outline: It is common to classify empirical research designs as either qualitative or quantitative. Typically, particular research methods (e.g., case studies, action research, experiments and surveys) are associated with one or the other of these types of design. Studies in empirical software engineering (ESE) are often exploratory and often involve software developers and development organizations. As a consequence, it may be difficult to plan all aspects of the studies, and to be successful, ESE studies must often be designed to handle possible changes during the conduct of the study. A problem with the above classification is that it does not cater for flexibility in design.\r\n\r\nPosition: This paper suggests viewing research in ESE along the axis of flexible and fixed designs, which is both orthogonal to the axis of quantitative and qualitative designs, and independent of the particular research method. According to the traditional view of ESE, changes to the research design in the course of a study are typically regarded as threats to the validity of the results. However, viewing the study designs as flexible, practical challenges can provide useful information. The validity of the results of studies with flexible research designs can be established by applying techniques that are traditionally used for qualitative designs. This paper urges an increased recognition of flexible designs in ESE and discusses techniques for establishing the trustworthiness in flexible designs.","n":0.066}}},{"i":726,"$":{"0":{"v":"Technical debt : what software practitioners have to say","n":0.333}}},{"i":727,"$":{"0":{"v":"Search‐based software test data generation: a survey","n":0.378},"1":{"v":"The use of metaheuristic search techniques for the automatic generation of test data has been a burgeoning interest for many researchers in recent years. Previous attempts to automate the test generation process have been limited, having been constrained by the size and complexity of software, and the basic fact that in general, test data generation is an undecidable problem. Metaheuristic search techniques oer much promise in regard to these problems. Metaheuristic search techniques are highlevel frameworks, which utilise heuristics to seek solutions for combinatorial problems at a reasonable computational cost. To date, metaheuristic search techniques have been applied to automate test data generation for structural and functional testing; the testing of grey-box properties, for example safety constraints; and also non-functional properties, such as worst-case execution time. This paper surveys some of the work undertaken in this eld, discussing possible new future directions of research for each of its dieren t individual areas.","n":0.081}}},{"i":728,"$":{"0":{"v":"Translation techniques in cross-language information retrieval","n":0.408},"1":{"v":"Cross-language information retrieval (CLIR) is an active sub-domain of information retrieval (IR). Like IR, CLIR is centered on the search for documents and for information contained within those documents. Unlike IR, CLIR must reconcile queries and documents that are written in different languages. The usual solution to this mismatch involves translating the query and/or the documents before performing the search. Translation is therefore a pivotal activity for CLIR engines. Over the last 15 years, the CLIR community has developed a wide range of techniques and models supporting free text translation. This article presents an overview of those techniques, with a special emphasis on recent developments.","n":0.098}}},{"i":729,"$":{"0":{"v":"Search-based software engineering: Trends, techniques and applications","n":0.378},"1":{"v":"In the past five years there has been a dramatic increase in work on Search-Based Software Engineering (SBSE), an approach to Software Engineering (SE) in which Search-Based Optimization (SBO) algorithms are used to address problems in SE. SBSE has been applied to problems throughout the SE lifecycle, from requirements and project planning to maintenance and reengineering. The approach is attractive because it offers a suite of adaptive automated and semiautomated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives.   This article1 provides a review and classification of literature on SBSE. The work identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.","n":0.089}}},{"i":730,"$":{"0":{"v":"The Current State and Future of Search Based Software Engineering","n":0.316},"1":{"v":"This paper describes work on the application of optimization techniques in software engineering. These optimization techniques come from the operations research and metaheuristic computation research communities. The paper briefly reviews widely used optimization techniques and the key ingredients required for their successful application to software engineering, providing an overview of existing results in eight software engineering application domains. The paper also describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of open problems, challenges and areas for future work.","n":0.105}}},{"i":731,"$":{"0":{"v":"A Theoretical and Empirical Study of Search-Based Testing: Local, Global, and Hybrid Search","n":0.277},"1":{"v":"Search-based optimization techniques have been applied to structural software test data generation since 1992, with a recent upsurge in interest and activity within this area. However, despite the large number of recent studies on the applicability of different search-based optimization approaches, there has been very little theoretical analysis of the types of testing problem for which these techniques are well suited. There are also few empirical studies that present results for larger programs. This paper presents a theoretical exploration of the most widely studied approach, the global search technique embodied by Genetic Algorithms. It also presents results from a large empirical study that compares the behavior of both global and local search-based optimization on real-world programs. The results of this study reveal that cases exist of test data generation problem that suit each algorithm, thereby suggesting that a hybrid global-local search (a Memetic Algorithm) may be appropriate. The paper presents a Memetic Algorithm along with further empirical results studying its performance.","n":0.079}}},{"i":732,"$":{"0":{"v":"Search-based software engineering","n":0.577},"1":{"v":"Abstract   This paper claims that a new field of software engineering research and practice is emerging: search-based software engineering. The paper argues that software engineering is ideal for the application of metaheuristic search techniques, such as genetic algorithms, simulated annealing and tabu search. Such search-based techniques could provide solutions to the difficult problems of balancing competing (and some times inconsistent) constraints and may suggest ways of finding acceptable solutions in situations where perfect solutions are either theoretically impossible or practically infeasible.  In order to develop the field of search-based software engineering, a reformulation of classic software engineering problems as search problems is required. The paper briefly sets out key ingredients for successful reformulation and evaluation criteria for search-based software engineering.","n":0.091}}},{"i":733,"$":{"0":{"v":"Search based software engineering: techniques, taxonomy, tutorial","n":0.378},"1":{"v":"The aim of Search Based Software Engineering (SBSE) research is to move software engineering problems from human-based search to machine-based search, using a variety of techniques from the metaheuristic search, operations research and evolutionary computation paradigms. The idea is to exploit humans' creativity and machines' tenacity and reliability, rather than requiring humans to perform the more tedious, error prone and thereby costly aspects of the engineering process. SBSE can also provide insights and decision support. This tutorial will present the reader with a step-by-step guide to the application of SBSE techniques to Software Engineering. It assumes neither previous knowledge nor experience with Search Based Optimisation. The intention is that the tutorial will cover sufficient material to allow the reader to become productive in successfully applying search based optimisation to a chosen Software Engineering problem of interest.","n":0.086}}},{"i":734,"$":{"0":{"v":"Search-based refactoring for software maintenance","n":0.447},"1":{"v":"The high cost of software maintenance could be reduced by automatically improving the design of object-oriented programs without altering their behaviour. We have constructed a software tool capable of refactoring object-oriented programs to conform more closely to a given design quality model, by formulating the task as a search problem in the space of alternative designs. This novel approach is validated by two case studies, where programs are automatically refactored to increase flexibility, reusability and understandability as defined by a contemporary quality model. Both local and simulated annealing searches were found to be effective in this task.","n":0.102}}},{"i":735,"$":{"0":{"v":"Search-based software maintenance","n":0.577},"1":{"v":"The high cost of software maintenance could potentially be greatly reduced by the automatic refactoring of object-oriented programs to increase their understandability, adaptability and extensibility. This paper describes a novel approach in providing automated refactoring support for software maintenance; the formulation of the task as a search problem in the space of alternative designs. Such a search is guided by a quality evaluation function that must accurately reflect refactoring goals. We have constructed a search-based software maintenance tool and report here the results of experimental refactoring of two Java programs, which yielded improvements in terms of the quality functions used. We also discuss the comparative merits of the three quality functions employed and the actual effect on program design that resulted from their use.","n":0.09}}},{"i":736,"$":{"0":{"v":"Experimental assessment of software metrics using automated refactoring","n":0.354},"1":{"v":"A large number of software metrics have been proposed in the literature, but there is little understanding of how these metrics relate to one another. We propose a novel experimental technique, based on search-based refactoring, to assess software metrics and to explore relationships between them. Our goal is not to improve the program being refactored, but to assess the software metrics that guide the auto- mated refactoring through repeated refactoring experiments.   We apply our approach to five popular cohesion metrics using eight real-world Java systems, involving 300,000 lines of code and over 3,000 refactorings. Our results demonstrate that cohesion metrics disagree with each other in 55% of cases, and show how our approach can be used to reveal novel and surprising insights into the software metrics under investigation.","n":0.088}}},{"i":737,"$":{"0":{"v":"Metrics are fitness functions too","n":0.447},"1":{"v":"Metrics, whether collected statically or dynamically, and whether constructed from source code, systems or processes, are largely regarded as a means of evaluating some property of interest. This viewpoint has been very successful in developing a body of knowledge, theory and experience in the application of metrics to estimation, predication, assessment, diagnosis, analysis and improvement. This paper shows that there is an alternative, complementary, view of a metric: as a fitness function, used to guide a search for optimal or near optimal individuals in a search space of possible solutions. This 'Metrics as Fitness Functions' (MAFF) approach offers a number of additional benefits to metrics research and practice because it allows metrics to be used to improve software as well as to assess it and because it provides an additional mechanism of metric analysis and validation. This paper presents a brief survey of search-based approaches and shows how metrics have been combined with the search based techniques to improve software systems. It describes the properties of a metric which make it a good fitness function and explains the benefits for metric analysis and validation which accrue from the MAFF approach.","n":0.073}}},{"i":738,"$":{"0":{"v":"Using Heuristic Search Techniques To Extract Design Abstractions From Source Code","n":0.302},"1":{"v":"As modern software systems are large and complex, appropriate abstractions of their structure are needed to make them more understandable and, thus, easier to maintain. Software clustering tools are useful to support the creation of these abstractions. In this paper we describe our search algorithms for software clustering, and conduct a case study to demonstrate how altering the clustering parameters impacts the behavior and performance of our algorithms.","n":0.121}}},{"i":739,"$":{"0":{"v":"Search-Based Software Project Management","n":0.5},"1":{"v":"Project management presents the manager with a complex set of related optimisation problems. Decisions made can more profoundly affect the outcome of a project than any other activity. In the chapter, we provide an overview of Search-Based Software Project Management, in which search-based software engineering (SBSE) is applied to problems in software project management. We show how SBSE has been used to attack the problems of staffing, scheduling, risk, and effort estimation. SBSE can help to solve the optimisation problems the manager faces, but it can also yield insight. SBSE therefore provides both decision making and decision support. We provide a comprehensive survey of search-based software project management and give directions for the development of this subfield of SBSE.","n":0.092}}},{"i":740,"$":{"0":{"v":"The Case for Non-Cohesive Packages","n":0.447},"1":{"v":"While the lack of cohesiveness of modules in procedural languages is a good way to identify modules with potential quality problems, we doubt that it is an adequate measure for packages in object-oriented systems. Indeed, mapping procedural metrics to object-oriented systems should take into account the building principles of object-oriented programming: inheritance and late binding. Inheritance offers the possibility to create packages by just extending classes with the necessary increment of behavior. Late binding coupled to the \" Hollywood Principle \" are a key to build frameworks and let the users branch their extensions in the framework. Therefore, a package extending a framework does not have to be cohesive, since it inherits the framework logic, which is encapsulated in framework packages. In such a case, the correct modularization of an extender application may imply low cohesion for some of the packages. In this paper we confirm these conjectures on various real systems (JHotdraw, Eclipse, JEdit, JFace) using or extending OO frameworks. We carry out a dependency analysis of packages to measure their relation with their framework. The results show that framework dependencies form a considerable portion of the overall package dependencies. This means that non-cohesive packages should not be considered systematically as packages of low quality.","n":0.07}}},{"i":741,"$":{"0":{"v":"Code-Imp: a tool for automated search-based refactoring","n":0.378},"1":{"v":"Manual refactoring is tedious and error-prone, so it is natural to try to automate this process as much as possible. Fully automated refactoring usually involves using metaheuristic search to determine which refactorings should be applied to improve the program according to some fitness function, expressed in terms of standard software quality metrics.   Code-Imp (Combinatorial Optimisation for Design Improvement) is such an automated refactoring platform for the Java language. It can apply a range of refactorings, supports several search types, and implements over 25 software quality metrics which can be combined in various ways to form a fitness function. The original goal of the Code-Imp project was to investigate the use of automated refactoring to improve software quality as expressed by a contemporary metrics suite.   In this paper we present a technical overview of the Code-Imp implementation, and summarise three active research strands involving Code-Imp: refactoring for testability, metrics exploration, and multi-level design improvement.","n":0.081}}},{"i":742,"$":{"0":{"v":"Evolving transformation sequences using genetic algorithms","n":0.408},"1":{"v":"Program transformation is useful in a number of applications including program comprehension, reverse engineering and compiler optimization. In all these applications, transformation algorithms are constructed by hand for each different transformation goal. Loosely speaking, a transformation algorithm defines a sequence of transformation steps to apply to a given program. It is notoriously hard to find good transformation sequences automatically, and so much (costly) human intervention is required. This work shows how search-based meta-heuristic algorithms can be used to automate, or partly automate the problem of finding good transformation sequences. In this case, the goal of transformation is to reduce program size, but the approach is sufficiently general that it can be used to optimize any source-code level metric. The search techniques used are random search (RS), hill climbing (HC) and genetic algorithms (GA). The paper reports the result of initial experiments on small synthetic program transformation problems. The results are encouraging. They indicate that the genetic algorithm performs significantly better than either hill climbing or random search.","n":0.077}}},{"i":743,"$":{"0":{"v":"An Empirical Study About Search-Based Refactoring Using Alternative Multiple and Population-Based Search Techniques","n":0.277},"1":{"v":"Automated maintenance of object-oriented software system designs via refactoring is a performance demanding combinatorial optimization problem. In this study, we made an empirical comparative study to see the performances of alternative search algorithms under a quality model defined by an aggregated software fitness metric. We handled 20 different refactoring actions that realize searches on design landscape defined by combination of 24 object-oriented software metrics. The investigated algorithms include random, steepest descent, multiple first descent, multiple steepest descent, simulated annealing and artificial bee colony searches. The study is realized by using a tool called A-CMA developed in Java that accepts bytecode compiled Java codes as its input. The empiricial study showed that multiple steepest descent and population-based artificial bee colony algorithms are two most suitable approaches for the efficient solution of the search based refactoring problem.","n":0.086}}},{"i":744,"$":{"0":{"v":"Controversy Corner: Search Based Software Engineering: Review and analysis of the field in Brazil","n":0.267},"1":{"v":"Search Based Software Engineering (SBSE) is the field of software engineering research and practice that applies search based techniques to solve different optimization problems from diverse software engineering areas. SBSE approaches allow software engineers to automatically obtain solutions for complex and labor-intensive tasks, contributing to reduce efforts and costs associated to the software development. The SBSE field is growing rapidly in Brazil. The number of published works and research groups has significantly increased in the last three years and a Brazilian SBSE community is emerging. This is mainly due to the Brazilian Workshop on Search Based Software Engineering (WOES), co-located with the Brazilian Symposium on Software Engineering (SBES). Considering these facts, this paper presents results of a mapping we have performed in order to provide an overview of the SBSE field in Brazil. The main goal is to map the Brazilian SBSE community on SBES by identifying the main researchers, focus of the published works, fora and frequency of publications. The paper also introduces SBSE concerns and discusses trends, challenges, and open research problems to this emergent area. We hope the work serves as a reference to this novel field, contributing to disseminate SBSE and to its consolidation in Brazil.","n":0.071}}},{"i":745,"$":{"0":{"v":"Automated design flaw correction in object-oriented systems","n":0.378},"1":{"v":"Software inevitably changes. As a consequence, we observe the phenomenon referred to as \"software entropy\" or \"software decay\": the software design continually degrades making maintenance and functional extensions overly costly if not impossible. There exist a number of approaches to identify design flaws (problem detection) and to remedy them (refactoring). There is, however, a conceptual gap between these two stages: There is no appropriate support for the automated mapping of design flaws to possible solutions. Here we propose an integrated, quality-driven and tool-supported methodology to support object-oriented software evolution. Our approach is based on the novel concept of \"correction strategies\". Correction strategies serve as reference descriptions that enable a human-assisted tool to plan and perform all necessary steps for the safe removal of detected design flaws, with special concern towards the targeted quality goals of the restructuring process. We briefly sketch our tool chain and illustrate our approach with the help of a medium-sized real-world case-study.","n":0.08}}},{"i":746,"$":{"0":{"v":"Getting the most from search-based refactoring","n":0.408},"1":{"v":"Object-oriented systems that undergo repeated addition of functionality commonly suffer a loss of quality in their underlying design. This problem must often be remedied in a costly refactoring phase before further maintenance programming can take place. Recently search-based approaches to automating the task of softwarere factoring, based on the concept of treating object-oriented designas a combinatorial optimisation problem, have been proposed. However, because search-based refactoring is a novel approach it has yet to be established which search techniques are most suitable forthe task.   In this paper we report the results of an empirical comparison of simulated annealing, genetic algorithm and multiple ascent hill-climbing in search-based refactoring. A prototype automated refactoring tool is employed, capable of making radical changes to the design of an existing program in order that it conform more closely to a contemporary quality model. Results show multiple-ascent hill climbing to outperform both simulated annealing and genetic algorithm over a set of four input programs.","n":0.08}}},{"i":747,"$":{"0":{"v":"Managing Technical Debt: Shortcuts that save money and time today can cost you down the road.","n":0.25}}},{"i":748,"$":{"0":{"v":"Executable source code and non-executable source code: analysis and relationships","n":0.316},"1":{"v":"The concept of source code, understood as the source components used to obtain a binary, ready to execute version of a program, comprises currently more than source code written in a programming language. Specially when we move apart from systems-programming and enter the realm of end-user applications, we find source files with documentation, interface specifications, internationalization and localization modules, multimedia files, etc. All of them are source code in the sense that the developer works directly with them, and the application is built automatically using them as input. This work discusses the relationship between 'classical' source code (usually written in a programming language) and these other files by analyzing a publicly-available software versioning repository. Aspects that have been studied include the nature of the software repository, the different mixtures of source code found in several software projects stored in it, the specialization of developers to the different tasks, etc.","n":0.082}}},{"i":749,"$":{"0":{"v":"A stochastic approach to automated design improvement","n":0.378},"1":{"v":"The object-oriented approach to software development facilitates and encourages programming practices that increase reusability, correctness and maintainability in code. This is achieved in Java by providing mechanisms for inheritance, abstraction and encapsulation. By measuring properties that indicate to what extent these mechanisms are utilised we can determine to a large extent how good a design is. However, because these properties often conflict with other goals such as high cohesion and low coupling it can be difficult for a programmer to recognise the best compromise.We propose the novel approach of treating object-oriented design as a combinatorial optimisation problem, where the goal is maximisation of a set of design metrics. With a view to developing a fully automated design improvement tool we have developed a prototype that uses a small metrics suite combined with automated refactorings to move methods to their optimum positions in the class hierarchy. The action of this simulated annealing system produces a new design that is superior in terms of the metrics used and when judged on object-oriented principles.","n":0.076}}},{"i":750,"$":{"0":{"v":"Parameter-Based Refactoring and the Relationship with Fan-in/Fan-out Coupling","n":0.354},"1":{"v":"Refactoring is an activity which, in theory, should have minimal impact on the overall structure of a system. That said, certain refactorings change the coupling profile of a system and over time those cumulative changes in coupling can have serious implications for system maintenance effort. In this paper, we analyse effect of the fan-in and fan-out metrics from the perspective of two refactorings -- namely 'Add parameter' to, and 'Remove Parameter' from, a method. We developed a bespoke pattern-matching tool to collect these two refactorings from multiple releases of the Tomcat open-source system using the Evolizer tool to extract method signature data and the JHawk metrics tool to collect the two coupling metrics. Results point to significant differences in the profiles of fan-in and fan-out between refactored and non-refactored classes. We describe how software company can take advantage from this knowledge by defining a priority list of classes which could require a refactoring. A strong over-arching theme emerged: developers seemed to focus on the refactoring of classes with relatively high fan-in and fan-out rather than classes with high values in any one. The study is the first that we know of to analyse the direct effect of a subset of Fowler's refactorings on fan-in and fan-out - relevant metrics of the overall structure of a system.","n":0.068}}},{"i":751,"$":{"0":{"v":"Refactoring Support for Modularity Maintenance in Erlang","n":0.378},"1":{"v":"Low coupling between modules and high cohesion inside each module are key features of good software architecture. Systems written in modern programming languages generally start with some reasonably well-designed module structure, however with continuous feature additions, modifications and bug fixes, software modularity gradually deteriorates. So, there is a need for incremental improvements to modularity to avoid the situation when the structure of the system becomes too complex to maintain. We demonstrate how Wrangler, a general-purpose refactoring tool for Erlang, can be used to maintain and improve the modularity of programs written in Erlang without dramatically changing the existing module structure. We identify a set of \"modularity smells&quot, and show how they can be detected by Wrangler and removed by way of a variety of refactorings implemented in Wrangler. Validation of the approach and usefulness of the tool are demonstrated by case studies.","n":0.084}}},{"i":752,"$":{"0":{"v":"Finding Effective Software Metrics to Classify Maintainability Using a Parallel Genetic Algorithm","n":0.289},"1":{"v":"The ability to predict the quality of a software object can be viewed as a classification problem, where software metrics are the features and expert quality rankings the class labels. Evolutionary computational techniques such as genetic algorithms can be used to find a subset of metrics that provide an optimal classification for the quality of software objects. Genetic algorithms are also parallelizable, in that the fitness function (how well a set of metrics can classify the software objects) can be calculated independently from other possible solutions. A manager-worker parallel version of a genetic algorithm to find optimal metrics has been implemented using MPI and tested on a Beowulf cluster resulting in an efficiency of 0.94. Such a speed-up facilitated using larger populations for longer generations. Sixty-four source code metrics from a 366 class Java-based biomedical data analysis program were used and resulted in classification accuracy of 78.4%.","n":0.082}}},{"i":753,"$":{"0":{"v":"An Empirical Validation of Coupling Metrics Using Automated Refactoring","n":0.333},"1":{"v":"The validation of software metrics has received much interest over the years due to a desire to promote metrics which are both well-founded theoretically and have also been shown empirically to reflect our intuition and be practically useful. In this paper we describe how we used automatic refactoring to investigate the changes which occur to a number of metrics as software evolves. We use the concept of software volatility to quantify our comparison of the metrics.","n":0.115}}},{"i":754,"$":{"0":{"v":"Evolution doctor: a framework to control software system evolution","n":0.333},"1":{"v":"Real world software systems undergo, during their lifetime, to repeated maintenance activities. Due to the market pressure and to the need for having back the system operational in the shortest time possible, maintenance tends to introduce negative side effects. Some examples are the growth of the cloning percentage, the increase of library size, the presence of unused objects, or the lost of source file organization. This thesis proposes a framework, named evolution doctor, to diagnose and cure such phenomena. The framework permits the analysis and prediction of several indicators of software system evolution (size, complexity, cloning). Then, the framework defines a set of methods and tools to cure the problems: remove clones and unused objects, reorganize libraries, and restructure the source file directory organizations.","n":0.09}}},{"i":755,"$":{"0":{"v":"Applying evolution programming Search Based Software Engineering (SBSE) in selecting the best open source software maintainability metrics","n":0.243},"1":{"v":"The nature of an Open Source Software development paradigm forces individual practitioners and organization to adopt software through trial and error approach. This leads to the problems of coming across software and then abandoning it after realizing its lack of important qualities to suit their requirements or facing negative challenges in maintaining the software. These contributed by lack of recognizing guidelines to lead the practitioners in selecting out of the dozens available metrics, the best metric(s) to measure quality OSS. In this study, the novel results provide the guidelines that lead to the development of metrics model that can select the best metric(s) to predict maintainability of Open Source Software.","n":0.095}}},{"i":756,"$":{"0":{"v":"Parameter-based refactoring and the relationship with fan-in/fan-out coupling.","n":0.354}}},{"i":757,"$":{"0":{"v":"An Updated Survey on Search-Based Software Design","n":0.378}}},{"i":758,"$":{"0":{"v":"A heuristic-based approach to code-smell detection","n":0.408},"1":{"v":"Encapsulation and data hiding are central tenets of the object oriented paradigm. Deciding what data and behaviour to form into a class and where to draw the line between its public and private details can make the difference between a class that is an understandable, flexible and reusable abstraction and one which is not. This decision is a difficult one and may easily result in poor encapsulation which can then have serious implications for a number of system qualities. It is often hard to identify such encapsulation problems within large software systems until they cause a maintenance problem (which is usually too late) and attempting to perform such analysis manually can also be tedious and error prone. Two of the common encapsulation problems that can arise as a consequence of this decomposition process are data classes and god classes. Typically, these two problems occur together – data classes are lacking in functionality that has typically been sucked into an over-complicated and domineering god class. This paper describes the architecture of a tool which automatically detects data and god classes that has been developed as a plug-in for the Eclipse IDE. The technique has been evaluated in a controlled study on two large open source systems which compare the tool results to similar work by Marinescu, who employs a metrics-based approach to detecting such features. The study provides some valuable insights into the strengths and weaknesses of the two approaches","n":0.065}}},{"i":759,"$":{"0":{"v":"Experimentation in Software Engineering","n":0.5},"1":{"v":"Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and choosing between different methods, techniques, languages and tools. The purpose of Experimentation in Software Engineering is to introduce students, teachers, researchers, and practitioners to empirical studies in software engineering, using controlled experiments. The introduction to experimentation is provided through a process perspective, and the focus is on the steps that we have to go through to perform an experiment. The book is divided into three parts. The first part provides a background of theories and methods used in experimentation. Part II then devotes one chapter to each of the five experiment steps: scoping, planning, execution, analysis, and result presentation. Part III completes the presentation with two examples. Assignments and statistical material are provided in appendixes. Overall the book provides indispensable information regarding empirical studies in particular for experiments, but also for case studies, systematic literature reviews, and surveys. It is a revision of the authors book, which was published in 2000. In addition, substantial new material, e.g. concerning systematic literature reviews and case study research, is introduced. The book is self-contained and it is suitable as a course book in undergraduate or graduate studies where the need for empirical studies in software engineering is stressed. Exercises and assignments are included to combine the more theoretical material with practical aspects. Researchers will also benefit from the book, learning more about how to conduct empirical studies, and likewise practitioners may use it as a cookbook when evaluating new methods or techniques before implementing them in their organization.","n":0.06}}},{"i":760,"$":{"0":{"v":"Empirical analysis of CK metrics for object-oriented design complexity: implications for software defects","n":0.277},"1":{"v":"To produce high quality object-oriented (OO) applications, a strong emphasis on design aspects, especially during the early phases of software development, is necessary. Design metrics play an important role in helping developers understand design aspects of software and, hence, improve software quality and developer productivity. In this paper, we provide empirical evidence supporting the role of OO design complexity metrics, specifically a subset of the Chidamber and Kemerer (1991, 1994) suite (CK metrics), in determining software defects. Our results, based on industry data from software developed in two popular programming languages used in OO development, indicate that, even after controlling for the size of the software, these metrics are significantly associated with defects. In addition, we find that the effects of these metrics on defects vary across the samples from two programming languages-C++ and Java. We believe that these results have significant implications for designing high-quality software products using the OO approach.","n":0.081}}},{"i":761,"$":{"0":{"v":"Exploring the relationship between design measures and software quality in object-oriented systems","n":0.289},"1":{"v":"Abstract   One goal of this paper is to empirically explore the relationships between existing object-oriented (OO) coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. In other words, we wish to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. The second goal is to propose an investigation and analysis strategy to make these kind of studies more repeatable and comparable, a problem which is pervasive in the literature on quality measurement. Results show that many of the measures capture similar dimensions in the data set, thus reflecting the fact that many of them are based on similar principles and hypotheses. However, it is shown that by using a subset of measures, accurate models can be built to predict which classes most of the faults are likely to lie in. When predicting fault-prone classes, the best model shows a percentage of correct classifications higher than 80% and finds more than 90% of faulty classes. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault-proneness.","n":0.072}}},{"i":762,"$":{"0":{"v":"MIDAS: a design quality assessment method for industrial software","n":0.333},"1":{"v":"Siemens Corporate Development Center Asia Australia (CT DC AA) develops and maintains software applications for the Industry, Energy, Healthcare, and Infrastructure & Cities sectors of Siemens. The critical nature of these applications necessitates a high level of software design quality. A survey of software architects indicated a low level of satisfaction with existing design assessment practices in CT DC AA and highlighted several shortcomings of existing practices. To address this, we have developed a design assessment method called MIDAS (Method for Intensive Design ASsessments). MIDAS is an expert-based method wherein manual assessment of design quality by experts is directed by the systematic application of design analysis tools through the use of a three view-model consisting of design principles, project-specific constraints, and an “ility”-based quality model. In this paper, we describe the motivation for MIDAS, its design, and its application to three projects in CT DC AA. We believe that the insights from our MIDAS experience not only provide useful pointers to other organizations and practitioners looking to assess and improve software design quality but also suggest research questions for the software engineering community to explore.","n":0.074}}},{"i":763,"$":{"0":{"v":"From a domain analysis to the specification and detection of code and design smells","n":0.267},"1":{"v":"Code and design smells are recurring design problems in software systems that must be identified to avoid their possible negative consequences on development and maintenance. Consequently, several smell detection approaches and tools have been proposed in the literature. However, so far, they allow the detection of predefined smells but the detection of new smells or smells adapted to the context of the analysed systems is possible only by implementing new detection algorithms manually. Moreover, previous approaches do not explain the transition from specifications of smells to their detection. Finally, the validation of the existing approaches and tools has been limited on few proprietary systems and on a reduced number of smells. In this paper, we introduce an approach to automate the generation of detection algorithms from specifications written using a domain-specific language. This language is defined from a thorough domain analysis. It allows the specification of smells using high-level domain-related abstractions. It allows the adaptation of the specifications of smells to the context of the analysed systems. We specify 10 smells, generate automatically their detection algorithms using templates, and validate the algorithms in terms of precision and recall on Xerces v2.7.0 and GanttProject v1.10.2, two open-source object-oriented systems. We also compare the detection results with those of a previous approach, iPlasma.","n":0.069}}},{"i":764,"$":{"0":{"v":"Fine-Tuning Some Resistant Rules for Outlier Labeling","n":0.378},"1":{"v":"Abstract A previous study examined the performance of a standard rule from Exploratory Data Analysis, which uses the sample fourths, FL and FU , and labels as “outside” any observations below FL – k(FU – FL ) or above FU + k(FU – FL ), customarily with k = 1.5. In terms of the order statistics X (1) ≤ X (2) ≤ X (n) the standard definition of the fourths is FL = X(f) and FU = X (n + 1 − f), where f = ½[(n + 3)/2] and [·] denotes the greatest-integer function. The results of that study suggest that finer interpolation for the fourths might yield smoother behavior in the face of varying sample size. In this article we show that using f i = n/4 + (5/12) to define the fourths produces the desired smoothness. Corresponding to a common definition of quartiles, fQ = n/4 + (1/4) leads to similar results. Instead of allowing the some-outside rate per sample (the probability that a sample contains one or more outside observations, analogous to the experimentwise error rate in simultaneous inference) to vary, some us...","n":0.073}}},{"i":765,"$":{"0":{"v":"Diagnosing design problems in object oriented systems","n":0.378},"1":{"v":"Software decay is a phenomenon that plagues aging software systems. While in recent years, there has been significant progress in the area of automatic detection of \"code smells\" on one hand, and code refactorings on the other hand, we claim that existing restructuring practices are seriously hampered by their symptomatic and informal (non-repeatable) nature. This paper makes a clear distinction between structural problems and structural symptoms (also known as code smells), and presents a novel, causal approach to restructuring object oriented systems. Our approach is based on two innovations: the encapsulation of correlations of symptoms and additional contextual information into higher-level design problems, and the univocal, explicit mapping of problems to unique refactoring solutions. Due to its explicit, repeatable nature, the approach shows high potential for increased levels of automation in the restructuring process, and consequently a decrease in maintenance costs.","n":0.084}}},{"i":766,"$":{"0":{"v":"Benchmarking Technical Quality of Software Products","n":0.408},"1":{"v":"To enable systematic comparison of technical quality of (groups of) software products, we have collected measurement data of a wide range of systems into a benchmark repository. The measurements were taken over the course of several years of delivering software assessment services to corporations and public institutes. The granularity of the collected data follows the layered structure of a model for software product quality, based on the ISO/IEC 9126 international standard, which we developed previously. In this paper, we describe the design of our benchmark repository, and we explain how it can be used to perform comparisons of systems. To provide a concrete illustration of the concept without revealing confidential data, we use a selection of open source systems as example.","n":0.091}}},{"i":767,"$":{"0":{"v":"A Survey on the Importance of Object-Oriented Design Best Practices","n":0.316},"1":{"v":"To measure object-oriented design quality, metric-based approaches have been established. These have then been enhanced by identifying design smells in code. While these approaches are useful for identifying hot spots that should be refactored, they are still too vague to sufficiently guide software developers to implement improvements. This is why our previous work focuses on measuring the compliance of source code with object-oriented design best practices. These design best practices were systematically derived from the literature and can be mapped to design principles, which can help reveal fundamental object-oriented design issues in a software product. Despite the successful applications of this approach in industrial and open source projects, there is little accepted knowledge about the importance of various design best practices. Consequently, this paper shows the result of an online survey aimed at identifying the importance of 49 design best practices on design quality. In total, 214 people participated in the survey, resulting in an average of 138 opinions for each practice. Based on these opinions, five very important, 21 important, 12 moderately important and 11 unimportant design best practices could be derived. This information about importance helps managing design improvements in a focused way.","n":0.072}}},{"i":768,"$":{"0":{"v":"Measuring, Assessing and Improving SoftwareQuality based on Object-Oriented DesignPrinciples","n":0.333}}},{"i":769,"$":{"0":{"v":"Applying and evaluating concern-sensitive design heuristics","n":0.408},"1":{"v":"Manifestation of crosscutting concerns in software systems is often an indicative of design modularity flaws and further design instabilities as those systems evolve. Without proper design evaluation mechanisms, the identification of harmful crosscutting concerns can become counter-productive and impractical. Nowadays, metrics and heuristics are the basic mechanisms to support their identification and classification either in object-oriented or aspect-oriented programs. However, conventional mechanisms have a number of limitations to support an effective identification and classification of crosscutting concerns in a software system. In this paper, we claim that those limitations are mostly caused by the fact that existing metrics and heuristics are not sensitive to primitive concern properties, such as either their degree of tangling and scattering or their specific structural shapes. This means that modularity assessment is rooted only at conventional attributes of modules, such as module cohesion, coupling and size. This paper proposes a representative suite of concern-sensitive heuristic rules. The proposed heuristics are supported by a prototype tool. The paper also reports an exploratory study to evaluate the accuracy of the proposed heuristics by applying them to seven systems. The results of this exploratory analysis give evidences that the heuristics offer support for: (i) addressing the shortcomings of conventional metrics-based assessments, (ii) reducing the manifestation of false positives and false negatives in modularity assessment, (iii) detecting sources of design instability, and (iv) finding the presence of design modularity flaws in both object-oriented and aspect-oriented programs. Although our results are limited to a number of decisions we made in this study, they indicate a promising research direction. Further analyses are required to confirm or refute our preliminary findings and, so, this study should be seen as a stepping stone on understanding how concerns can be useful assessment abstractions. We conclude this paper by discussing the limitations of this exploratory study focusing on some situations which hinder the accuracy of concern-sensitive heuristics.","n":0.057}}},{"i":770,"$":{"0":{"v":"IDS: An Immune-Inspired Approach for the Detection of Software Design Smells","n":0.302},"1":{"v":"We propose a parallel between object-oriented system designs and living creatures. We suggest that, like any living creature, system designs are subject to diseases, which are design smells (code smells and anti patterns). Design smells are conjectured in the literature to impact the quality and life of systems and, therefore, their detection has drawn the attention of both researchers and practitioners with various approaches. With our parallel, we propose a novel approach built on models of the immune system responses to pathogenic material. We show that our approach can detect more than one smell at a time. We build and test our approach on Gantt Project v1.10.2 and Xerces v2.7.0, for which manually-validated and publicly available smells exist. The results show a significant improvement in detection time, precision, and recall, in comparison to the state–of–the–art approaches.","n":0.086}}},{"i":771,"$":{"0":{"v":"Maintaining mental models: a study of developer work habits","n":0.333},"1":{"v":"To understand developers' typical tools, activities, and practices and their satisfaction with each, we conducted two surveys and eleven interviews. We found that many problems arose because developers were forced to invest great effort recovering implicit knowledge by exploring code and interrupting teammates and this knowledge was only saved in their memory. Contrary to expectations that email and IM prevent expensive task switches caused by face-to-face interruptions, we found that face-to-face communication enjoys many advantages. Contrary to expectations that documentation makes understanding design rationale easy, we found that current design documents are inadequate. Contrary to expectations that code duplication involves the copy and paste of code snippets, developers reported several types of duplication. We use data to characterize these and other problems and draw implications for the design of tools for their solution.","n":0.087}}},{"i":772,"$":{"0":{"v":"An Exploratory Study of How Developers Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks","n":0.25},"1":{"v":"Much of software developers' time is spent understanding unfamiliar code. To better understand how developers gain this understanding and how software development environments might be involved, a study was performed in which developers were given an unfamiliar program and asked to work on two debugging tasks and three enhancement tasks for 70 minutes. The study found that developers interleaved three activities. They began by searching for relevant code both manually and using search tools; however, they based their searches on limited and misrepresentative cues in the code, environment, and executing program, often leading to failed searches. When developers found relevant code, they followed its incoming and outgoing dependencies, often returning to it and navigating its other dependencies; while doing so, however, Eclipse's navigational tools caused significant overhead. Developers collected code and other information that they believed would be necessary to edit, duplicate, or otherwise refer to later by encoding it in the interactive state of Eclipse's package explorer, file tabs, and scroll bars. However, developers lost track of relevant code as these interfaces were used for other tasks, and developers were forced to find it again. These issues caused developers to spend, on average, 35 percent of their time performing the mechanics of navigation within and between source files. These observations suggest a new model of program understanding grounded in theories of information foraging and suggest ideas for tools that help developers seek, relate, and collect information in a more effective and explicit manner","n":0.064}}},{"i":773,"$":{"0":{"v":"Experiences gamifying developer adoption of practices and tools","n":0.354},"1":{"v":"As software development practices evolve, toolsmiths face the continuous challenge of getting developers to adopt new practices and tools. We tested an idea with industrial software developers that adding game-like feedback to the development environment would improve adoption of tools and practices for code navigation. We present results from a pre-study survey of 130 developers' opinions on gamification and motivation, usage data from a study with an intact team of six developers of a game on code navigation practices, and feedback collected in post-study interviews. Our pre-study survey showed that most developers were interested in gamification, though some have strong negative opinions. Study results show that two of the six study developers adjusted their practices when presented with competitive game elements.","n":0.091}}},{"i":774,"$":{"0":{"v":"An Essay on Software Reuse","n":0.447},"1":{"v":"This paper explores software reuse. It discusses briefly some economic incentives for developing effective software reuse technology and notes that different kinds of software reuse, such as direct use without modification and reuse of abstract software modules after refinement, have different technological implications. It sketches some problem areas to be addressed if we are to achieve the goal of devising practical software reuse systems. These include information retrieval problems and finding effective methods to aid us in understanding how programs work. There is a philosophical epilogue which stresses the importance of having realistic expectations about the benefits of software reuse.","n":0.1}}},{"i":775,"$":{"0":{"v":"What Programmers Really Do - An Observational Study.","n":0.354}}},{"i":776,"$":{"0":{"v":"Artificial Intelligence: A Modern Approach","n":0.447},"1":{"v":"The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.","n":0.099}}},{"i":777,"$":{"0":{"v":"An empirical comparison of seven programming languages","n":0.378},"1":{"v":"Often heated, debates regarding different programming languages' effectiveness remain inconclusive because of scarce data and a lack of direct comparisons. The author addresses that challenge, comparatively analyzing 80 implementations of the phone-code program in seven different languages (C, C++, Java, Perl, Python, Rexx and Tcl). Further, for each language, the author analyzes several separate implementations by different programmers. The comparison investigates several aspects of each language, including program length, programming effort, runtime efficiency, memory consumption, and reliability. The author uses comparisons to present insight into program language performance.","n":0.107}}},{"i":778,"$":{"0":{"v":"Extreme programming: a survey of empirical data from a controlled case study","n":0.289},"1":{"v":"Extreme programming (XP) is a well known agile software development method. While a number of experience reports have been published in recent years, agile software development in general and XP in particular have strongly been criticized for the lack of empirical data. This paper reports a survey of the empirical data obtained from a controlled case study on extreme programming in practical settings. Thus, no hypotheses were set a priori. Four software engineers were acquired to implement a Web-based system (7698 Locs, 820 hours) for data management in a delivery schedule of eight weeks. Development environment was close to the agile home ground. Collected empirical data is grounded on three basic data points: time, size and defects. Data is organized around five system releases, each which were tested by 17 customer testers. System release defect-density was 1.43 defects/KLOC, team overall productivity 16.90 Locs/hour and rework costs were 9.8% of the total development effort. The implications of this study are discussed.","n":0.079}}},{"i":779,"$":{"0":{"v":"SEAgle: Effortless Software Evolution Analysis","n":0.447},"1":{"v":"The analysis of software evolution by means of mining public repositories has been established as one of the dominant approaches for empirical studies in software engineering. However, even the investigation of the simplest research question demands a mazy process involving installation and configuration of tools, climbing their learning curve and tedious collection of desired information. Acknowledging the need for effortless querying of remote repositories we introduce a Web-based 'one-click approach' to perform software evolution analysis of Git projects.","n":0.113}}},{"i":780,"$":{"0":{"v":"Response Surface Methodology: Process and Product Optimization Using Designed Experiments","n":0.316},"1":{"v":"From the Publisher:\r\nUsing a practical approach, it discusses two-level factorial and fractional factorial designs, several aspects of empirical modeling with regression techniques, focusing on response surface methodology, mixture experiments and robust design techniques. Features numerous authentic application examples and problems. Illustrates how computers can be a useful aid in problem solving. Includes a disk containing computer programs for a response surface methodology simulation exercise and concerning mixtures.","n":0.122}}},{"i":781,"$":{"0":{"v":"The Art of Computer Programming","n":0.447}}},{"i":782,"$":{"0":{"v":"Genetic Algorithms and Grouping Problems","n":0.447},"1":{"v":"From the Publisher:\r\nA reader-friendly introduction to the exciting, vast potential of Genetic Algorithms.\r\nThe book gives readers a general understanding of the concepts underlying the technology, an insight into its perceived benefits and failings, and a clear and practical illustration of how optimization problems can be solved more efficiently using Falkenauer's new class of algorithms.","n":0.136}}},{"i":783,"$":{"0":{"v":"Object-oriented analysis and design with applications, third edition","n":0.354},"1":{"v":"Object-Oriented Design with Applications has long been the essential reference to object-oriented technology, which, in turn, has evolved to join the mainstream of industrial-strength software development. In this third edition--the first revision in 13 years--readers can learn to apply object-oriented methods using new paradigms such as Java, the Unified Modeling Language (UML) 2.0, and .NET.The authors draw upon their rich and varied experience to offer improved methods for object development and numerous examples that tackle the complex problems faced by software engineers, including systems architecture, data acquisition, cryptoanalysis, control systems, and Web development. They illustrate essential concepts, explain the method, and show successful applications in a variety of fields. You'll also find pragmatic advice on a host of issues, including classification, implementation strategies, and cost-effective project management.New to this new edition are An introduction to the new UML 2.0, from the notation's most fundamental and advanced elements with an emphasis on key changes New domains and contexts A greatly enhanced focus on modeling--as eagerly requested by readers--with five chapters that each delve into one phase of the overall development lifecycle. Fresh approaches to reasoning about complex systems An examination of the conceptual foundation of the widely misunderstood fundamental elements of the object model, such as abstraction, encapsulation, modularity, and hierarchy How to allocate the resources of a team of developers and mange the risks associated with developing complex software systems An appendix on object-oriented programming languagesThis is the seminal text for anyone who wishes to use object-oriented technology to manage the complexity inherent in many kinds of systems.Sidebarsi¾ i¾ Prefacei¾ Acknowledgments i¾ i¾ About the Authors i¾ i¾ Section I: Conceptsi¾ i¾ Chapter 1: Complexityi¾ i¾ i¾ Chapter 2: The Object Model i¾ i¾ Chapter 3: Classes and Objects i¾ i¾ Chapter 4: Classification i¾ i¾ Section II: Method i¾ Chapter 5: Notation i¾ i¾ Chapter 6: Process Chapter 7: Pragmaticsi¾ i¾ i¾ Chapter 8: System Architecture: Satellite-Based Navigation i¾ i¾ Chapter 9: Control System: Traffic Management i¾ i¾ Chapter 10: Artificial Intelligence: Cryptanalysis i¾ i¾ Chapter 11: Data Acquisition: Weather Monitoring Station i¾ Chapter 12: Web Application: Vacation Tracking System i¾ i¾ i¾ Appendix A: Object-Oriented Programming Languagesi¾ Appendix B: Further Reading i¾ i¾ Notes i¾ i¾ Glossary i¾ i¾ Classified Bibliography i¾ i¾ Index i¾ i¾","n":0.051}}},{"i":784,"$":{"0":{"v":"On parameter tuning in search based software engineering","n":0.354},"1":{"v":"When applying search-based software engineering (SBSE) techniques one is confronted with a multitude of different parameters that need to be chosen: Which population size for a genetic algorithm? Which selection mechanism to use? What settings to use for dozens of other parameters? This problem not only troubles users who want to apply SBSE tools in practice, but also researchers performing experimentation - how to compare algorithms that can have different parameter settings? To shed light on the problem of parameters, we performed the largest empirical analysis on parameter tuning in SBSE to date, collecting and statistically analysing data from more than a million experiments. As case study, we chose test data generation, one of the most popular problems in SBSE. Our data confirm that tuning does have a critical impact on algorithmic performance, and over-fitting of parameter tuning is a dire threat to external validity of empirical analyses in SBSE. Based on this large empirical evidence, we give guidelines on how to handle parameter tuning.","n":0.078}}},{"i":785,"$":{"0":{"v":"Laws of Software Evolution Revisited","n":0.447},"1":{"v":"Data obtained during a 1968 study of the software process [8] led to an investigation of the evolution of OS/360 [13] and and, over a period of twenty years, to formulation of eight Laws of Software Evolution. The FEAST project recently initiated (see sections 4–6 below) is expected to throw additional light on the phenomenology underlying these laws, to increase understanding of them, to explore their finer detail, to expose their wider relevance and implications and to develop means for their beneficial exploitation. This paper is intended to trigger wider interest in the laws and in the FEAST study of feedback and feedback control in the context of the software process and its improvement to ensure beneficial exploitation of their potential.","n":0.091}}},{"i":786,"$":{"0":{"v":"Search-based refactoring: an empirical study","n":0.447},"1":{"v":"Object-oriented systems that undergo repeated addition of functionality commonly suffer a loss of quality in their underlying design. This problem must often be remedied in a costly refactoring phase before further maintenance programming can take place. Recently search-based approaches to automating the task of software refactoring, based on the concept of treating object-oriented design as a combinatorial optimization problem, have been proposed. However, because search-based refactoring is a novel approach it is yet to be established as to which search techniques are most suitable for the task. In this paper we report the results of an empirical comparison of simulated annealing (SA), genetic algorithms (GAs) and multiple ascent hill-climbing (HCM) in search-based refactoring. A prototype automated refactoring tool is employed, capable of making radical changes to the design of an existing program in order that it conforms more closely to a contemporary quality model. Results show HCM to outperform both SA and GA over a set of five input programs. Copyright © 2008 John Wiley & Sons, Ltd.","n":0.077}}},{"i":787,"$":{"0":{"v":"The Art of Computer Programming, 2nd Ed. (Addison-Wesley Series in Computer Science and Information","n":0.267}}},{"i":788,"$":{"0":{"v":"Elegant Object-Oriented Software Design via Interactive, Evolutionary Computation","n":0.354},"1":{"v":"Design is fundamental to software development but can be demanding to perform. Thus, to assist the software designer, evolutionary computing is being increasingly applied using machine-based, quantitative fitness functions to evolve software designs. However, in nature, elegance and symmetry play a crucial role in the reproductive fitness of various organisms. In addition, subjective evaluation has also been exploited in interactive evolutionary computation (IEC). Therefore, to investigate the role of elegance and symmetry in software design, four novel elegance measures are proposed which are based on the evenness of distribution of design elements. In controlled experiments in a dynamic IEC environment, designers are presented with visualizations of object-oriented software designs, which they rank according to a subjective assessment of elegance. For three out of the four elegance measures proposed, it is found that a significant correlation exists between elegance values and reward elicited. These three elegance measures assess the evenness of distribution of 1) attributes and methods among classes; 2) external couples between classes; and 3) the ratio of attributes to methods. It is concluded that symmetrical elegance is in some way significant in software design, and that this can be exploited in dynamic, multiobjective IEC to produce elegant software designs.","n":0.071}}},{"i":789,"$":{"0":{"v":"Putting the developer in-the-loop: an interactive GA for software re-modularization","n":0.316},"1":{"v":"This paper proposes the use of Interactive Genetic Algorithms (IGAs) to integrate developer's knowledge in a re-modularization task. Specifically, the proposed algorithm uses a fitness composed of automatically-evaluated factors--accounting for the modularization quality achieved by the solution--and a human-evaluated factor, penalizing cases where the way re-modularization places components into modules is considered meaningless by the developer.\r\n\r\nThe proposed approach has been evaluated to re-modularize two software systems, SMOS and GESA. The obtained results indicate that IGA is able to produce solutions that, from a developer's perspective, are more meaningful than those generated using the full-automated GA. While keeping feedback into account, the approach does not sacrifice the modularization quality, and may work requiring a very limited set of feedback only, thus allowing its application also for large systems without requiring a substantial human effort.","n":0.087}}},{"i":790,"$":{"0":{"v":"Random-Weighted Search-Based Multi-objective Optimization Revisited","n":0.447},"1":{"v":"Weight-based multi-objective optimization requires assigning appropriate weights using a weight strategy to each of the objectives such that an overall optimal solution can be obtained with a search algorithm. Choosing weights using an appropriate weight strategy has a huge impact on the obtained solutions and thus warrants the need to seek the best weight strategy. In this paper, we propose a new weight strategy called Uniformly Distributed Weights (UDW), which generates weights from uniform distribution, while satisfying a set of user-defined constraints among various cost and effectiveness measures. We compare UDW with two commonly used weight strategies, i.e., Fixed Weights (FW) and Randomly-Assigned Weights (RAW), based on five cost/effectiveness measures for an industrial problem of test minimization defined in the context of Video Conferencing System Product Line developed by Cisco Systems. We empirically evaluate the performance of UDW, FW, and RAW in conjunction with four search algorithms ((1+1) Evolutionary Algorithm (EA), Genetic Algorithm, Alternating Variable Method, and Random Search) using the industrial case study and 500 artificial problems of varying complexity. Results show that UDW along with (1+1) EA achieves the best performance among the other combinations of weight strategies and algorithms.","n":0.072}}},{"i":791,"$":{"0":{"v":"Placement of Entities in Object-Oriented Systems by Means of a Single-Objective Genetic Algorithm","n":0.277},"1":{"v":"Behavior and state allocation in object-oriented systems is a rather non-trivial task that is hard to master and automate since it is guided by conceptual criteria and therefore relies on human expertise. Since attributes and methods can be placed in the classes of a system in uncountable different ways, the task can be regarded as a search space exploration problem. In this paper we present our experience from treating this issue by a genetic algorithm, which in contrast to previous approaches, is aiming at single-objective optimization. The fitness function is based on a novel metric which ensures that optimization improves both coupling and cohesion. The approach has been implemented as an Eclipse plugin allowing the effortless experimentation on any system. The evaluation results indicate that the problem is suitable for single-objective genetic algorithms and that an optimal or near-optimal solution can be obtained within reasonable time.","n":0.083}}},{"i":792,"$":{"0":{"v":"Principal component analysis","n":0.577},"1":{"v":"Principal component analysis PCA is a multivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the PCA model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. PCA can be generalized as correspondence analysis CA in order to handle qualitative variables and as multiple factor analysis MFA in order to handle heterogeneous sets of variables. Mathematically, PCA depends upon the eigen-decomposition of positive semi-definite matrices and upon the singular value decomposition SVD of rectangular matrices. Copyright © 2010 John Wiley & Sons, Inc.","n":0.084}}},{"i":793,"$":{"0":{"v":"Experimentation in software engineering: an introduction","n":0.408},"1":{"v":"The purpose of Experimentation in Software Engineering: An Introduction is to introduce students, teachers, researchers, and practitioners to experimentation and experimental evaluation with a focus on software engineering. The objective is, in particular, to provide guidelines for performing experiments evaluating methods, techniques and tools in software engineering. The introduction is provided through a process perspective. The focus is on the steps that we go through to perform experiments and quasi-experiments. The process also includes other types of empirical studies. The motivation for the book emerged from the need for support we experienced when turning our software engineering research more experimental. Several books are available which either treat the subject in very general terms or focus on some specific part of experimentation; most focus on the statistical methods in experimentation. These are important, but there were few books elaborating on experimentation from a process perspective, none addressing experimentation in software engineering in particular. The scope of Experimentation in Software Engineering: An Introduction is primarily experiments in software engineering as a means for evaluating methods, techniques and tools. The book provides some information regarding empirical studies in general, including both case studies and surveys. The intention is to provide a brief understanding of these strategies and in particular to relate them to experimentation. Experimentation in Software Engineering: An Introduction is suitable for use as a textbook or a secondary text for graduate courses, and for researchers and practitioners interested in an empirical approach to software engineering. (Less)","n":0.064}}},{"i":794,"$":{"0":{"v":"Biometry: The Principles and Practice of Statistics in Biological Research","n":0.316},"1":{"v":"1. Introduction 2. Data in Biology 3. Computers and Data Analysis 4. Descriptive Statistics 5. Introduction to Probability Distributions 6. The Normal Probability Distribution 7. Hypothesis Testing and Interval Estimation 8. Introduction to Analysis of Variance 9. Single-Classification Analysis of Variance 10. Nested Analysis of Variance 11. Two-Way and Multiway Analysis of Variance 12. Statistical Power and Sample Size in the Analysis of Variance 13. Assumptions of Analysis of Variance 14. Linear Regression 15. Correlation 16. Multiple and Curvilinear Regression 17. Analysis of Frequencies 18. Meta-Analysis and Miscellaneous Methods","n":0.106}}},{"i":795,"$":{"0":{"v":"A Discipline of Programming","n":0.5}}},{"i":796,"$":{"0":{"v":"Aspect-Oriented Software Development with Use Cases","n":0.408},"1":{"v":"Preface. Acknowledgments. I. THE CASE FOR USE CASES AND ASPECTS. 1. Problem to Attack. The Use of Components Today. Limitation of Components. Approaching a Solution. Keeping Concerns Separate. 2. Attacking the Problem with Aspects. Approaching a Solution with Aspects. Keeping Peers Separate with Aspects. Keeping Extensions Separate with Aspects. Need for Methodological Guidance. 3. Today with Use Cases. Use Cases in Brief. Use-Case-Driven Development. Roles and Benefits of Use Cases. Gaps in the Use-Case Technique. Bridging the Gaps with Aspects. 4. Tomorrow with Use-Case Modules. Building Systems in Overlays with Use-Case Slices. Keeping Peer Use Cases Separate. Keeping Extension Use Cases Separate. Developing with Use-Case Modules. II. MODELING AND CAPTURING CONCERNS WITH USE CASES. 5. Modeling Concerns with Use Cases. Use-Case Modeling. Use-Case Instances and Flows of Events. Describing Use Cases. Visualizing Use-Case Flows. Summary and Highlights. 6. Structuring Use Cases. Use-Case Relationships. Use-Case Extend Relationship. Use-Case Include Relationship. Use-Case Generalization. Utility Use Cases. Summary and Highlights. 7. Capturing Concerns with Use Cases. Understanding Stakeholder Concerns. Capturing Application Use Cases. Capturing Infrastructure Use Cases. Summary and Highlights. III. KEEPING CONCERNS SEPARATE WITH USE-CASE MODULES. 8. Keeping Peer Use-Case Realizations Separate with Aspects. Realizing Peer Use Cases. Keeping Use-Case Specifics Separate. Dealing with Overlap. Summary and Highlights. 9. Keeping Extensions Separate with Pointcuts. Realizing Extension Use Cases. Keeping Modularity of Extension Use-Case Realizations. Parameterizing Pointcuts. Generalizing Extension Use-Case Realizations. Templating Use-Case Slices. Summary and Highlights. 10. Building Systems with Use-Case Modules. A System Comprises Models. Use-Case Model. Analysis Model. Design and Implementation Models. Use-Case Modules Cut Across Models. Composing and Configuring Use-Case Modules. Summary and Highlights. IV. ESTABLISHING AN ARCHITECTURE BASED ON USE CASES AND ASPECTS. 11. Road to a Resilient Architecture. What Is Architecture? What Is a Good Architecture? Steps to Establish an Architecture Baseline. Begin with a Platform-Independent Structure. Overlay Platform Specifics on Top. Summary and Highlights. 12. Separating Functional Requirements with Application Peer Use Cases. Analyzing Application Use Cases. Keeping Application Use Cases Separate. Designing Application Use Cases. Refining Design Elements. Summary and Highlights. 13. Separating Functional Requirements with Application-Extension Use Cases. Analyzing Application-Extension Use Cases. Keeping Application-Extension Use Cases Separate. Designing Application-Extension Use Cases. Dealing with Changes in the Base. Summary and Highlights. 14. Separating Nonfunctional Requirements with Infrastructure Use Cases. Analyzing an Infrastructure Use Case. Keeping Infrastructure Use Cases Separate. Designing Infrastructure Use Cases. Dealing with Multiple Infrastructure Use Cases. Summary and Highlights. 15. Separating Platform Specifics with Platform-Specific Use-Case Slices. Keeping Platform Specifics Separate. Overlaying User Interfaces. Overlaying Distribution. Overlaying Persistency. Preserving the Use-Case Structure. Summary and Highlights. 16. Separating Tests with Use-Case Test Slices. Test-First Approach. Identifying Test Cases from Use Cases. Identifying Elements to Be Tested. Designing and Implementing Tests. Summary and Highlights. 17. Evaluating the Architecture. Putting It Together. Evaluating Separation of Concerns. Evaluating and Achieving Systemwide Concerns. Summary and Highlights. 18. Describing the Architecture. Architecture Description Comprises Architectural Views. Architectural View of the Use-Case Model. Architectural View of the Analysis Model. Architectural View of the Design Model. Summary and Highlights. V. APPLYING USE CASES AND ASPECTS IN A PROJECT. 19. Running a Project. Iterative Development. Estimating Development Effort. Planning and Controlling the Project. Productivity Gains by Keeping Concerns Separate. Summary and Highlights. 20. Tailoring the Approach. Achieving the Right Balance. Selecting Disciplines to Apply. Adopting at Different Phases of a Project. Summary and Highlights. 21. Aspects and Beyond. Building a System in Extensions. Balancing Best Practices. The Road Ahead. Appendix A. Modeling Aspects and Use-Case Slices in UML. Appendix B. Notation Guide. References. Glossary. Index.","n":0.041}}},{"i":797,"$":{"0":{"v":"Evolving software product lines with aspects: an empirical study on design stability","n":0.289},"1":{"v":"Software product lines (SPLs) enable modular, large-scale reuse through a software architecture addressing multiple core and varying features. To reap the benefits of SPLs, their designs need to be stable. Design stability encompasses the sustenance of the product line's modularity properties in the presence of changes to both the core and varying features. It is usually assumed that aspect-oriented programming promotes better modularity and changeability of product lines than conventional variability mechanisms, such as conditional compilation. However, there is no empirical evidence on its efficacy to prolong design stability of SPLs through realistic development scenarios. This paper reports a quantitative study that evolves two SPLs to assess various design stability facets of their aspect-oriented implementations. Our investigation focused upon a multi-perspective analysis of the evolving product lines in terms of modularity, change propagation, and feature dependency. We have identified a number of scenarios which positively or negatively affect the architecture stability of aspectual SPLs.","n":0.081}}},{"i":798,"$":{"0":{"v":"Integrating feature modeling with the RSEB","n":0.408},"1":{"v":"We have integrated the feature modeling of Feature-Oriented Domain Analysis (FODA) into the processes and work products of the Reuse-Driven Software Engineering Business (RSEB). The RSEB is a use case driven systematic reuse process: architecture and reusable subsystems are first described by use cases and then transformed into object models that are traceable to these use cases. Variability in the RSEB is captured by structuring use case and object models using explicit variation points and variants. Traditional domain engineering steps have been distributed into the steps of the architectural and component system development methods of the RSEB. But the RSEB prescribes no explicit models of the essential features that characterize the different versions. Building on our experience in applying FODA and RSEB to the telecom domain, we have added explicit domain engineering steps and an explicit feature model to the RSEB to support domain engineering and component reuse. These additions provide an effective reuse oriented model as a 'catalog' capability to link use cases, variation points, reusable components and configured applications.","n":0.076}}},{"i":799,"$":{"0":{"v":"Do Crosscutting Concerns Cause Defects","n":0.447},"1":{"v":"There is a growing consensus that crosscutting concerns harm code quality. An example of a crosscutting concern is a functional requirement whose implementation is distributed across multiple software modules. We asked the question, \"How much does the amount that a concern is crosscutting affect the number of defects in a program?\" We conducted three extensive case studies to help answer this question. All three studies revealed a moderate to strong statistically significant correlation between the degree of scattering and the number of defects. This paper describes the experimental framework we developed to conduct the studies, the metrics we adopted and developed to measure the degree of scattering, the studies we performed, the efforts we undertook to remove experimental and other biases, and the results we obtained. In the process, we have formulated a theory that explains why increased scattering might lead to increased defects.","n":0.083}}},{"i":800,"$":{"0":{"v":"On the Reuse and Maintenance of Aspect-Oriented Software: An Assessment Framework","n":0.302},"1":{"v":"Aspect-oriented software development (AOSD) is gaining wide attention both in research environments and in industry. Aspect-oriented systems encompass new software engineering abstractions and different complexity dimensions. As a consequence, AOSD poses new problems to empirical software engineering. It requires new assessment frameworks specifically tailored to measure the reusability and maintainability degrees of aspect-oriented systems. This paper presents an assessment framework for AOSD, which is composed of two components: a suite of metrics and a quality model. These components are based on well-known principles and existing metrics in order to avoid the reinvention of well-tested solutions. The proposed framework has been evaluated in the context of two different empirical studies with different characteristics, diverse domains, varying control levels and different complexity degrees. Based on empirical and quantitative analysis, the advantages and drawbacks of the framework components are discussed.","n":0.085}}},{"i":801,"$":{"0":{"v":"On the impact of aspectual decompositions on design stability: an empirical study","n":0.289},"1":{"v":"Although one of the main promises of aspect-oriented (AO) programming techniques is to promote better software changeability than objectoriented (OO) techniques, there is no empirical evidence on their efficacy to prolong design stability in realistic development scenarios. For instance, no investigation has been performed on the effectiveness of AO decompositions to sustain overall system modularity and minimize manifestation of ripple-effects in the presence of heterogeneous changes. This paper reports a quantitative case study that evolves a real-life application to assess various facets of design stability of OO and AO implementations. Our evaluation focused upon a number of system changes that are typically performed during software maintenance tasks. They ranged from successive re-factorings to more broadly-scoped software increments relative to both crosscutting and non-crosscutting concerns. The study included an analysis of the application in terms of modularity, change propagation, concern interaction, identification of ripple-effects and adherence to well-known design principles.","n":0.082}}},{"i":802,"$":{"0":{"v":"Leveraging legacy system dollars for e-business","n":0.408},"1":{"v":"Although many firms have rapidly and enthusiastically adopted distributed architectures, many more are stuck with mainframe based mission-critical systems that continue to isolate them from their partner, supplier, and customer systems. Most companies want to transform their applications to meet new business demands, but because legacy systems tend to be unwieldy, monolithic, and inflexible, many firms regard modernization as somewhere between improbable and impossible. Reeling from the Y2K debacle and saddled with years of application backlog, the most these companies can hope for is to keep their legacy system alive. And keeping it alive is getting more expensive. It is also becoming harder to find qualified personnel to do the maintenance. All of this makes it difficult to add new functionality and keep up with business requirements. The ideal solution is to transform legacy systems to newer, more productive platforms so that companies can exploit faster and cheaper development technologies, like Java and XML (Extensible Markup Language). The focus then shifts to functionality, not the infrastructure, which means a company can respond more quickly to its changing business requirements and technology enhancements. RescueWare, legacy transformation software from Relativity Technologies, breaks business knowledge into stand-alone pieces, or e-components. The e-components are basically collections of objects that perform specific business services, have clearly defined application program interfaces (APIs), and are accessible through modern industry-standard protocols.","n":0.067}}},{"i":803,"$":{"0":{"v":"Discovering early aspects","n":0.577},"1":{"v":"Aspect-oriented software development has focused on the software life cycle's implementation phase: developers identify and capture aspects mainly in code. But aspects are evident earlier in the life cycle, such as during requirements engineering and architecture design. Early aspects are concerns that crosscut an artifact's dominant decomposition or base modules derived from the dominant separation-of-concerns criterion, in the early stages of the software life cycle. In this article, we describe how to identify and capture early aspects in requirements and architecture activities and how they're carried over from one phase to another. We'll focus on requirements and architecture design activities to illustrate the points, but the same ideas apply in other phases as well, such as domain analysis or in the fine-grained design activities that lie between architecture and implementation","n":0.088}}},{"i":804,"$":{"0":{"v":"Evolution of the linux kernel variability model","n":0.378},"1":{"v":"Understanding the challenges faced by real projects in evolving variability models, is a prerequisite for providing adequate support for such undertakings. We study the evolution of a model describing features and configurations in a large product line--the Linux kernel variability model. We analyze this evolution quantitatively and qualitatively.\r\n\r\nOur primary finding is that the Linux kernel model appears to evolve surprisingly smoothly. In the analyzed period, the number of features had doubled, and still the structural complexity of the model remained roughly the same. Furthermore, we provide an in-depth look at the effect of the kernel's development methodologies on the evolution of its model. We also include evidence about edit operations applied in practice, evidence of challenges in maintaining large models, and a range of recommendations (and open problems) for builders of modeling tools.","n":0.087}}},{"i":805,"$":{"0":{"v":"The PLUSS approach: domain modeling with features, use cases and use case realizations","n":0.277},"1":{"v":"This paper describes a product line use case modeling approach tailored towards organizations developing and maintaining extremely long lived software intensive systems. We refer to the approach as the PLUSS approach, Product Line Use case modeling for Systems and Software engineering . An industrial case study is presented where PLUSS is applied and evaluated in the target domain. Based on the case study data we draw the conclusion that PLUSS performs better than modeling according to the styles and guidelines specified by the IBM-Rational Unified Process (RUP) in the current industrial context.","n":0.104}}},{"i":806,"$":{"0":{"v":"Use Cases and Aspects - Working Seamlessly Together","n":0.354},"1":{"v":"Aspect oriented programming (AOP) is “the missing link” to allow you slice a system, use case by use case, over “all” lifecycle models. This will dramatically change the way complex systems are understood, how new features are added to systems, and how systems are implemented and tested. AOP will also add a new dimension of reuse to software development. And it is here to be harvested—now.","n":0.123}}},{"i":807,"$":{"0":{"v":"Distribution Map","n":0.707},"1":{"v":"Understanding large software systems is a challenging task, and to support it many approaches have been developed. Often, the result of these approaches categorize existing entities into new groups or associates them with mutually exclusive properties. In this paper we present the distribution map as a generic technique to visualize and analyze this type of result. Our technique is based on the notion of focus, which shows whether a property is well-encapsulated or cross-cutting, and the notion of spread, which shows whether the property is present in several parts of the system. We present a basic visualization and complement it with measurements that quantify focus and spread. To validate our technique we show evidence of applying it on the result sets of different analysis approaches. As a conclusion we propose that the distribution map technique should belong to any reverse engineering toolkit","n":0.084}}},{"i":808,"$":{"0":{"v":"An empirical analysis of the impact of software development problem factors on software maintainability","n":0.267},"1":{"v":"Many problem factors in the software development phase affect the maintainability of the delivered software systems. Therefore, understanding software development problem factors can help in not only reducing the incidence of project failure but can also ensure software maintainability. This study focuses on those software development problem factors which may possibly affect software maintainability. Twenty-five problem factors were classified into five dimensions; a questionnaire was designed and 137 software projects were surveyed. A K-means cluster analysis was performed to classify the projects into three groups of low, medium and high maintainability projects. For projects which had a higher level of severity of problem factors, the influence on software maintainability becomes more obvious. The influence of software process improvement (SPI) on project problems and the associated software maintainability was also examined in this study. Results suggest that SPI can help reduce the level of severity of the documentation quality and process management problems, and is only likely to enhance software maintainability to a medium level. Finally, the top 10 list of higher-severity software development problem factors was identified, and implications were discussed.","n":0.074}}},{"i":809,"$":{"0":{"v":"AOSD Ontology 1.0: Public Ontology of Aspect-Orientation","n":0.378},"1":{"v":"This report presents a Common Foundation for Aspect-Oriented Software Development. A Common Foundation is required to enable effective communication and to enable integration of activities within the Network of Excellence. This Common Foundation is realized by developing an ontology, i.e. the shared meaning of terms and concepts in the domain of AOSD. In the first part of this report, we describe the definitions of an initial set of common AOSD terms. There is general agreement on these definitions. In the second part, we describe the Common Foundation task in detail.","n":0.105}}},{"i":810,"$":{"0":{"v":"On the modularity of software architectures: a concern-driven measurement framework","n":0.316},"1":{"v":"Much of the complexity of software architecture design is derived from the inadequate modularization of key broadly-scoped concerns, such as exception handling, distribution, and persistence. However, conventional architecture metrics are not sensitive to the driving architectural concerns, thereby leading a number of false positives and false negatives in the design assessment process. Therefore, there is a need for assessment techniques that support a more effective identification of early design modularity anomalies relative to crosscutting concerns. In this context, this paper proposes a concern-driven measurement framework for assessing architecture modularity. It encompasses a mechanism for documenting architectural concerns, and a suite of concern-oriented architecture metrics. We evaluated the usefulness of the proposed framework while comparing the modularity of architecture design alternatives in three different case studies.","n":0.089}}},{"i":811,"$":{"0":{"v":"Modeling volatile concerns as aspects","n":0.447},"1":{"v":"A rapidly changing market leads to software systems with highly volatile requirements. These must be managed in a way that reduces the time and costs associated with updating a system to meet these new requirements. By externalizing volatile concerns, we can build a stepping-stone for future management of unanticipated requirements change. In this paper, we present a method for handling volatile concerns during early lifecycle software modeling. The key insight is that aspect-oriented techniques can be applied to modularize volatility and to weave volatile concerns into the base software artifacts.","n":0.105}}},{"i":812,"$":{"0":{"v":"The Linux Kernel Configurator as a Feature Modeling Tool.","n":0.333},"1":{"v":"In order to contribute to the understanding of how the SPL community and the open source community can benefit from each other, we present the Linux Kernel Configurator (LKC). We describe its capabilities and explain how it can be used for the design of feature models.","n":0.147}}},{"i":813,"$":{"0":{"v":"Using design patterns and constraints to automate the detection and correction of inter-class design defects","n":0.258},"1":{"v":"Developing code free of defects is a major concern for the object oriented software community. The authors classify design defects as those within classes (intra-class), those among classes (inter-classes), and those of semantic nature (behavioral). Then, we introduce guidelines to automate the detection and correction of inter-class design defects. We assume that design patterns embody good architectural solutions and that a group of entities with organization similar, but not equal, to a design pattern represents an inter-class design defect. Thus, the transformation of such a group of entities, such that its organization complies exactly with a design pattern, corresponds to the correction of an inter-class design defect. We use a meta-model to describe design patterns and we exploit the descriptions to infer sets of detection and transformation rules. A constraint solver with explanations uses the descriptions and rules to recognize groups of entities with organizations similar to the described design patterns. A transformation engine modifies the source code to comply with the recognized distorted design patterns. We apply these guidelines on the Composite pattern using PTIDEJ, our prototype tool that integrates the complete guidelines.","n":0.074}}},{"i":814,"$":{"0":{"v":"Assessing the precision of FindBugs by mining Java projects developed at a university","n":0.277},"1":{"v":"Software repositories are analyzed to extract useful information on software characteristics. One of them is external quality. A technique used to increase software quality is automatic static analysis, by means of bug finding tools. These tools promise to speed up the verification of source code; anyway, there are still many problems, especially the high number of false positives, that hinder their large adoption in software development industry. We studied the capability of a popular bug-finding tool, FindBugs, for defect prediction purposes, analyzing the issues revealed on a repository of university Java projects. Particularly, we focused on the percentage of them that indicates actual defects with respect to their category and priority, and we ranked them. We found that a very limited set of issues have high precision and therefore have a positive impact on code external quality.","n":0.085}}},{"i":815,"$":{"0":{"v":"Aspect-Oriented Requirements Engineering","n":0.577},"1":{"v":"Broadly-scoped requirements such as security, privacy, and response time are a major source of complexity in modern software systems. This is due to their tangled inter-relationships withand effects on other requirements. Aspect-Oriented Requirements Engineering (AORE) aims to facilitate modularisation of such broadly-scoped requirements, so that softwaredevelopers are able to reason about them in isolation - one at a time. AORE also captures these inter-relationships and effects in well-defined composition specifications, and, in sodoing exposes the causes for potential conflicts, trade-offs, and roots for the key earlyarchitectural decisions. Over the last decade, significant work has been carried out in the field of AORE. With this book the editors aim to provide a consolidated overview of these efforts and results. The individual contributions discuss how aspects can be identified, represented, composed and reasoned about, as well as how they are used in specific domains and in industry. Thus, the book does not present one particular AORE approach, but conveys a broad understanding of the aspect-oriented perspective on requirements engineering. The chaptersare organized into five sections: concern identification in requirements, concern modelling and composition, domain-specific use of AORE, aspect interactions, and AORE inindustry. This book provides readers with the most comprehensive coverage of AORE and the capabilities it offers to those grappling with the complexity arising from broadly-scopedrequirements - a phenomenon that is, without doubt, universal across software systems. Software engineers and related professionals in industry, as well as advanced undergraduateand post-graduate students and researchers, will benefit from these comprehensive descriptions and the industrial case studies.","n":0.063}}},{"i":816,"$":{"0":{"v":"A Model-driven Approach for Software Product Lines Requirements Engineering","n":0.333},"1":{"v":"UML and feature models complement each other well and can be the base techniques for a systematic method to identify and model software product line (SPL) requirements. In this paper, we present a modeldriven approach to trace both features and UML requirements analysis model elements, and to automatically derive valuable models for domain and application engineering. The resulting contribution is a synergetic approach for SPL requirements. We illustrate it by using a home automation system product line.","n":0.114}}},{"i":817,"$":{"0":{"v":"A dataset of feature additions and feature removals from the Linux kernel","n":0.289},"1":{"v":"This paper describes a dataset of feature additions and removals in the Linux kernel evolution history, spanning over seven years of kernel development. Features, in this context, denote configurable system options that users select when creating customized kernel images. The provided dataset is the largest corpus we are aware of capturing feature additions and removals, allowing researchers to assess the kernel evolution from a feature-oriented point-of-view. Furthermore, the dataset can be used to better understand how features evolve over time, and how different artifacts change as a result. One particular use of the dataset is to provide a real-world case to assess existing support for feature traceability and evolution. In this paper, we detail the dataset extraction process, the underlying database schema, and example queries. The dataset is directly available at our Bitbucket repository: https://bitbucket.org/lpassos/kconfigdb","n":0.086}}},{"i":818,"$":{"0":{"v":"Change Impact Analysis of Crosscutting in Software Architectural Design","n":0.333},"1":{"v":"Software architectures should be amenable to changes in user requirements and implementation technology. The analysis of the impact of these changes can be based on traceability of architectural design elements. Design elements have dependencies with other software artifacts but also evolve in time. Crosscutting dependencies may have a strong influence on modifiability of software architectures. We present an impact analysis of crosscutting dependencies in architectural design. The analysis is supported by a matrix representation of dependencies.","n":0.115}}},{"i":819,"$":{"0":{"v":"Early Analysis of Modularity in Software Product Lines.","n":0.354}}},{"i":820,"$":{"0":{"v":"The crosscutting pattern : a conceptual framework for the analysis of modularity across software development phases","n":0.25},"1":{"v":"Enhancing business performance in contemporary domains requires systems whose size and intricacy challenge most of the current software engineering methods and tools, In this setting, a wide spectrum of methodologies, models, languages and tools have been adopted to deal with the increasing complexity of software systems. Aspect-Oriented Software Development (AOSD) is one of these methodologies that have emerged to tackle the design and development of complex software systems.\nOne of the key principles in AOSD is Separation of Concerns (SOC). Related with this principle is the problem of crosscutting concerns. Crosscutting is usually described in terms of scattering and tangling. However, the distinction between these concepts has been traditionally left to developers' intuition, sometimes leading to ambiguous statements and confusion. In that sense, precise definitions are required for certain research areas, e.g. for the identification of crosscutting concerns or the definition of software metrics. This thesis proposes a conceptual framework that allows the formal definition of the terms of scattering, tangling and crosscutting. The conceptual framework is based on the concept of crosscutting pattern which denotes to the situation where two different domains, called source and target, are related by a traceability link or mapping. The terms of scattering, tangling and crosscutting are defined, thus, as special cases of this mapping. The utilization of this formal definition aims at identifying situations of crosscutting. Since the crosscutting pattern is not tied to any specific deployment artefact, it may be applied to any development phase, allowing the identification of crosscutting at any abstraction level (from requirements to implementation). Moreover, the crosscutting pattern can be applied across several refinement levels enabling traceability of crosscutting concerns.\nUsability of the framework is illustrated by means of applying it to several research areas such as aspect mining, software assessment, identification of crosscutting features in Software Product Lines (SPL) or maintainability analysis. As it is aforementioned, the utilization of the conceptual framework helps to identify crosscutting at any abstraction level. Aspect mining is its main application area. Aspect mining approaches have traditionally focused on the programming level, where architectural decisions have already been made. In this setting, in this thesis an aspect mining process to identify crosscutting is presented. The process extends the conceptual framework by using syntactical and dependency-based analyses to automatically identify trace relations between source and target elements. Although the process may be used at any development phase, its utilization is illustrated at the requirements level. The identification of crosscutting concerns at early stages of development aims at incorporating the benefits of aspect-orientation at the very beginning of the development process. Moreover, early aspect refactoring is given for UML use cases diagrams, improving modularity of the system. Using this refactoring, the system may be easily evolved just using simple composing rules which allow the weaving of base and crosscutting concerns.\nThe conceptual framework also allows the definition of concern driven metrics. These metrics may be used at indications for modularity assessment. Then, using these metrics, the aspect mining process previously presented may be completed. In particular, the metrics enable the application of an empirical analysis of modularity measuring the degree of crosscutting in a system. Again, the metrics presented are language-agnostic so that they are not tied to any development artefact. However, in order to illustrate its applicability, canonical instantiations of the crosscutting metrics are given for use cases in this thesis. The metrics are theoretical and empirically validated. By the theoretical validation, its accuracy for measuring crosscutting properties is demonstrated. This validation is performed by comparing the results obtained by the metrics with those obtained by similar metrics previously introduced in the literature. The results show how the metrics presented generalizes existing ones. They also provide evidences of the need of an specific metric for crosscutting that other metrics suites lack of.\nOn the other hand, by the empirical validaton of the metrics, its utility is demonstated in terms of being related to other software quality attributes in expected ways. In this thesis, this utility has been demonstrated by relating them to two ISO/IEC 9126 maintainability attributes, namely stability and changeability. In particular, a first and original exploratory study is shown which investigates the correlation between early crosscutting metrics and stability or changeability. The results obtained empirically demonstrate how crosscutting negatively affects to software stability and changeability at early stages of development. These results empirically support the ideas introduced by the aspect community (through several years) claiming that crosscutting is usually harmful to software quality.\nThe applicability of the conceptual framework is, finally, illustrated by applying the aspect mining process to identify crosscutting features in the Software Product Line (SPL) domain. Crosscutting is a special kind of dependency that compromises the composition and reutilization of SPL. Then, the identification of these dependencies helps to reduce them by refactoring these crosscutting features using aspect-oriented techniques. The need for introducing aspect-oriented techniques to model variable features in SPL has been introduced by several approaches in the literature. However, most of these approaches lack of a process to identify crosscutting features so that they consider as crosscutting features either all the variable features or the well-known crosscutting features widely identified in the literature. The utilization of the aspect mining process presented in this document automates the identification of the crosscutting features allowing their isolation and refactoring. The concern driven metrics proposed have been also applied to the SPL domain showing the benefits obtained by the assessment of crosscutting in SPL as well, e.g. anticipating the impact of a feature change before it occurs.","n":0.033}}},{"i":821,"$":{"0":{"v":"Naming the Pain in Requirements Engineering: Comparing Practices in Brazil and Germany","n":0.289},"1":{"v":"As part of the Naming the Pain in Requirements Engineering (NaPiRE) initiative, researchers compared problems that companies in Brazil and Germany encountered during requirements engineering (RE). The key takeaway was that in RE, human interaction is necessary for eliciting and specifying high-quality requirements, regardless of country, project type, or company size.","n":0.14}}},{"i":822,"$":{"0":{"v":"Investigating technical debt folklore: shedding some light on technical debt opinion","n":0.302},"1":{"v":"We identified and organized a number of statements about technical debt (TD Folklore list) expressed by practitioners in online websites, blogs and published papers. We chose 14 statements and we evaluated them through two surveys (37 practitioners answered the questionnaires), ranking them by agreement and consensus. The statements most agreed with show that TD is an important factor in software project management and not simply another term for \"bad code\". This study will help the research community in identifying folklore that can be translated into research questions to be investigated, thus targeting attempts to provide a scientific basis for TD management.","n":0.1}}},{"i":823,"$":{"0":{"v":"A Retrospective Study of Software Analytics Projects: In-Depth Interviews with Practitioners","n":0.302},"1":{"v":"Software analytics guide practitioners in decision making throughout the software development process. In this context, prediction models help managers efficiently organize their resources and identify problems by analyzing patterns on existing project data in an intelligent and meaningful manner. Over the past decade, the authors have worked with software organizations to build metric repositories and predictive models that address process-, product-, and people-related issues in practice. This article shares their experience over the years, reflecting the expectations and outcomes both from practitioner and researcher viewpoints.","n":0.108}}},{"i":824,"$":{"0":{"v":"Investigating the Link between User Stories and Documentation Debt on Software Projects","n":0.289},"1":{"v":"Technical debt is a metaphor that describes the effect of immature artefacts in software development. One of its types is documentation debt, which can be identified by locating missing, inadequate or incomplete artefacts in software projects. Nowadays, we can observe more organizations using agile methods to support their activities. In particular, the use of user stories reduces the focus on requirement specification tasks and, as a consequence, creates difficulties that need to be overcame by the development team. In order to investigate these difficulties and assess whether they create a favourable scenario for incurring documentation debt, this paper presents the results of a literature review and an exploratory study. The results from both studies allowed us to identify a list of causes that can lead the development team to incur documentation debt when working with agile requirements. This is an important step in order to manage the technical debt from a preventive perspective.","n":0.081}}},{"i":825,"$":{"0":{"v":"Data Mining","n":0.707}}},{"i":826,"$":{"0":{"v":"Data mining: practical machine learning tools and techniques with Java implementations","n":0.302},"1":{"v":"1. What's It All About? 2. Input: Concepts, Instances, Attributes 3. Output: Knowledge Representation 4. Algorithms: The Basic Methods 5. Credibility: Evaluating What's Been Learned 6. Implementations: Real Machine Learning Schemes 7. Moving On: Engineering The Input And Output 8. Nuts And Bolts: Machine Learning Algorithms In Java 9. Looking Forward","n":0.14}}},{"i":827,"$":{"0":{"v":"Evaluating defect prediction approaches: a benchmark and an extensive comparison","n":0.316},"1":{"v":"Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.","n":0.071}}},{"i":828,"$":{"0":{"v":"To what extent can maintenance problems be predicted by code smell detection? - An empirical study","n":0.25},"1":{"v":"Context: Code smells are indicators of poor coding and design choices that can cause problems during software maintenance and evolution. Objective: This study is aimed at a detailed investigation to which extent problems in maintenance projects can be predicted by the detection of currently known code smells. Method: A multiple case study was conducted, in which the problems faced by six developers working on four different Java systems were registered on a daily basis, for a period up to four weeks. Where applicable, the files associated to the problems were registered. Code smells were detected in the pre-maintenance version of the systems, using the tools Borland Together and InCode. In-depth examination of quantitative and qualitative data was conducted to determine if the observed problems could be explained by the detected smells. Results: From the total set of problems, roughly 30% percent were related to files containing code smells. In addition, interaction effects were observed amongst code smells, and between code smells and other code characteristics, and these effects led to severe problems during maintenance. Code smell interactions were observed between collocated smells (i.e., in the same file), and between coupled smells (i.e., spread over multiple files that were coupled). Conclusions: The role of code smells on the overall system maintainability is relatively minor, thus complementary approaches are needed to achieve more comprehensive assessments of maintainability. Moreover, to improve the explanatory power of code smells, interaction effects amongst collocated smells and coupled smells should be taken into account during analysis.","n":0.063}}},{"i":829,"$":{"0":{"v":"Top 10 list [software development]","n":0.447},"1":{"v":"Software's complexity and accelerated development schedules make avoiding defects difficult. We have found, however, that researchers have established objective and quantitative data, relationships, and predictive models that help software developers avoid predictable pitfalls and improve their ability to predict and control efficient software projects. The article presents 10 techniques that can help reduce the flaws in your code.","n":0.131}}},{"i":830,"$":{"0":{"v":"Continuous Integration: Improving Software Quality and Reducing Risk","n":0.354},"1":{"v":"For any software developer who has spent days in “integration hell,” cobbling together myriad software components, Continuous Integration: Improving Software Quality and Reducing Risk illustrates how to transform integration from a necessary evil into an everyday part of the development process. The key, as the authors show, is to integrate regularly and often using continuous integration (CI) practices and techniques.The authors first examine the concept of CI and its practices from the ground up and then move on to explore other effective processes performed by CI systems, such as database integration, testing, inspection, deployment, and feedback. Through more than forty CI-related practices using application examples in different languages, readers learn that CI leads to more rapid software development, produces deployable software at every step in the development lifecycle, and reduces the time between defect introduction and detection, saving time and lowering costs. With successful implementation of CI, developers reduce risks and repetitive manual processes, and teams receive better project visibility.The book covers How to make integration a “non-event” on your software development projects How to reduce the amount of repetitive processes you perform when building your software Practices and techniques for using CI effectively with your teams Reducing the risks of late defect discovery, low-quality software, lack of visibility, and lack of deployable software Assessments of different CI servers and related tools on the market The book's companion Web site, www.integratebutton.com, provides updates and code examples.","n":0.065}}},{"i":831,"$":{"0":{"v":"Architecture Technical Debt: Understanding Causes and a Qualitative Model","n":0.333},"1":{"v":"A known problem in large software companies is to balance the prioritization of short-term with long-term responsiveness. Specifically, architecture violations (Architecture Technical Debt) taken to deliver fast might hinder future feature development, which would hinder agility. We conducted a multiple-case embedded case study in 7 sites at 5 large companies in order to shed light on the current causes for the accumulation of Architectural Technical Debt that causes effort. We provide a taxonomy of the factors and their influence in the accumulation of debt, and we provide a qualitative model of how the debt is accumulated and recovered over time.","n":0.1}}},{"i":832,"$":{"0":{"v":"Continuous Software Engineering","n":0.577},"1":{"v":"This book provides essential insights on the adoption of modern software engineering practices at large companies producing software-intensive systems, where hundreds or even thousands of engineers collaborate to deliver on new systems and new versions of already deployed ones. It is based on the findings collected and lessons learned at the Software Center (SC), a unique collaboration between research and industry, with Chalmers University of Technology, Gothenburg University and Malm University as academic partners and Ericsson, AB Volvo, Volvo Car Corporation, Saab Electronic Defense Systems, Grundfos, Axis Communications, Jeppesen (Boeing) and Sony Mobile as industrial partners. The 17 chapters present the Stairway to Heaven model, which represents the typical evolution path companies move through as they develop and mature their software engineering capabilities. The chaptersdescribe theoretical frameworks, conceptual models and, most importantly, the industrial experiences gained by the partner companies in applying novel software engineering techniques. The books structure consists of six parts. Part I describes the model in detail and presents an overview of lessons learned in the collaboration between industry and academia. Part II deals with the first step of the Stairway to Heaven, in which R&D adopts agile work practices. Part III of the book combines the next two phases, i.e., continuous integration (CI) and continuous delivery (CD), as they are closely intertwined. Part IV is concerned with the highest level, referred to as R&D as an innovation system, while Part V addresses a topic that is separate from the Stairway to Heaven and yet critically important in large organizations: organizational performance metrics that capture data, and visualizations of the status of software assets, defects and teams. Lastly, Part VI presents the perspectives of two of the SC partner companies. The book is intended for practitioners and professionals in the software-intensive systems industry, providing concrete models, frameworks and case studies that show the specific challenges that the partner companies encountered, their approaches to overcoming them, and the results. Researchers will gain valuable insights on the problems faced by large software companies, and on how to effectively tackle them in the context of successful cooperation projects.","n":0.054}}},{"i":833,"$":{"0":{"v":"Agile Collaborative Research: Action Principles for Industry-Academia Collaboration","n":0.354},"1":{"v":"Both the software industry and academia promote collaboration to solve challenges together that neither can solve alone. Collaboration brings opportunities to understand and improve in ways not possible when working apart, but it succeeds only if both parties are contributing. A collaboration model developed from eight years' experience setting up and managing a research center explicitly focused on industry needs is based on five success factors enabling research results (need orientation, industry goal alignment, deployment impact, industry benefit, and innovativeness), five success factors enabling research activities (management engagement, network access, collaborator match, communication ability, and continuity), and 10 action principles for industry-academia collaboration management.","n":0.098}}},{"i":834,"$":{"0":{"v":"Measuring and Visualizing Code Stability -- A Case Study at Three Companies","n":0.289},"1":{"v":"Monitoring performance of software development organizations can be achieved from a number of perspectives - e.g. using such tools as Balanced Scorecards or corporate dashboards. In this paper we present results from a study on using code stability indicators as a tool for product stability and organizational performance, conducted at three different software development companies - Ericsson AB, Saab AB Electronic Defense Systems (Saab) and Volvo Group Trucks Technology (Volvo Group). The results show that visualizing the source code changes using heat maps and linking these visualizations to defect inflow profiles provide indicators of how stable the product under development is and whether quality assurance efforts should be directed to specific parts of the product. Observing the indicator and making decisions based on its visualization leads to shorter feedback loops between development and test, thus resulting in lower development costs, shorter lead time and increased quality. The industrial case study in the paper shows that the indicator and its visualization can show whether the modifications of software products are focused on parts of the code base or are spread widely throughout the product.","n":0.074}}},{"i":835,"$":{"0":{"v":"Successful process implementation","n":0.577},"1":{"v":"We propose measuring software process improvement (SPI) success through implementation success - the extent to which initiatives lead to actual changes in software engineering practice. First, without implementation success, SPI success is impossible. Second, only when implementation succeeds can we see how SPI initiatives affect software practices. Third, implementation success is easy to assess. Finally, focusing on implementation success is a pragmatic way to steer SPI initiatives toward success. We studied the approach and outcome of 18 different SPI initiatives conducted over a five-year period at the telecom company Ericsson AB, based in Gothenburg, Sweden. Doing so gave us insight into how SPI initiatives can best 1) ensure stakeholder commitment 2) support organizational learning 3) distribute resources over different activities 4) manage customer relations.","n":0.09}}},{"i":836,"$":{"0":{"v":"A method for forecasting defect backlog in large streamline software development projects and its industrial evaluation","n":0.25},"1":{"v":"Context: Predicting a number of defects to be resolved in large software projects (defect backlog) usually requires complex statistical methods and thus is hard to use on a daily basis by practitioners in industry. Making predictions in simpler and more robust way is often required by practitioners in software engineering industry. Objective: The objective of this paper is to present a simple and reliable method for forecasting the level of defect backlog in large, lean-based software development projects. Method: The new method was created as part of an action research project conducted at Ericsson. In order to create the method we have evaluated multivariate linear regression, expert estimations and analogy-based predictions w.r.t. their accuracy and ease-of-use in industry. We have also evaluated the new method in a life project at one of the units of Ericsson during a period of 21weeks (from the beginning of the project until the release of the product). Results: The method for forecasting the level of defect backlog uses an indicator of the trend (an arrow) as a basis to forecast the level of defect backlog. Forecasts are based on moving average which combined with the current level of defect backlog was found to be the best prediction method (Mean Magnitude of Relative Error of 16%) for the level of future defect backlog. Conclusion: We have found that ease-of-use and accuracy are the main aspects for practitioners who use predictions in their work. In this paper it is concluded that using the simple moving average provides a sufficiently-good accuracy (much appreciated by practitioners involved in the study). We also conclude that using the indicator (forecasting the trend) instead of the absolute number of defects in the backlog increases the confidence in our method compared to our previous attempts (regression, analogy-based, and expert estimates).","n":0.058}}},{"i":837,"$":{"0":{"v":"Identifying Implicit Architectural Dependencies Using Measures of Source Code Change Waves","n":0.302},"1":{"v":"The principles of Agile software development are increasingly used in large software development projects, e.g. using Scrum of Scrums or combining Agile and Lean development methods. When large software products are developed by self-organized, usually feature-oriented teams, there is a risk that architectural dependencies between software components become uncontrolled. In particular there is a risk that the prescriptive architecture models in form of diagrams are outdated and implicit architectural dependencies may become more frequent than the explicit ones. In this paper we present a method for automated discovery of potential dependencies between software components based on analyzing revision history of software repositories. The result of this method is a map of implicit dependencies which is used by architects in decisions on the evolution of the architecture. The software architects can assess the validity of the dependencies and can prevent unwanted component couplings and design erosion hence minimizing the risk of post-release quality problems. Our method was evaluated in a case study at one large product at Saab Electronic Defense Systems (Saab EDS) and one large software product at Ericsson AB.","n":0.075}}},{"i":838,"$":{"0":{"v":"Presenting Software Metrics Indicators: A Case Study","n":0.378},"1":{"v":"Industrial measurement systems in software projects can generate a large number of indicators (main measurements). Having a large number of indicators might result in failing to present an overview of the status of measured entities. In consequence, managers might experience problems when making decisions based on indicators. In essence, visualizing indicators and their dependencies can communicate the information to the stakeholders efficiently if done correctly, or mislead them if not done properly. In this paper we present results of a case study conducted in a unit of Ericsson. During the case study we identified the main requirements for methods for visualizing the indicators, developed these visualizations and conducted a series of interviews evaluating them. The results show that the dashboard presentation is the best solution, but that the simple, tabular visualizations are next best suited for communicating the information to the managers.","n":0.084}}},{"i":839,"$":{"0":{"v":"Monitoring Evolution of Code Complexity and Magnitude of Changes","n":0.333},"1":{"v":"Background:  Complexity management has become a crucial activity in  continuous  software \r\ndevelopment. While the overall perceived complexity of a product grows rather insignificantly, \r\nthe small units, such as functions and files,  can  have noticeable  complexity growth  with every \r\nincrement of product features. This kind of evolution triggers risks of escalating fault-proneness \r\nand deteriorating maintainability.\r\nGoal:  The goal of this research was  to develop a measurement system which enables effective \r\nmonitoring of complexity evolution.\r\nMethod:  An action research has been conducted in two large software development organiza-tions.  We have measured three  complexity  and  two change properties  of code  for  two large \r\nindustrial products. The complexity growth has been measured for five consecutive releases of \r\nproducts.  Different patterns of  growth have been identified and  evaluated  with  software engi-neers in industry.\r\nResults:  The results show that  monitoring cyclomatic complexity evolution of functions and \r\nnumber of revisions of  files  focuses the attention of  designers to  potentially problematic  files \r\nand functions  for manual assessment and improvement. A measurement system  was  developed \r\nat Ericsson to support the monitoring process.","n":0.077}}},{"i":840,"$":{"0":{"v":"A view of cloud computing","n":0.447},"1":{"v":"Clearing the clouds away from the true potential and obstacles posed by this computing capability.","n":0.258}}},{"i":841,"$":{"0":{"v":"CloudSim: a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms","n":0.243},"1":{"v":"Cloud computing is a recent advancement wherein IT infrastructure and applications are provided as ‘services’ to end-users under a usage-based payment model. It can leverage virtualized services even on the fly based on requirements (workload patterns and QoS) varying with time. The application services hosted under Cloud computing model have complex provisioning, composition, configuration, and deployment requirements. Evaluating the performance of Cloud provisioning policies, application workload models, and resources performance models in a repeatable manner under varying system and user configurations and requirements is difficult to achieve. To overcome this challenge, we propose CloudSim: an extensible simulation toolkit that enables modeling and simulation of Cloud computing systems and application provisioning environments. The CloudSim toolkit supports both system and behavior modeling of Cloud system components such as data centers, virtual machines (VMs) and resource provisioning policies. It implements generic application provisioning techniques that can be extended with ease and limited effort. Currently, it supports modeling and simulation of Cloud computing environments consisting of both single and inter-networked clouds (federation of clouds). Moreover, it exposes custom interfaces for implementing policies and provisioning techniques for allocation of VMs under inter-networked Cloud computing scenarios. Several researchers from organizations, such as HP Labs in U.S.A., are using CloudSim in their investigation on Cloud resource provisioning and energy-efficient management of data center resources. The usefulness of CloudSim is demonstrated by a case study involving dynamic provisioning of application services in the hybrid federated clouds environment. The result of this case study proves that the federated Cloud computing model significantly improves the application QoS requirements under fluctuating resource and service demand patterns. Copyright © 2010 John Wiley & Sons, Ltd.","n":0.061}}},{"i":842,"$":{"0":{"v":"Reinforcement Learning: An Introduction","n":0.5},"1":{"v":"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.","n":0.08}}},{"i":843,"$":{"0":{"v":"CloudScale: elastic resource scaling for multi-tenant cloud systems","n":0.354},"1":{"v":"Elastic resource scaling lets cloud systems meet application service level objectives (SLOs) with minimum resource provisioning costs. In this paper, we present CloudScale, a system that automates fine-grained elastic resource scaling for multi-tenant cloud computing infrastructures. CloudScale employs online resource demand prediction and prediction error handling to achieve adaptive resource allocation without assuming any prior knowledge about the applications running inside the cloud. CloudScale can resolve scaling conflicts between applications using migration, and integrates dynamic CPU voltage/frequency scaling to achieve energy savings with minimal effect on application SLOs. We have implemented CloudScale on top of Xen and conducted extensive experiments using a set of CPU and memory intensive applications (RUBiS, Hadoop, IBM System S). The results show that CloudScale can achieve significantly higher SLO conformance than other alternatives with low resource and energy cost. CloudScale is non-intrusive and light-weight, and imposes negligible overhead (","n":0.083}}},{"i":844,"$":{"0":{"v":"A Review of Auto-scaling Techniques for Elastic Applications in Cloud Environments","n":0.302},"1":{"v":"Cloud computing environments allow customers to dynamically scale their applications. The key problem is how to lease the right amount of resources, on a pay-as-you-go basis. Application re-dimensioning can be implemented effortlessly, adapting the resources assigned to the application to the incoming user demand. However, the identification of the right amount of resources to lease in order to meet the required Service Level Agreement, while keeping the overall cost low, is not an easy task. Many techniques have been proposed for automating application scaling. We propose a classification of these techniques into five main categories: static threshold-based rules, control theory, reinforcement learning, queuing theory and time series analysis. Then we use this classification to carry out a literature review of proposals for auto-scaling in the cloud.","n":0.089}}},{"i":845,"$":{"0":{"v":"A Performance Study on the VM Startup Time in the Cloud","n":0.302},"1":{"v":"One of many advantages of the cloud is the elasticity, the ability to dynamically acquire or release computing resources in response to demand. However, this elasticity is only meaningful to the cloud users when the acquired Virtual Machines (VMs) can be provisioned in time and be ready to use within the user expectation. The long unexpected VM startup time could result in resource under-provisioning, which will inevitably hurt the application performance. A better understanding of the VM startup time is therefore needed to help cloud users to plan ahead and make in-time resource provisioning decisions. In this paper, we study the startup time of cloud VMs across three real-world cloud providers -- Amazon EC2, Windows Azure and Rackspace. We analyze the relationship between the VM startup time and different factors, such as time of the day, OS image size, instance type, data center location and the number of instances acquired at the same time. We also study the VM startup time of spot instances in EC2, which show a longer waiting time and greater variance compared to on-demand instances.","n":0.075}}},{"i":846,"$":{"0":{"v":"Elasticity in Cloud Computing: What It Is, and What It Is Not.","n":0.289},"1":{"v":"Originating from the field of physics and economics, the term elasticity is nowadays heavily used in the context of cloud computing. In this context, elasticity is commonly understood as the ability of a system to automatically provision and deprovision computing resources on demand as workloads change. However, elasticity still lacks a precise definition as well as representative metrics coupled with a benchmarking methodology to enable comparability of systems. Existing definitions of elasticity are largely inconsistent and unspecific, which leads to confusion in the use of the term and its differentiation from related terms such as scalability and efficiency; the proposed measurement methodologies do not provide means to quantify elasticity without mixing it with efficiency or scalability aspects. In this short paper, we propose a precise definition of elasticity and analyze its core properties and requirements explicitly distinguishing from related terms such as scalability and efficiency. Furthermore, we present a set of appropriate elasticity metrics and sketch a new elasticity tailored benchmarking methodology addressing the special requirements on workload design and calibration.","n":0.076}}},{"i":847,"$":{"0":{"v":"The design of the force.com multitenant internet application development platform","n":0.316},"1":{"v":"Force.com is the preeminent on-demand application development platform in use today, supporting some 55,000+ organizations. Individual enterprises and commercial software-as-a-service (SaaS) vendors trust the platform to deliver robust, reliable, Internet-scale applications. To meet the extreme demands of its large user population, Force.com's foundation is a metadatadriven software architecture that enables multitenant applications.   The focus of this paper is multitenancy, a fundamental design approach that can dramatically improve SaaS application management. This paper defines multitenancy, explains its benefits, and demonstrates why metadata-driven architectures are the premier choice for implementing multitenancy.","n":0.106}}},{"i":848,"$":{"0":{"v":"Machine Learning with R","n":0.5},"1":{"v":"Learn how to use R to apply powerful machine learning methods and gain an insight into real-world applications Overview Harness the power of R for statistical computing and data science Use R to apply common machine learning algorithms with real-world applications Prepare, examine, and visualize data for analysis Understand how to choose between machine learning models Packed with clear instructions to explore, forecast, and classify data In Detail Machine learning, at its core, is concerned with transforming data into actionable knowledge. This fact makes machine learning well-suited to the present-day era of \"big data\" and \"data science\". Given the growing prominence of Ra cross-platform, zero-cost statistical programming environmentthere has never been a better time to start applying machine learning. Whether you are new to data science or a veteran, machine learning with R offers a powerful set of methods for quickly and easily gaining insight from your data. \"Machine Learning with R\" is a practical tutorial that uses hands-on examples to step through real-world application of machine learning. Without shying away from the technical details, we will explore Machine Learning with R using clear and practical examples. Well-suited to machine learning beginners or those with experience. Explore R to find the answer to all of your questions. How can we use machine learning to transform data into action? Using practical examples, we will explore how to prepare data for analysis, choose a machine learning method, and measure the success of the process. We will learn how to apply machine learning methods to a variety of common tasks including classification, prediction, forecasting, market basket analysis, and clustering. By applying the most effective machine learning methods to real-world problems, you will gain hands-on experience that will transform the way you think about data. \"Machine Learning with R\" will provide you with the analytical tools you need to quickly gain insight from complex data. What you will learn from this book Understand the basic terminology of machine learning and how to differentiate among various machine learning approaches Use R to prepare data for machine learning Explore and visualize data with R Classify data using nearest neighbor methods Learn about Bayesian methods for classifying data Predict values using decision trees, rules, and support vector machines Forecast numeric values using linear regression Model data using neural networks Find patterns in data using association rules for market basket analysis Group data into clusters for segmentation Evaluate and improve the performance of machine learning models Learn specialized machine learning techniques for text mining, social network data, and big data Approach Written as a tutorial to explore and understand the power of R for machine learning. This practical guide that covers all of the need to know topics in a very systematic way. For each machine learning approach, each step in the process is detailed, from preparing the data for analysis to evaluating the results. These steps will build the knowledge you need to apply them to your own data science tasks.","n":0.045}}},{"i":849,"$":{"0":{"v":"An adaptive hybrid elasticity controller for cloud infrastructures","n":0.354},"1":{"v":"Cloud elasticity is the ability of the cloud infrastructure to rapidly change the amount of resources allocated to a service in order to meet the actual varying demands on the service while enforcing SLAs. In this paper, we focus on horizontal elasticity, the ability of the infrastructure to add or remove virtual machines allocated to a service deployed in the cloud. We model a cloud service using queuing theory. Using that model we build two adaptive proactive controllers that estimate the future load on a service. We explore the different possible scenarios for deploying a proactive elasticity controller coupled with a reactive elasticity controller in the cloud. Using simulation with workload traces from the FIFA world-cup web servers, we show that a hybrid controller that incorporates a reactive controller for scale up coupled with our proactive controllers for scale down decisions reduces SLA violations by a factor of 2 to 10 compared to a regression based controller or a completely reactive controller.","n":0.079}}},{"i":850,"$":{"0":{"v":"Multi-tenant SaaS applications: maintenance dream or nightmare?","n":0.378},"1":{"v":"Multi-tenancy is a relatively new software architecture principle in the realm of the Software as a Service (SaaS) business model. It allows to make full use of the economy of scale, as multiple customers - \"tenants\" - share the same application and database instance. All the while, the tenants enjoy a highly configurable application, making it appear that the application is deployed on a dedicated server. The major benefits of multi-tenancy are increased utilization of hardware resources and improved ease of maintenance, in particular on the deployment side. These benefits should result in lower overall application costs, making the technology attractive for service providers targeting small and medium enterprises (SME). However, as this paper advocates, a wrong architectural choice might entail that multi-tenancy becomes a maintenance nightmare.","n":0.089}}},{"i":851,"$":{"0":{"v":"Algorithmic Nuggets in Content Delivery","n":0.447},"1":{"v":"This paper \"peeks under the covers\" at the subsystems that provide the basic functionality of a leading content delivery network. Based on our experiences in building one of the largest distributed systems in the world, we illustrate how sophisticated algorithmic research has been adapted to balance the load between and within server clusters, manage the caches on servers, select paths through an overlay routing network, and elect leaders in various contexts. In each instance, we first explain the theory underlying the algorithms, then introduce practical considerations not captured by the theoretical models, and finally describe what is implemented in practice. Through these examples, we highlight the role of algorithmic research in the design of complex networked systems. The paper also illustrates the close synergy that exists between research and industry where research ideas cross over into products and product requirements drive future research.","n":0.084}}},{"i":852,"$":{"0":{"v":"AutoElastic: Automatic Resource Elasticity for High Performance Applications in the Cloud","n":0.302},"1":{"v":"Elasticity is undoubtedly one of the most striking characteristics of cloud computing. Especially in the area of high performance computing (HPC), elasticity can be used to execute irregular and CPU-intensive applications. However, the on- the-fly increase/decrease in resources is more widespread in Web systems, which have their own IaaS-level load balancer. Considering the HPC area, current approaches usually focus on batch jobs or assumptions such as previous knowledge of application phases, source code rewriting or the stop-reconfigure-and-go approach for elasticity. In this context, this article presents AutoElastic, a PaaS-level elasticity model for HPC in the cloud. Its differential approach consists of providing elasticity for high performance applications without user intervention or source code modification. The scientific contributions of AutoElastic are twofold: (i) an Aging-based approach to resource allocation and deallocation actions to avoid unnecessary virtual machine (VM) reconfigurations (thrashing) and (ii) asynchronism in creating and terminating VMs in such a way that the application does not need to wait for completing these procedures. The prototype evaluation using OpenNebula middleware showed performance gains of up to 26 percent in the execution time of an application with the AutoElastic manager. Moreover, we obtained low intrusiveness for AutoElastic when reconfigurations do not occur.","n":0.071}}},{"i":853,"$":{"0":{"v":"A Survey of the Stable Marriage Problem and Its Variants","n":0.316},"1":{"v":"The stable marriage problem is to find a matching between men and women, considering preference lists in which each person expresses his/her preference over the members of the opposite gender. The output matching must be stable, which intuitively means that there is no man- woman pair both of which have incentive to elope. This problem was introduced in 1962 in the seminal paper of Gale and Shapley, and has attracted researchers in several areas, including mathematics, economics, game theory, computer science, etc. This paper introduces old and recent results on the stable marriage problem and some other related problems.","n":0.101}}},{"i":854,"$":{"0":{"v":"BUNGEE: an elasticity benchmark for self-adaptive IaaS cloud environments","n":0.333},"1":{"v":"Today's infrastructure clouds provide resource elasticity (i.e. auto-scaling) mechanisms enabling self-adaptive resource provisioning to reflect variations in the load intensity over time. These mechanisms impact on the application performance, however, their effect in specific situations is hard to quantify and compare. To evaluate the quality of elasticity mechanisms provided by different platforms and configurations, respective metrics and benchmarks are required. Existing metrics for elasticity only consider the time required to provision and deprovision resources or the costs impact of adaptations. Existing benchmarks lack the capability to handle open workloads with realistic load intensity profiles and do not explicitly distinguish between the performance exhibited by the provisioned underlying resources, on the one hand, and the quality of the elasticity mechanisms themselves, on the other hand.   In this paper, we propose reliable metrics for quantifying the timing aspects and accuracy of elasticity. Based on these metrics, we propose a novel approach for benchmarking the elasticity of Infrastructure-as-a-Service (IaaS) cloud platforms independent of the performance exhibited by the provisioned underlying resources. We show that the proposed metrics provide consistent ranking of elastic platforms on an ordinal scale. Finally, we present an extensive case study of real-world complexity demonstrating that the proposed approach is applicable in realistic scenarios and can cope with different levels of resource efficiency.","n":0.069}}},{"i":855,"$":{"0":{"v":"Workload Classification for Efficient Auto-Scaling of Cloud Resources","n":0.354},"1":{"v":"Elasticity algorithms for cloud infrastructures dynamically change the amount of resources allocated to a running service according to the current and predicted future load. Since there is no perfe ...","n":0.183}}},{"i":856,"$":{"0":{"v":"Feedback-Control-Based Performance Regulation for Multi-Tenant Applications","n":0.408},"1":{"v":"The ability to deliver different performance levels based on tenant-specific service level agreements (SLAs) is a key requirement for multi-tenant internet applications. However, workload variations and extensive resource sharing among tenants make this goal hard to achieve. We address the issue with a performance regulator based on feedback-control. The regulator has a hierarchical structure, with which a high-level controller manages request admission rates to prevent overloading and a low-level controller manages resource allocation for admitted requests to track a specified level of service differentiation between the cohosted tenants. A prototype implementation of the performance regulator based on Tomcat and MySQL is provided and a multi-tenant version of RUBBoS benchmark is used for evaluation. Experimental results indicate that the regulator effectively bounds the response time for each tenant while maintaining high resource utilization levels.","n":0.087}}},{"i":857,"$":{"0":{"v":"On heuristics for two-sided matching: revisiting the stable marriage problem as a multiobjective problem","n":0.267},"1":{"v":"The stable marriage problem is prototypical of two-sided matching problems, widely encountered in practice, in which agents having preferences, interests and capacities for action of their own are paired up or matched. Standardly, variants of the well-known Gale-Shapley deferred acceptance algorithm (GS/DAA) are used to find stable matches. Using evolutionary computation and an agent-based model heuristics, this paper investigates the stable marriage problem as a multiobjective problem, looking at social welfare and equity or fairness, in addition to stability as important aspects of any proposed match. The paper finds that these heuristics are reliably able to discover matches that are Pareto superior to those found by the GS/DAA procedure. Ramifications of this finding are briefly explored, including the question of whether stability in a matching is often strictly required","n":0.088}}},{"i":858,"$":{"0":{"v":"Elasticity debt: a debt-aware approach to reason about elasticity decisions in the cloud","n":0.277},"1":{"v":"Cloud elasticity provides the underlying primitives to dynamically acquire and release shared computational resources on demand. Therefore, elasticity constantly takes adaptation decisions to adjust the resource provisioning constrained by quality of service and operating costs minimization. However, dynamic trade-offs for resource provisioning rarely consider the value of the adaptation decisions under uncertainty. Part of the problem stems from the lack of a utility-driven model to reason about it. In this paper, we introduce the concept of elasticity debt as an approach to reason about elasticity decisions from a utility-driven perspective, where we apply the technical debt metaphor in the context of cloud elasticity. Moreover, we extended CloudSim as a proof of concept to show that a debt-aware elasticity decision-making can achieve a higher utility over time. We provide an elasticity conceptual model that links the key factors to consider when adapting resource provisioning and the potential debts incurred by these decisions. We propose a new perspective to value elasticity decisions in the uncertain cloud environment by introducing a technical debt perspective.","n":0.076}}},{"i":859,"$":{"0":{"v":"Modeling and Extracting Load Intensity Profiles","n":0.408},"1":{"v":"Today’s system developers and operators face the challenge of creating software systems that make efficient use of dynamically allocated resources under highly variable and dynamic load profiles, while at the same time delivering reliable performance. Autonomic controllers, for example, an advanced autoscaling mechanism in a cloud computing context, can benefit from an abstracted load model as knowledge to reconfigure on time and precisely. Existing workload characterization approaches have limited support to capture variations in the interarrival times of incoming work units over time (i.e., a variable load profile). For example, industrial and scientific benchmarks support constant or stepwise increasing load, or interarrival times defined by statistical distributions or recorded traces. These options show shortcomings either in representative character of load variation patterns or in abstraction and flexibility of their format.   In this article, we present the Descartes Load Intensity Model (DLIM) approach addressing these issues. DLIM provides a modeling formalism for describing load intensity variations over time. A DLIM instance is a compact formal description of a load intensity trace. DLIM-based tools provide features for benchmarking, performance, and recorded load intensity trace analysis. As manually obtaining and maintaining DLIM instances becomes time consuming, we contribute three automated extraction methods and devised metrics for comparison and method selection. We discuss how these features are used to enhance system management approaches for adaptations during runtime, and how they are integrated into simulation contexts and enable benchmarking of elastic or adaptive behavior.   We show that automatically extracted DLIM instances exhibit an average modeling error of 15.2% over 10 different real-world traces that cover between 2 weeks and 7 months. These results underline DLIM model expressiveness. In terms of accuracy and processing speed, our proposed extraction methods for the descriptive models are comparable to existing time series decomposition methods. Additionally, we illustrate DLIM applicability by outlining approaches of workload modeling in systems engineering that employ or rely on our proposed load intensity modeling formalism.","n":0.056}}},{"i":860,"$":{"0":{"v":"Restructuring and refinancing technical debt","n":0.447},"1":{"v":"Given the increasing importance of software to society, the issue of technical debt is becoming more pervasive in software development. Its implications range from incurring small amounts of technical debt to speed up development - a positive - to stalling and making development no longer possible - a huge negative. In this paper, we present a framework that attempts to refine the understanding of technical debt by tracing more links to the financial metaphor, specifically focusing on the concepts of restructuring and refinancing technical debt. This paper looks at technical debt as a leverage product that is contingent upon the liquidity of the debtor. From this perspective, it is then possible to more effectively assess the incurment of technical debt and also to more effectively strategize the use of leverage in software development - accounting for the respective risks and benefits it provides.","n":0.084}}},{"i":861,"$":{"0":{"v":"Developing Multi-tenant Applications for the Cloud on Windows Azure","n":0.333},"1":{"v":"How can you create an application that has truly global reach, and can scale rapidly to meet sudden massive spikes in demand? Historically, companies had to invest in an infrastructure capable of supporting such an application themselves, and plan for peak demandwhich often means that much of the capacity sits idle for much of the time. Typically, only large companies would have the available resources to risk such an enterprise. The cloud has changed the rules of the game. By making infrastructure available on a pay as you go basis, creating a massively scalable, global application is within the reach of both large and small companies. Yes, by moving applications to the cloud youre giving up some control and autonomy, but youre also going to benefit from reduced costs, increased flexibility, and scalable computation and storage. This guide is the third release of the second volume in a series about Windows Azure. It demonstrates how you can create from scratch a multi-tenant, Software as a Service (SaaS) application to run in the cloud by using the Windows Azure tools and the increasing range of capabilities of Windows Azure. The guide focuses on both good practice design and the practicalities of implementation for multi-tenant applications, but also contains a wealth of information on factors such as security, scalability, availability, and elasticity that are relevant to all types of cloud hosted applications. The guide is intended for any architect, developer, or information technology (IT) professional who designs, builds, or operates applications and services that run on or interact with the cloud. Although applications do not need to be based on the Windows operating system to work in Windows Azure, or be written using a .NET language, this guide is written for people who work with Windows based systems. You should be familiar with the .NET Framework, Visual Studio, ASP.NET MVC, and Visual C#.","n":0.057}}},{"i":862,"$":{"0":{"v":"Economics-Driven Approach for Managing Technical Debt in Cloud-Based Architectures","n":0.333},"1":{"v":"Cloud-based Service-Oriented Architectures are composed of web services, offered via the cloud. The substitution decision may introduce technical debt, which needs to be managed, cleared and transformed to value-added. We define the concept of technical debt for Cloud-based SOA. We formulate the problem of web service substitution and its technical debt valuation as an option problem (option-to-switch between services). We use options analysis to manage and clear technical debt. We report on the formulation, which exploits Binomial Options Analysis. We evaluate the approach using an example.","n":0.108}}},{"i":863,"$":{"0":{"v":"Hubbub-Scale: Towards Reliable Elastic Scaling under Multi-tenancy","n":0.378},"1":{"v":"Elastic resource provisioning is used to guarantee service level objective (SLO) with reduced cost in a Cloud platform. However, performance interference in the hosting platform introduces uncertainty in the performance guarantees of provisioned services. Existing elasticity controllers are either unaware of this interference or over-provision resources to meet the SLO. In this paper, we show that assuming predictable performance of VMs to build an elasticity controller will fail if interference is not modelled. We identify and control the different sources of unpredictability and build Hubbub-Scale, an elasticity controller that is reliable in the presence of performance interference. Our evaluation with Redis and Memcached show that Hubbub-Scale efficiently conforms to the SLO requirements under scenarios where standard modelling approaches fail.","n":0.092}}},{"i":864,"$":{"0":{"v":"Elasticity in Service Level Agreements","n":0.447},"1":{"v":"Elasticity has been identified as one of the key features of delivering IT services over the internet. Major definitions of cloud computing consider elasticity as an essential property of cloud services, and as one of the main reasons to use cloud infrastructures. However, customers expect and hope for the elasticity promise without having any guarantee or contractual agreement. While elasticity has mostly been considered on qualitative terms, there have been recent efforts in quantifying elasticity and thus enabling the measurement of elasticity. Based on new approaches of defining elasticity, it becomes possible to specify precise and meaningful elasticity guarantees, which is advantageous for both consumers and providers of cloud services. This work contributes to the ongoing elasticity research by including elasticity terms into service level agreements (SLAs). This is achieved by restricting the validity of service level objectives with respect to the available elasticity. The approach is explained in detail and its advantages and consequences for service providers and consumers are discussed.","n":0.079}}},{"i":865,"$":{"0":{"v":"Extreme Programming Explained: Embrace Change","n":0.447},"1":{"v":"Software development projects can be fun, productive, and even daring. Yet they can consistently deliver value to a business and remain under control.Extreme Programming (XP) was conceived and developed to address the specific needs of software development conducted by small teams in the face of vague and changing requirements. This new lightweight methodology challenges many conventional tenets, including the long-held assumption that the cost of changing a piece of software necessarily rises dramatically over the course of time. XP recognizes that projects have to work to achieve this reduction in cost and exploit the savings once they have been earned.Fundamentals of XP include: Distinguishing between the decisions to be made by business interests and those to be made by project stakeholders. Writing unit tests before programming and keeping all of the tests running at all times. Integrating and testing the whole system--several times a day. Producing all software in pairs, two programmers at one screen. Starting projects with a simple design that constantly evolves to add needed flexibility and remove unneeded complexity. Putting a minimal system into production quickly and growing it in whatever directions prove most valuable.Why is XP so controversial? Some sacred cows don't make the cut in XP: Don't force team members to specialize and become analysts, architects, programmers, testers, and integrators--every XP programmer participates in all of these critical activities every day. Don't conduct complete up-front analysis and design--an XP project starts with a quick analysis of the entire system, and XP programmers continue to make analysis and design decisions throughout development. Develop infrastructure and frameworks as you develop your application, not up-front--delivering business value is the heartbeat that drives XP projects. Don't write and maintain implementation documentation--communication in XP projects occurs face-to-face, or through efficient tests and carefully written code.You may love XP, or you may hate it, but Extreme Programming Explained will force you to take a fresh look at how you develop software. 0201616416B04062001","n":0.056}}},{"i":866,"$":{"0":{"v":"Software Security: Building Security In","n":0.447},"1":{"v":"Summary form only given. Software security has come a long way in the last few years, but we've really only just begun. I will present a detailed approach to getting past theory and putting software security into practice. The three pillars of software security are applied risk management, software security best practices (which I call touchpoints), and knowledge. By describing a manageably small set of touchpoints based around the software artifacts that you already produce, I avoid religious warfare over process and get on with the business of software security. That means you can adopt the touchpoints without radically changing the way you work. The touchpoints I will describe include: code review using static analysis tools; architectural risk analysis; penetration testing; security testing; abuse case development; and security requirements. Like the yin and the yang, software security requires a careful balance-attack and defense, exploiting and designing, breaking and building-bound into a coherent package. Create your own Security Development Lifecycle by enhancing your existing software development lifecycle with the touchpoints","n":0.077}}},{"i":867,"$":{"0":{"v":"The Security Development Lifecycle","n":0.5},"1":{"v":"This introduction to the Security Development Lifecycle (SDL) provides a history of the methodology and guides you through each stage of a proven process-from design to release-that helps minimize security defects.","n":0.18}}},{"i":868,"$":{"0":{"v":"Software security","n":0.707},"1":{"v":"Software security is the idea of engineering software so that it continues to function correctly under malicious attack. Most technologists acknowledge this undertaking's importance, but they need some help in understanding how to tackle it. The article aims to provide that help by exploring software security best practices. A central and critical aspect of the computer security problem is a software problem. Software defects with security ramifications, including implementation bugs such as buffer overflows and design flaws such as inconsistent error handling, promise to be with us for years. All too often, malicious intruders can hack into systems by exploiting software defects. Internet-enabled software applications present the most common security risk encountered today, with software's ever-expanding complexity and extensibility adding further fuel to the fire. By any measure, security holes in software are common, and the problem is growing.","n":0.085}}},{"i":869,"$":{"0":{"v":"The DevOps Handbook: How to Create World-Class Agility, Reliability, and Security in Technology Organizations","n":0.267},"1":{"v":"Increase profitability, elevate work culture, and exceed productivity goals through DevOps practices. More than ever, the effective management of technology is critical for business competitiveness. For decades, technology leaders have struggled to balance agility, reliability, and security. The consequences of failure have never been greaterwhether it's the healthcare.gov debacle, cardholder data breaches, or missing the boat with Big Data in the cloud. And yet, high performers using DevOps principles, such as Google, Amazon, Facebook, Etsy, and Netflix, are routinely and reliably deploying code into production hundreds, or even thousands, of times per day. Following in the footsteps of The Phoenix Project, The DevOps Handbook shows leaders how to replicate these incredible outcomes, by showing how to integrate Product Management, Development, QA, IT Operations, and Information Security to elevate your company and win in the marketplace. Take the DORA DevOps X-ray Assessment and see where you stand! Visit devops-survey.com with your access code to take the DevOps X-ray Assessment.","n":0.08}}},{"i":870,"$":{"0":{"v":"A common criteria based security requirements engineering process for the development of secure information systems","n":0.258},"1":{"v":"In order to develop security critical Information Systems, specifying security quality requirements is vitally important, although it is a very difficult task. Fortunately, there are several security standards, like the Common Criteria (ISO/IEC 15408), which help us handle security requirements. This article will present a Common Criteria centred and reuse-based process that deals with security requirements at the early stages of software development in a systematic and intuitive way, by providing a security resources repository as well as integrating the Common Criteria into the software lifecycle, so that it unifies the concepts of requirements engineering and security engineering.","n":0.101}}},{"i":871,"$":{"0":{"v":"On \"Software engineering\"","n":0.577}}},{"i":872,"$":{"0":{"v":"Debt with Potential Repudiation: Theoretical and Empirical Analysis","n":0.354}}},{"i":873,"$":{"0":{"v":"Effort estimation in agile software development: a systematic literature review","n":0.316},"1":{"v":"Context: Ever since the emergence of agile methodologies in 2001, many software companies have shifted to Agile Software Development (ASD), and since then many studies have been conducted to investigate effort estimation within such context; however to date there is no single study that presents a detailed overview of the state of the art in effort estimation for ASD. Objectives: The aim of this study is to provide a detailed overview of the state of the art in the area of effort estimation in ASD. Method: To report the state of the art, we conducted a systematic literature review in accordance with the guidelines proposed in the evidence-based software engineering literature. Results: A total of 25 primary studies were selected; the main findings are: i) Subjective estimation techniques (e.g. expert judgment, planning poker, use case points estimation method) are the most frequently applied in an agile context; ii) Use case points and story points are the most frequently used size metrics respectively; iii) MMRE (Mean Magnitude of Relative Error) and MRE (Magnitude of Relative Error) are the most frequently used accuracy metrics; iv) team skills, prior experience and task size are cited as the three important cost drivers for effort estimation in ASD; and v) Extreme Programming (XP) and SCRUM are the only two agile methods that are identified in the primary studies. Conclusion: Subjective estimation techniques, e.g. expert judgment-based techniques, planning poker or the use case points method, are the one used the most in agile effort estimation studies. As for the size metrics, the ones that were used the most in the primary studies were story points and use case points. Several research gaps were identified, relating to the agile methods, size metrics and cost drivers, thus suggesting numerous possible avenues for future work.","n":0.058}}},{"i":874,"$":{"0":{"v":"Cloud security engineering","n":0.577},"1":{"v":"Security vulnerabilities and defects are results of poorly constructed software that can lead to easy exploitation by the cyber criminals. A large number of Cloud software systems are facing security threats, and even the sophisticated security tools and mechanisms are not able to detect it. Such prevailing problem necessitates the monitoring and controlling of the software development process and its maintenance. Security is considered to be one of the nonfunctional requirements that have significant effect on the architectural designing of the Cloud Software as a Service (SaaS). In addition, there is prevalence of differential views between the two software engineering concepts, i.e., conventional and contemporary and then this presents a significant challenge for the software development team to deal with security at the implementation and maintenance stage of the SDLC. Thus, we have discussed a real world case study includes 103 failed real cases that were generated manually or automatically by real applications through various testing techniques and we have illustrated some preliminary results. The evaluation results showed appearance of a significant number of security vulnerabilities in the early stages of Cloud Software/Service Development Life Cycle (CSDLC). Hence, this needs to be maintained in advance. Based on such results, this paper presents a generic framework to deal with such security at the early stages of the CSDLC. This framework aims at adding an extra security level at the early stages of the CSDLC, which has been further illustrated by a case study showing the applicability of the framework.","n":0.064}}},{"i":875,"$":{"0":{"v":"A descriptive study of Microsoft's threat modeling technique","n":0.354},"1":{"v":"Microsoft's STRIDE is a popular threat modeling technique commonly used to discover the security weaknesses of a software system. In turn, discovered weaknesses are a major driver for incepting security requirements. Despite its successful adoption, to date no empirical study has been carried out to quantify the cost and effectiveness of STRIDE. The contribution of this paper is the evaluation of STRIDE via a descriptive study that involved 57 students in their last master year in computer science. The study addresses three research questions. First, it assesses how many valid threats per hour are produced on average. Second, it evaluates the correctness of the analysis results by looking at the average number of false positives, i.e., the incorrect threats. Finally, it determines the completeness of the analysis results by looking at the average number of false negatives, i.e., the overlooked threats.","n":0.084}}},{"i":876,"$":{"0":{"v":"Reliability Validation and Improvement Framework","n":0.447},"1":{"v":"Abstract : Software-reliant systems such as rotorcraft and other aircraft have experienced exponential growth in software size and complexity. The current software engineering practice of build then test has made them unaffordable to build and qualify. This report discusses the challenges of qualifying such systems, presenting the findings of several government and industry studies. It identifies several root cause areas and proposes a framework for reliability validation and improvement that integrates several recommended technology solutions: validation of formalized requirements; an architecture-centric, model-based engineering approach that uncovers system-level problems early through analysis; use of static analysis for validating system behavior and other system properties; and managed confidence in qualification through system assurance. This framework also provides the basis for a set of metrics for cost-effective reliability improvement that overcome the challenges of existing software complexity, reliability, and cost metrics.","n":0.085}}},{"i":877,"$":{"0":{"v":"SecDevOps: Is It a Marketing Buzzword? - Mapping Research on Security in DevOps","n":0.277},"1":{"v":"DevOps is changing the way organizations develop and deploy applications and service customers. Many organizations want to apply DevOps, but they are concerned by the security aspects of the produced software. This has triggered the creation of the terms SecDevOps and DevSecOps. These terms refer to incorporating security practices in a DevOps environment by promoting the collaboration between the development teams, the operations teams, and the security teams. This paper surveys the literature from academia and industry to identify the main aspects of this trend. The main aspects that we found are: definition, security best practices, compliance, process automation, tools for SecDevOps, software configuration, team collaboration, availability of activity data and information secrecy. Although the number of relevant publications is low, we believe that the terms are not buzzwords, they imply important challenges that the security and software communities shall address to help organizations develop secure software while applying DevOps processes.","n":0.081}}},{"i":878,"$":{"0":{"v":"Information assurance techniques","n":0.577},"1":{"v":"The assurance technique is a fundamental component of the assurance ecosystem; it is the mechanism by which we assess security to derive a measure of assurance. Despite this importance, the characteristics of these assurance techniques have not been comprehensively explored within academic research from the perspective of industry stakeholders. Here, a framework of 20 \"assurance techniques\" is defined along with their interdependencies. A survey was conducted which received 153 responses from industry stakeholders, in order to determine perceptions of the characteristics of these assurance techniques. These characteristics include the expertise required, number of people required, time required for completion, effectiveness and cost. The extent to which perceptions differ between those in practitioner and management roles is considered. The findings were then used to compute a measure of cost-effectiveness for each assurance technique. Survey respondents were also asked about their perceptions of complementary assurance techniques. These findings were used to establish 15 combinations, of which the combined effectiveness and cost-effectiveness was assessed.","n":0.079}}},{"i":879,"$":{"0":{"v":"How is security testing done in agile teams? A cross-case analysis of four software teams","n":0.258},"1":{"v":"Security testing can broadly be described as (1) the testing of security requirements that concerns confidentiality, integrity, availability, authentication, authorization, nonrepudiation and (2) the testing of the software to validate how much it can withstand an attack. Agile testing involves immediately integrating changes into the main system, continuously testing all changes and updating test cases to be able to run a regression test at any time to verify that changes have not broken existing functionality. Software companies have a challenge to systematically apply security testing in their processes nowadays. There is a lack of guidelines in practice as well as empirical studies in real-world projects on agile security testing; industry in general needs a more systematic approach to security. The findings of this research are not surprising, but at the same time are alarming. The lack of knowledge on security by agile teams in general, the large dependency on incidental pen-testers, and the ignorance in static testing for security are indicators that security testing is highly under addressed and that more efforts should be addressed to security testing in agile teams.","n":0.074}}},{"i":880,"$":{"0":{"v":"An Empirical Study on the Relationship between Software Security Skills, Usage and Training Needs in Agile Settings","n":0.243},"1":{"v":"Organizations recognize that protecting their assets against attacks is an important business. However, achieving what is adequate security requires taking bold steps to address security practices within the organization. In the Agile software development world, security engineering process is unacceptable as it runs counter to the agile values. Agile teams have thus approached software security activities in their own way. To improve security within agile settings requires that management understands the current practices of software security activities within their agile teams. In this study, we use survey to investigate software security usage, competence, and training needs in two agile organizations. We find that (1) The two organizations perform differently in core software security activities but are similar when activities that could be leveraged for security are considered (2) regardless of cost or benefit, skill drives the kind of activities that are performed (3) Secure design is expressed as the most important training need by all groups in both organizations (4) Effective software security adoption in agile setting is not automatic, it requires a driver.","n":0.076}}},{"i":881,"$":{"0":{"v":"Security Requirements Engineering for Software Systems: Case Studies in Support of Software Engineering Education","n":0.267},"1":{"v":"Software engineering curricula too often neglect the development of security requirements for software systems. As a consequence, programmers often produce buggy code with weak security measures. This report focuses on three case studies in which graduate students applied a novel security requirements engineering methodology to real-world software development projects. The experiences showed promise for curriculum integration in educating students about the importance of security requirements in software engineering, as well as how to develop such requirements.","n":0.115}}},{"i":882,"$":{"0":{"v":"Secure Software Design in Practice","n":0.447},"1":{"v":"This paper presents a set of practical techniques and tools for creating secure software with a special focus on the design phase of the development lifecycle. The target group is the ordinary \"developer-on-the- street\", who is not primarily interested in (or knowledgeable about) security, but must focus on designing/implementing as much functionality as possible before the deadline and on budget.","n":0.129}}},{"i":883,"$":{"0":{"v":"Technical debt and agile software development practices and processes: An industry practitioner survey","n":0.277},"1":{"v":"Abstract   Context: Contemporary software development is typically conducted in dynamic, resource-scarce environments that are prone to the accumulation of technical debt. While this general phenomenon is acknowledged, what remains unknown is how technical debt specifically manifests in and affects software processes, and how the software development techniques employed accommodate or mitigate the presence of this debt.  Objectives: We sought to draw on practitioner insights and experiences in order to classify the effects of agile method use on technical debt management, given the popularity and perceived success of agile methods. We explore the breadth of practitioners’ knowledge about technical debt; how technical debt is manifested across the software process; and the perceived effects of common agile software development practices and processes on technical debt. In doing so, we address a research gap in technical debt knowledge and provide novel and actionable managerial recommendations.  Method: We designed, tested and executed a multi-national survey questionnaire to address our objectives, receiving 184 responses from practitioners in Brazil, Finland, and New Zealand.  Results: Our findings indicate that: 1) Practitioners are aware of technical debt, although, there was under utilization of the concept, 2) Technical debt commonly resides in legacy systems, however, concrete instances of technical debt are hard to conceptualize which makes it problematic to manage, 3) Queried agile practices and processes help to reduce technical debt; in particular, techniques that verify and maintain the structure and clarity of implemented artifacts (e.g., Coding standards and Refactoring) positively affect technical debt management.  Conclusions: The fact that technical debt instances tend to have characteristics in common means that a systematic approach to its management is feasible. However, notwithstanding the positive effects of some agile practices on technical debt management, competing stakeholders’ interests remain a concern.","n":0.059}}},{"i":884,"$":{"0":{"v":"Challenges and Experiences with Applying Microsoft Threat Modeling in Agile Development Projects","n":0.289},"1":{"v":"The goal of secure software engineering is to create software that keeps performing as intended even when exposed to attacks. Threat modeling is considered to be a key activity, but can be challenging to perform for developers, and even more so in agile software development. Hence, threat modeling has not seen widespread use in agile software projects. The goal of this paper is to investigate the challenges facing adoption of threat modeling using the Microsoft approach with STRIDE. We performed a case study in a company comprising five agile development projects. We identified 21 challenges to threat modeling that emerged from our observations. We then mapped these challenges to challenges found in the literature. Some challenges overlap the findings from the literature; the extra challenges we have found in our exploratory study came mostly from the activities of asset identification and also from our observations on what happened after the threat modeling meetings. This study shows that we still have to address many challenges in order to get a proper adoption of threat modeling in agile development projects.","n":0.075}}},{"i":885,"$":{"0":{"v":"Managing Security Work in Scrum: Tensions and Challenges.","n":0.354}}},{"i":886,"$":{"0":{"v":"Fitting Security into Agile Software Development","n":0.408}}},{"i":887,"$":{"0":{"v":"R: A language and environment for statistical computing.","n":0.354}}},{"i":888,"$":{"0":{"v":"Toward Efficient Multi-Keyword Fuzzy Search Over Encrypted Outsourced Data With Accuracy Improvement","n":0.289},"1":{"v":"Keyword-based search over encrypted outsourced data has become an important tool in the current cloud computing scenario. The majority of the existing techniques are focusing on multi-keyword exact match or single keyword fuzzy search. However, those existing techniques find less practical significance in real-world applications compared with the multi-keyword fuzzy search technique over encrypted data. The first attempt to construct such a multi-keyword fuzzy search scheme was reported by Wang  et al. , who used locality-sensitive hashing functions and Bloom filtering to meet the goal of multi-keyword fuzzy search. Nevertheless, Wang’s scheme was only effective for a one letter mistake in keyword but was not effective for other common spelling mistakes. Moreover, Wang’s scheme was vulnerable to server out-of-order problems during the ranking process and did not consider the keyword weight. In this paper, based on Wang  et al. ’s scheme, we propose an efficient multi-keyword fuzzy ranked search scheme based on Wang  et al. ’s scheme that is able to address the aforementioned problems. First, we develop a new method of keyword transformation based on the uni-gram, which will simultaneously improve the accuracy and creates the ability to handle other spelling mistakes. In addition, keywords with the same root can be queried using the stemming algorithm. Furthermore, we consider the keyword weight when selecting an adequate matching file set. Experiments using real-world data show that our scheme is practically efficient and achieve high accuracy.","n":0.065}}},{"i":889,"$":{"0":{"v":"Quantitative analysis of faults and failures in a complex software system","n":0.302},"1":{"v":"The authors describe a number of results from a quantitative study of faults and failures in two releases of a major commercial software system. They tested a range of basic software engineering hypotheses relating to: the Pareto principle of distribution of faults and failures; the use of early fault data to predict later fault and failure data; metrics for fault prediction; and benchmarking fault data. For example, we found strong evidence that a small number of modules contain most of the faults discovered in prerelease testing and that a very small number of modules contain most of the faults discovered in operation. We found no evidence to support previous claims relating module size to fault density nor did we find evidence that popular complexity metrics are good predictors of either fault-prone or failure-prone modules. We confirmed that the number of faults discovered in prerelease testing is an order of magnitude greater than the number discovered in 12 months of operational use. The most important result was strong evidence of a counter-intuitive relationship between pre- and postrelease faults; those modules which are the most fault-prone prerelease are among the least fault-prone postrelease, while conversely, the modules which are most fault-prone postrelease are among the least fault-prone prerelease. This observation has serious ramifications for the commonly used fault density measure. Our results provide data-points in building up an empirical picture of the software development process.","n":0.066}}},{"i":890,"$":{"0":{"v":"Understanding bag-of-words model: A statistical framework","n":0.408},"1":{"v":"The bag-of-words model is one of the most popular representation methods for object categorization. The key idea is to quantize each extracted key point into one of visual words, and then represent each image by a histogram of the visual words. For this purpose, a clustering algorithm (e.g., K-means), is generally used for generating the visual words. Although a number of studies have shown encouraging results of the bag-of-words representation for object categorization, theoretical studies on properties of the bag-of-words model is almost untouched, possibly due to the difficulty introduced by using a heuristic clustering process. In this paper, we present a statistical framework which generalizes the bag-of-words representation. In this framework, the visual words are generated by a statistical process rather than using a clustering algorithm, while the empirical performance is competitive to clustering-based method. A theoretical analysis based on statistical consistency is presented for the proposed framework. Moreover, based on the framework we developed two algorithms which do not rely on clustering, while achieving competitive performance in object categorization when compared to clustering-based bag-of-words representations.","n":0.075}}},{"i":891,"$":{"0":{"v":"A Hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering","n":0.277},"1":{"v":"Randomized algorithms are widely used to address many types of software engineering problems, especially in the area of software verification and validation with a strong emphasis on test automation. However, randomized algorithms are affected by chance and so require the use of appropriate statistical tests to be properly analysed in a sound manner. This paper features a systematic review regarding recent publications in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms in software engineering tend to not properly account for the random nature of these algorithms. Many of the novel techniques presented clearly appear promising, but the lack of soundness in their empirical evaluations casts unfortunate doubts on their actual usefulness. In software engineering, although there are guidelines on how to carry out empirical analyses involving human subjects, those guidelines are not directly and fully applicable to randomized algorithms. Furthermore, many of the textbooks on statistical analysis are written from the viewpoints of social and natural sciences, which present different challenges from randomized algorithms. To address the questionable overall quality of the empirical analyses reported in the systematic review, this paper provides guidelines on how to carry out and properly analyse randomized algorithms applied to solve software engineering tasks, with a particular focus on software testing, which is by far the most frequent application area of randomized algorithms within software engineering. Copyright © 2012 John Wiley & Sons, Ltd.","n":0.066}}},{"i":892,"$":{"0":{"v":"Learning to rank relevant files for bug reports using domain knowledge","n":0.302},"1":{"v":"When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity. This paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code files into methods, API descriptions of library components used in the code, the bug-fixing history, and the code change history. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluated our system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the newly introduced learning-to-rank approach significantly outperforms two recent state-of-the-art methods in recommending relevant files for bug reports. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70% of the bug reports in the Eclipse Platform and Tomcat projects.","n":0.067}}},{"i":893,"$":{"0":{"v":"Robust Statistical Methods for Empirical Software Engineering","n":0.378},"1":{"v":"There have been many changes in statistical theory in the past 30 years, including increased evidence that non-robust methods may fail to detect important results. The statistical advice available to software engineering researchers needs to be updated to address these issues. This paper aims both to explain the new results in the area of robust analysis methods and to provide a large-scale worked example of the new methods. We summarise the results of analyses of the Type 1 error efficiency and power of standard parametric and non-parametric statistical tests when applied to non-normal data sets. We identify parametric and non-parametric methods that are robust to non-normality. We present an analysis of a large-scale software engineering experiment to illustrate their use. We illustrate the use of kernel density plots, and parametric and non-parametric methods using four different software engineering data sets. We explain why the methods are necessary and the rationale for selecting a specific analysis. We suggest using kernel density plots rather than box plots to visualise data distributions. For parametric analysis, we recommend trimmed means, which can support reliable tests of the differences between the central location of two or more samples. When the distribution of the data differs among groups, or we have ordinal scale data, we recommend non-parametric methods such as Cliff's ź or a robust rank-based ANOVA-like method.","n":0.067}}},{"i":894,"$":{"0":{"v":"Do Code and Comments Co-Evolve? On the Relation between Source Code and Comment Changes","n":0.267},"1":{"v":"Comments are valuable especially for program understanding and maintenance, but do developers comment their code? To which extent do they add comments or adapt them when they evolve the code? We examine the question whether source code and associated comments are really changed together along the evolutionary history of a software system. In this paper, we describe an approach to map code and comments to observe their co-evolution over multiple versions. We investigated three open source systems (i.e., ArgoUML, Azureus, and JDT core) and describe how comments and code co-evolved over time. Some of our findings show that: 1) newly added code - despite its growth rate - barely gets commented; 2) class and method declarations are commented most frequently but far less, for example, method calls; and 3) that 97% of comment changes are done in the same revision as the associated source code change.","n":0.083}}},{"i":895,"$":{"0":{"v":"Information retrieval on Turkish texts","n":0.447},"1":{"v":"In this study, we investigate information retrieval (IR) on Turkish texts using a large-scale test collection that contains 408,305 documents and 72 ad hoc queries. We examine the effects of several stemming options and query-document matching functions on retrieval performance. We show that a simple word truncation approach, a word truncation approach that uses language-dependent corpus statistics, and an elaborate lemmatizer-based stemmer provide similar retrieval effectiveness in Turkish IR. We investigate the effects of a range of search conditions on the retrieval performance; these include scalability issues, query and document length effects, and the use of stopword list in indexing. © 2008 Wiley Periodicals, Inc.","n":0.098}}},{"i":896,"$":{"0":{"v":"Dominance statistics: Ordinal analyses to answer ordinal questions.","n":0.354},"1":{"v":"Much behavioral rescarch involves comparing the central tendencies of different groups, or of the same subjects under different conditions, and the usual analysis is some form of mean comparison. This article suggests that an ordinal statistic, d, is often more appropriate. d compares the number of times a score from one group or condition is higher than one from the other, compared with the reverse. Compared to mean comparisons, d is more robust and equally or more powerful; it is invariant under transformation; and it often conforms more closely to the experimenter's research hypothesis. It is suggested that inferences from d be based on sample estimates of its variance rather than on the more traditional assumption of identical distributions","n":0.092}}},{"i":897,"$":{"0":{"v":"Lightweight Transformation and Fact Extraction with the srcML Toolkit","n":0.333},"1":{"v":"The srcML toolkit for lightweight transformation and fact-extraction of source code is described. srcML is an XML format for C/C++/Java source code. The open source toolkit that includes the source-to-srcML and srcML-to-source translators for round-trip reverse engineering is freely available. The direct use of XPath and XSLT is supported, an archive format for large projects is included, and a rich set of input and output formats through a command-line interface is available. Applying transformations and formulating queries using srcML is very convenient. Application use-cases of transformations and fact-extraction are shown and demonstrated to be practical and scalable.","n":0.102}}},{"i":898,"$":{"0":{"v":"Defect Prediction: Accomplishments and Future Challenges","n":0.408},"1":{"v":"As software systems play an increasingly important role in our lives, their complexity continues to increase. The increased complexity of software systems makes the assurance of their quality very difficult. Therefore, a significant amount of recent research focuses on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention for over 40 years is software defect prediction, where predictions are made to determine where future defects might appear. Since then, there have been many studies and many accomplishments in the area of software defect prediction. At the same time, there remain many challenges that face that field of software defect prediction. The paper aims to accomplish four things. First, we provide a brief overview of software defect prediction and its various components. Second, we revisit the challenges of software prediction models as they were seen in the year 2000, in order to reflect on our accomplishments since then. Third, we highlight our accomplishments and current trends, as well as, discuss the game changers that had a significant impact on software defect prediction. Fourth, we highlight some key challenges that lie ahead in the near (and not so near) future in order for us as a research community to tackle these future challenges.","n":0.069}}},{"i":899,"$":{"0":{"v":"A taxonomy of crowdsourcing based on task complexity","n":0.354},"1":{"v":"Although a great many different crowdsourcing approaches are available to those seeking to accomplish individual or organizational tasks, little research attention has yet been given to characterizing how those approaches might be based on task characteristics. To that end, we conducted an extensive review of the crowdsourcing landscape, including a look at what types of taxonomies are currently available. Our review found that no taxonomy explored the multidimensional nature of task complexity. This paper develops a taxonomy whose specific intent is the classification of approaches in terms of the types of tasks for which they are best suited. To develop this task-based taxonomy, we followed an iterative approach that considered over 100 well-known examples of crowdsourcing. The taxonomy considers three dimensions of task complexity: (a) task structure - is the task well-defined, or does it require a more open-ended solution; (2) task interdependence - can the task be solved by an individual, or does it require a community of problem solvers; and (3) task commitment - what level of commitment is expected from crowd members? Based on this taxonomy, we identify seven categories of crowdsourcing and discuss prototypical examples of each approach. Furnished with such an understanding, one should be able to determine which crowdsourcing approach is most suitable for a particular task situation.","n":0.068}}},{"i":900,"$":{"0":{"v":"Towards statistical prioritization for software product lines testing","n":0.354},"1":{"v":"Software Product Lines (SPLs) are inherently difficult to test due to the combinatorial explosion of the number of products to consider. To reduce the number of products to test, sampling techniques such as combinatorial interaction testing have been proposed. They usually start from a feature model and apply a coverage criterion (e.g. pairwise feature interaction or dissimilarity) to generate tractable, fault-finding, lists of configurations to be tested. Prioritization can also be used to sort/generate such lists, optimizing coverage criteria or weights assigned to features. However, current sampling/prioritization techniques barely take product behaviour into account. We explore how ideas of statistical testing, based on a usage model (a Markov chain), can be used to extract configurations of interest according to the likelihood of their executions. These executions are gathered in featured transition systems, compact representation of SPL behaviour. We discuss possible scenarios and give a prioritization procedure validated on a web-based learning management software.","n":0.081}}},{"i":901,"$":{"0":{"v":"Predicting Bugs from History","n":0.5},"1":{"v":"Version and bug databases contain a wealth of information about software failures— how the failure occurred, who was affected, and how it was fixed. Such defect information can be automatically mined from software archives; and it frequently turns out that some modules are far more defect-prone than others. How do these differences come to be? We research how code properties like (a) code complexity, (b) the problem domain, (c) past history, or (d) process quality affect software defects, and how their correlation with defects in the past can be used to predict future software properties—where the defects are, how to fix them, as well as the associated cost.","n":0.096}}},{"i":902,"$":{"0":{"v":"Will This Bug-Fixing Change Break Regression Testing?","n":0.378},"1":{"v":"Context: Software source code is frequently changed for fixing revealed bugs. These bug-fixing changes might introduce unintended system behaviors, which are inconsistent with scenarios of existing regression test cases, and consequently break regression testing. For validating the quality of changes, regression testing is a required process before submitting changes during the development of software projects. Our pilot study shows that 48.7% bug-fixing changes might break regression testing at first run, which means developers have to run regression testing at least a couple of times for 48.7% changes. Such process can be tedious and time consuming. Thus, before running regression test suite, finding these changes and corresponding regression test cases could be helpful for developers to quickly fix these changes and improve the efficiency of regression testing. Goal: This paper proposes bug- fixing change impact prediction (BFCP), for predicting whether a bug-fixing change will break regression testing or not before running regression test cases, by mining software change histories. Method: Our approach employs the machine learning algorithms and static call graph analysis technique. Given a bug-fixing change, BFCP first predicts whether it will break existing regression test cases; second, if the change is predicted to break regression test cases, BFCP can further identify the might-be-broken test cases. Results: Results of experiments on 552 real bug-fixing changes from four large open source projects show that BFCP could achieve prediction precision up to 83.3%， recall up to 92.3%， and F-score up to 81.4%. For identifying the might-be-broken test cases, BFCP could achieve 100% recall.","n":0.063}}},{"i":903,"$":{"0":{"v":"Does the level of detail of UML diagrams affect the maintainability of source code?: a family of experiments","n":0.236},"1":{"v":"Although the UML is considered to be the de facto standard notation with which to model software, there is still resistance to model-based development. UML modeling is perceived to be expensive and not necessarily cost-effective. It is therefore important to collect empirical evidence concerning the conditions under which the use of UML makes a practical difference. The focus of this paper is to investigate whether and how the Level of Detail (LoD) of UML diagrams impacts on the performance of maintenance tasks in a model-centric approach. A family of experiments consisting of one controlled experiment and three replications has therefore been carried out with 81 students with different abilities and levels of experience from 3 countries (The Netherlands, Spain, and Italy). The analysis of the results of the experiments indicates that there is no strong statistical evidence as to the influence of different LoDs. The analysis suggests a slight tendency toward better results when using low LoD UML diagrams, especially if used for the modification of the source code, while a high LoD would appear to be helpful in understanding the system. The participants in our study also favored low LoD diagrams because they were perceived as easier to read. Although the participants expressed a preference for low LoD diagrams, no statistically significant conclusions can be drawn from the set of experiments. One important finding attained from this family of experiments was that the participants minimized or avoided the use of UML diagrams, regardless of their LoD. This effect was probably the result of using small software systems from well-known domains as experimental materials.","n":0.062}}},{"i":904,"$":{"0":{"v":"Is Learning-to-Rank Cost-Effective in Recommending Relevant Files for Bug Localization?","n":0.316},"1":{"v":"Software bug localization aiming to determine the locations needed to be fixed for a bug report is one of the most tedious and effort consuming activities in software debugging. Learning-to-rank (LR) is the state-of-the-art approach proposed by Ye et al. to recommending relevant files for bug localization. Ye et al.'s experimental results show that the LR approach significantly outperforms previous bug localization approaches in terms of \"precision\" and \"accuracy\". However, this evaluation does not take into account the influence of the size of the recommended files on the efficiency in detecting bugs. In practice, developers will generally spend more code inspection effort to detect bugs if larger files are recommended. In this paper, we use six large-scale open-source Java projects to evaluate the LR approach in the context of effort-aware bug localization. Our results, surprisingly, show that, when taking into account the code inspection effort to detect bugs, the LR approach is similar to or even worse than the standard VSM (Vector Space Model), a naive IR-based bug localization approach.","n":0.077}}},{"i":905,"$":{"0":{"v":"TF-SIDF: Term frequency, sketched inverse document frequency","n":0.378},"1":{"v":"Exact calculation of the TF-IDF weighting function in massive streams of documents involves challenging memory space requirements. In this work, we propose TF-SIDF, a novel solution for extracting relevant words from streams of documents with a high number of terms. TF-SIDF relies on the Count-Min Sketch data structure, which allows to estimate the counts of all the terms in the stream. Results of the experiments conducted with two dataset show that this sketch-based algorithm achieves good approximations of the TF-IDF weighting values (as a rule, the top terms with highest TF-IDF values remaining the same), while substantial savings in memory usage are observed. It is also observed that the performance is highly correlated with the sketch size, and that wider sketch configurations are preferable given the same sketch size.","n":0.088}}},{"i":906,"$":{"0":{"v":"Signing Off: The State of the Journal","n":0.378}}},{"i":907,"$":{"0":{"v":"Progressive Random Sampling With Stratification","n":0.447},"1":{"v":"A number of applications, including claims made under federal social welfare programs, requires retrospective sampling over multiple time periods. A common characteristic of such samples is that population members could appear in multiple time periods. When this occurs, and when the marginal cost of obtaining multiperiod information is minimum for a member appearing in the sample of the period being actively sampled, the progressive random sampling (PRS) method developed by the authors earlier can be applied. This paper enhances the progressive random sampling method by combining it with stratification schemes; the resultant stratified progressive random sampling (SPRS) technique is shown to provide significant improvement over traditional sampling techniques whenever stratification is appropriate. An empirical example based on a data transformation of a real-world application is provided to illustrate the practical application of the technique.","n":0.086}}},{"i":908,"$":{"0":{"v":"Robust statistical methods","n":0.577}}},{"i":909,"$":{"0":{"v":"Requirements Engineering: Processes and Techniques","n":0.447},"1":{"v":"Requirements Engineering Processes and Techniques Why this book was written The value of introducing requirements engineering to trainee software engineers is to equip them for the real world of software and systems development. What is involved in Requirements Engineering? As a discipline, newly emerging from software engineering, there are a range of views on where requirements engineering starts and finishes and what it should encompass. This book offers the most comprehensive coverage of the requirements engineering process to date - from initial requirements elicitation through to requirements validation. How and Which methods and techniques should you use? As there is no one catch-all technique applicable to all types of system, requirements engineers need to know about a range of different techniques. Tried and tested techniques such as data-flow and object-oriented models are covered as well as some promising new ones. They are all based on real systems descriptions to demonstrate the applicability of the approach. Who should read it? Principally written for senior undergraduate and graduate students studying computer science, software engineering or systems engineering, this text will also be helpful for those in industry new to requirements engineering. Accompanying Website: http: //www.comp.lancs.ac.uk/computing/resources/re Visit our Website: http://www.wiley.com/college/wws","n":0.071}}},{"i":910,"$":{"0":{"v":"User Stories Applied: For Agile Software Development","n":0.378},"1":{"v":"Agile requirements: discovering what your users really want. With this book, you will learn to: Flexible, quick and practical requirements that work Save time and develop better software that meets users' needs Gathering user stories -- even when you can't talk to users How user stories work, and how they differ from use cases, scenarios, and traditional requirements Leveraging user stories as part of planning, scheduling, estimating, and testing Ideal for Extreme Programming, Scrum, or any other agile methodology ----------------------------------------------------------------------------------------------------------Thoroughly reviewed and eagerly anticipated by the agile community, User Stories Applied offers a requirements process that saves time, eliminates rework, and leads directly to better software.The best way to build software that meets users' needs is to begin with \"user stories\": simple, clear, brief descriptions of functionality that will be valuable to real users. In User Stories Applied, Mike Cohn provides you with a front-to-back blueprint for writing these user stories and weaving them into your development lifecycle.You'll learn what makes a great user story, and what makes a bad one. You'll discover practical ways to gather user stories, even when you can't speak with your users. Then, once you've compiled your user stories, Cohn shows how to organize them, prioritize them, and use them for planning, management, and testing. User role modeling: understanding what users have in common, and where they differ Gathering stories: user interviewing, questionnaires, observation, and workshops Working with managers, trainers, salespeople and other \"proxies\" Writing user stories for acceptance testing Using stories to prioritize, set schedules, and estimate release costs Includes end-of-chapter practice questions and exercisesUser Stories Applied will be invaluable to every software developer, tester, analyst, and manager working with any agile method: XP, Scrum... or even your own home-grown approach.ADDISON-WESLEY PROFESSIONALBoston, MA 02116www.awprofessional.comISBN: 0-321-20568-5","n":0.059}}},{"i":911,"$":{"0":{"v":"Embracing change with extreme programming","n":0.447},"1":{"v":"Traditional software engineering means have been characterized by a rather predictable process in the past. Users tell once and for all exactly what they want. Programmers design the system that will deliver those features. They code it; test it, and all is well. But all was not always well. The users did not tell once and for all exactly what they wanted. They changed their minds, and the users were not the only problem. Programmers could misjudge their progress. The academic software engineering community took the high cost of changing software as a challenge, creating technologies like relational databases, modular programming, and information hiding. This is where extreme programming comes in. Rather than planning, analyzing, and designing for the far-flung future, XP exploits the reduction in the cost of changing software to do all of these activities a little at a time, throughout software development. The paper discusses the major practices of XP.","n":0.081}}},{"i":912,"$":{"0":{"v":"Agile software development: the business of innovation","n":0.378},"1":{"v":"The rise and fall of the dotcom-driven Internet economy shouldn't distract us from seeing that the business environment continues to change at a dramatically increasing pace. To thrive in this turbulent environment, we must confront the business need for relentless innovation and forge the future workforce culture. Agile software development approaches, such as extreme programming, Crystal methods, lean development, Scrum, adaptive software development (ASD) and others, view change from a perspective that mirrors today's turbulent business and technology environment.","n":0.113}}},{"i":913,"$":{"0":{"v":"Project management: cost, time and quality, two best guesses and a phenomenon, its time to accept other success criteria","n":0.229},"1":{"v":"Abstract   This paper provides some thoughts about success criteria for IS–IT project management. Cost, time and quality (The Iron Triangle), over the last 50 years have become inextricably linked with measuring the success of project management. This is perhaps not surprising, since over the same period those criteria are usually included in the description of project management. Time and costs are at best, only guesses, calculated at a time when least is known about the project. Quality is a phenomenon, it is an emergent property of peoples different attitudes and beliefs, which often change over the development life-cycle of a project. Why has project management been so reluctant to adopt other criteria in addition to the Iron Triangle, such as stakeholder benefits against which projects can be assessed? This paper proposes a new framework to consider success criteria, The Square Route.","n":0.084}}},{"i":914,"$":{"0":{"v":"New directions on agile methods: a comparative analysis","n":0.354},"1":{"v":"Agile software development methods have caught the attention of software engineers and researchers worldwide. Scientific research is yet scarce. This paper reports results from a study, which aims to organize, analyze and make sense out of the dispersed field of agile software development methods. The comparative analysis is performed using the method's life-cycle coverage, project management support, type of practical guidance, fitness-for-use and empirical evidence as the analytical lenses. The results show that agile software development methods, without rationalization, cover certain/different phases of the software development life-cycle and most of the them do not offer adequate support for project management. Yet, many methods still attempt to strive for universal solutions (as opposed to situation appropriate) and the empirical evidence is still very limited Based on the results, new directions are suggested In principal it is suggested to place emphasis on methodological quality -- not method quantity.","n":0.083}}},{"i":915,"$":{"0":{"v":"Agile Project Management with Scrum","n":0.447},"1":{"v":"Backdrop: The science of Scrum New management responsibilities The ScrumMaster Bringing order from chaos The product owner Planning a Scrum project Project reporting The team Scaling projects using Scrum","n":0.186}}},{"i":916,"$":{"0":{"v":"Goal identification and refinement in the specification of software-based information systems","n":0.302}}},{"i":917,"$":{"0":{"v":"Mapping CMMI Project Management Process Areas to SCRUM Practices","n":0.333},"1":{"v":"Over the past years, the capability maturity model (CMM) and capability maturity model integration (CMMI) have been broadly used for assessing organizational maturity and process capability throughout the world. However, the rapid pace of change in information technology has caused increasing frustration to the heavyweight plans, specifications, and other documentation imposed by contractual inertia and maturity model compliance criteria. In light of that, agile methodologies have been adopted to tackle this challenge. The aim of our paper is to present mapping between CMMI and one of these methodologies, Scrum. It shows how Scrum addresses the Project Management Process Areas of CMMI. This is useful for organizations that have their plan-driven process based on the CMMI model and are planning to improve its processes toward agility or to help organizations to define a new project management framework based on both CMMI and Scrum practices.","n":0.084}}},{"i":918,"$":{"0":{"v":"Challenges and future trends in software requirements prioritization","n":0.354},"1":{"v":"The right requirements are considered as a part and parcel of software quality. Since the emergence of software engineering the perfect requirements have a deeper effect on the overall quality of software systems. This research paper is focusing on the shortcomings or limitations of existing software requirements prioritization techniques and paves the way for researchers to explore new horizons in software requirements prioritization. Most of the techniques are solving the plight of small projects or toy applications. There is not a single evidence of a successful prioritization technique that would solve the problem of large set of requirements. The innovative requirements prioritization approaches are required for systems where user requirements increase in hundreds and even in thousands. The existing techniques are not providing sufficient automation for such systems due to their certain limitations.","n":0.087}}},{"i":919,"$":{"0":{"v":"Theories underlying requirements engineering: an overview of NATURE at Genesis","n":0.316},"1":{"v":"NATURE is a collaborative basic research project on theories underlying requirements engineering funded by the ESPRIT III program of the European communities. Its goals are to develop a theory of knowledge representation that embraces subject, usage and development worlds surrounding the system, including expressive freedoms; a theory of domain engineering that facilitates the identification, acquisition and formalization of domain knowledge as well as similarity-based matching and classifying of software engineering knowledge; and a process engineering theory that promotes context and decision-based control of the development process. These theories are integrated and evaluated in a prototype environment constructed around an extended version of the conceptual modeling language Telos. >","n":0.096}}},{"i":920,"$":{"0":{"v":"A conceptual model of client-driven agile requirements prioritization: results of a case study","n":0.277},"1":{"v":"Requirements (re)prioritization is an essential mechanism of agile development approaches to maximize the value for the clients and to accommodate changing requirements. Yet, in the agile Requirements Engineering (RE) literature, very little is known about how agile (re)prioritization happens in practice. Conceptual models about this process are missing, which, in turn, makes it difficult for both practitioners and researchers to reason about requirements decision-making at inter-iteration time. We did a multiple case study on agile requirements prioritization methods to yield a conceptual model for understanding the inter-iteration prioritization process. The model is derived by using interview data from practitioners in 8 development organizations. Such a model makes explicit the concepts that are used tacitly in the agile requirements prioritization practice and can be used for structuring future empirical investigations about this topic, and for analyzing, supporting, and improving the process in real-life projects.","n":0.084}}},{"i":921,"$":{"0":{"v":"Group recommendation algorithms for requirements prioritization","n":0.408},"1":{"v":"Group recommendation is successfully applied in different domains such as Interactive Television, Ambient Intelligence, and e-Tourism. The focus of this paper is to analyze the applicability of group recommendation to requirements prioritization. We provide an overview of relevant group recommendation heuristics and report the results of an empirical study which focused on the analysis of the prediction quality of these heuristics.","n":0.128}}},{"i":922,"$":{"0":{"v":"Prioritizing software requirements in an industrial setting","n":0.378},"1":{"v":"The planning of additional features and releases is a major concern for commercial software companies. We describe how, in collaboration with Ericsson Radio Systems, we developed and tested an industrially useful approach to software requirements prioritization.","n":0.167}}},{"i":923,"$":{"0":{"v":"Sample selection: an algorithm for requirements prioritization","n":0.378},"1":{"v":"In this paper, we illustrate sample selection as an effective technique for surveying customers in order to prioritize requirements. This technique allows customers to rank a relatively small set of items then combines these rankings over a large number of customers to determine an ordering for a large requirements set. The algorithm selects a subset of items to present to the next participant based on previous responses, the size of the requirements set, and the number of customers. By giving each item a fair opportunity, a complete ordering of the requirements set is possible. We utilize a simulation to verify the technique and to determine if the results meet the objectives.","n":0.095}}},{"i":924,"$":{"0":{"v":"A survey of mobile cloud computing: architecture, applications, and approaches","n":0.316},"1":{"v":"Together with an explosive growth of the mobile applications and emerging of cloud computing concept, mobile cloud computing (MCC) has been introduced to be a potential technology for mobile services. MCC integrates the cloud computing into the mobile environment and overcomes obstacles related to the performance (e.g., battery life, storage, and bandwidth), environment (e.g., heterogeneity, scalability, and availability), and security (e.g., reliability and privacy) discussed in mobile computing. This paper gives a survey of MCC, which helps general readers have an overview of the MCC including the definition, architecture, and applications. The issues, existing solutions, and approaches are presented. In addition, the future research directions of MCC are discussed. Copyright © 2011 John Wiley & Sons, Ltd.","n":0.092}}},{"i":925,"$":{"0":{"v":"Consumer value creation in mobile banking services","n":0.378},"1":{"v":"The paper presents findings of the study that explored consumer value creation in various mobile banking services. New electronic channels are replacing the more traditional ones. Mobile devices represent the recent development in electronic service distribution. An exploratory study was conducted on experienced electronic banking customers by using a qualitative in-depth interviewing method. The findings increase the understanding of customer-perceived value and value creation on the basis of attributes of mobile services and customer-perceived disadvantages of mobile phones in electronic banking context. The findings allow practitioners to improve their services and marketing strategies and pass on information to the academics about interesting future research areas.","n":0.098}}},{"i":926,"$":{"0":{"v":"A novel methodology for efficient throughput evaluation in virtualized routers","n":0.316},"1":{"v":"This paper analyzes a novel methodology for calculating the throughput in a device, which hosts multiple virtualized network interconnect devices (i.e. virtual routers). The proposed methodology, which extends the well-known procedure (for non-virtualized IP routers) adopted from RFC 2544, considers the impact of heterogeneity of the offered load at the level of virtual routers. The utility of this methodology is demonstrated, analyzing the throughput of virtualized routers by four different virtualization platforms that use two different techniques, which are the paravirtualization (Xen and Citrix Xen) and the OS-level virtualization (Linux Containers and Jails). The results indicate that the virtualization platforms behave differently to distribution of traffic load among virtual routers. Finally, the need for the proposed methodology is motivated by performing extensive throughput tests on the aforementioned platforms at different work points of the network device (i.e. different offered traffic load distribution between virtual routers).","n":0.083}}},{"i":927,"$":{"0":{"v":"An evaluation of cloud-based mobile services with limited capacity: a linear approach","n":0.289},"1":{"v":"Mobile computing is pervading networks at an increasing speed as mobile devices are used with diverse forms of wireless technologies to access data. This paper evaluates different cloud-supported mobile services subject to limited capacity, as the selection of a service may introduce additional costs, such as those that derive from the additional amount of memory required for processing. In this context, a novel linear model and algorithm in the mobile cloud computing environment are proposed from the service capacity perspective, considering the cost that derives from the unused capacity. The probability of overutilization or underutilization of the selected service is also researched, once a linear growth in the number of users occurs. To further make effective and strategic investment decisions when selecting the appropriate cloud-based mobile service to lease off, the model formulation is based on a cost–benefit appraisal. The proposed quantification approach is evaluated with respect to four different case scenarios, exploiting a web tool that has been developed as a proof of concept and implementing the algorithm to calculate and compare the benefits and costs in the mobile cloud-based service level.","n":0.074}}},{"i":928,"$":{"0":{"v":"Predicting and quantifying the technical debt in cloud software engineering","n":0.316},"1":{"v":"Identifying and managing effectively the Technical Debt has become an issue of great importance over recent years. In cloud marketplaces, where the cloud services can be leased, the difficulty to promptly predict and manage the Technical Debt has a significant impact. In this paper, we examine the Technical Debt, which stems from budget constraints during the software development process as well as the capacity of a cloud service. In this context, the budget and the cloud service selection decisions may introduce Technical Debt. Towards reaching a conclusion, two approaches are taken into consideration. Initially, a cost estimation approach is researched, which is related to implementing Software as a Service (SaaS) in the cloud for three scenarios aiming to predict the incurrence of the Technical Debt in the future. The Constructive Cost Model (COCOMO) is exploited, in order to estimate the implementation cost and define a range of secureness. In addition, a Technical Debt quantification approach is adopted, which is associated with leasing a cloud Software as a Service (SaaS), towards indicating the most appropriate cloud service to be selected.","n":0.075}}},{"i":929,"$":{"0":{"v":"A Fluctuation-Based Modelling Approach to Quantification of the Technical Debt on Mobile Cloud-Based Service Level","n":0.258},"1":{"v":"Enterprise mobility has become a top technology priority for companies over recent years and many organizations are accelerating the adoption of mobile cloud application models. The mobile cloud can be considered as a marketplace, where the mobile services of the mobile cloud-based system architectures can be leased off via the cloud. In this context, this paper elaborates on a novel fluctuation-based quantification model, which is based on a cost-benefit appraisal, adopting a non- linear and asymmetric approach. The proposed model aims to predict the incurrence and the risk of entering into a new technical debt (TD) in the future and provide insights to inform effective investment decision making. The lease of a cloud- based mobile service was considered, when developing the formula, and the research approach is investigated with respect to the cost that derives from the unused capacity. The probability of overutilization or underutilization of the selected service is examined, as fluctuations in the number of users are forecasted. A quantification tool has been also developed as a proof of concept, implementing the proposed model and intending to quantify and evaluate the technical debt on mobile cloud-based service level, when fluctuations in the demand occur.","n":0.071}}},{"i":930,"$":{"0":{"v":"Resource usage prediction for optimal and balanced provision of multimedia services","n":0.302},"1":{"v":"This paper proposes a novel network architecture for optimal and balanced provision of multimedia services, exploiting a resource prediction system. This architecture enables for the long-term prediction of multimedia services future demands, based on the history of previous network resources usage. The proposed research approach provides the opportunity for the optimal distribution of streaming data, among Content Delivery Networks, cloud-based providers and Home Media Gateways. The short-term prediction that is performed, enables for making the proper decisions by the system, according to specific network metrics, towards achieving higher Quality of Service and Quality of Experience for the end users. The validity of the proposed system is verified through several sets of extended experimental simulation tests, carried out under controlled simulation conditions.","n":0.091}}},{"i":931,"$":{"0":{"v":"Preemptive Management of Model Driven Technical Debt for Improving Software Quality","n":0.302},"1":{"v":"Technical debt has been the subject of numerous studies over the last few years. To date, most of the research has concentrated on management (detection, quantification, and decision making) approaches ?most performed at code and implementation levels through various static analysis tools. However, if practitioners are to adopt model driven techniques, then the management of technical debt also requires that we address this problem during the specification and architectural phases. This position paper discusses several questions that need to be addressed in order to improve the quality of software architecture by exploring the management of technical debt during modeling, and suggests various lines of research that are worthwhile subjects for further investigation.","n":0.094}}},{"i":932,"$":{"0":{"v":"The Technical Debt in Cloud Software Engineering: A Prediction-Based and Quantification Approach","n":0.289}}},{"i":933,"$":{"0":{"v":"Virtual Network Functions exploitation through a prototype resource management framework","n":0.316},"1":{"v":"This paper elaborates on the design of a prototype framework for the exploitation of virtual Network Functions (NFs) and virtual Network Resources (NRs) in a cloud-based network architecture. The proposed framework is based on a Marketplace approach, utilizing a centralized network architecture, where the exploitation of the available NFs/NRs is managed by a Brokerage Module. This Brokerage Module applies trading policies based on a resource predictive entity that monitors the resources, towards matching the end-user demands and providing the acceptable bandwidth required.","n":0.11}}},{"i":934,"$":{"0":{"v":"A Proportional Hazards Model for the Subdistribution of a Competing Risk","n":0.302},"1":{"v":"Abstract With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we pro...","n":0.085}}},{"i":935,"$":{"0":{"v":"Putting the enterprise into the enterprise system","n":0.378},"1":{"v":"Enterprise systems present a new model of corporate computing. They allow companies to replace their existing information systems, which are often incompatible with one another, with a single, integrated system. By streamlining data flows throughout an organization, these commercial software packages, offered by vendors like SAP, promise dramatic gains in a company's efficiency and bottom line. It's no wonder that businesses are rushing to jump on the ES bandwagon. But while these systems offer tremendous rewards, the risks they carry are equally great. Not only are the systems expensive and difficult to implement, they can also tie the hands of managers. Unlike computer systems of the past, which were typically developed in-house with a company's specific requirements in mind, enterprise systems are off-the-shelf solutions. They impose their own logic on a company's strategy, culture, and organization, often forcing companies to change the way they do business. Managers would do well to heed the horror stories of failed implementations. FoxMeyer Drug, for example, claims that its system helped drive it into bankruptcy. Drawing on examples of both successful and unsuccessful ES projects, the author discusses the pros and cons of implementing an enterprise system, showing how a system can produce unintended and highly disruptive consequences. Because of an ES's profound business implications, he cautions against shifting responsibility for its adoption to technologists. Only a general manager will be able to mediate between the imperatives of the system and the imperatives of the business.","n":0.064}}},{"i":936,"$":{"0":{"v":"Regression Models and Life-Tables","n":0.5},"1":{"v":"The analysis of censored failure times is considered. It is assumed that on each individual arc available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.","n":0.12}}},{"i":937,"$":{"0":{"v":"Architectural Innovation: The Reconfiguration of Existing Product Technologies and The Failure of Established Firms","n":0.267}}},{"i":938,"$":{"0":{"v":"Mining version histories to guide software changes","n":0.378},"1":{"v":"We apply data mining to version histories in order to guide programmers along related changes: \"Programmers who changed these functions also changed....\" Given a set of existing changes, the mined association rules 1) suggest and predict likely further changes, 2) show up item coupling that is undetectable by program analysis, and 3) can prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict further locations to be changed; the best predictive power is obtained for changes to existing software. In our evaluation based on the history of eight popular open source projects, ROSE's topmost three suggestions contained a correct location with a likelihood of more than 70 percent.","n":0.094}}},{"i":939,"$":{"0":{"v":"Enterprise resource planning: Implementation procedures and critical success factors","n":0.333},"1":{"v":"Abstract   Enterprise resource planning (ERP) systems are highly complex information systems. The implementation of these systems is a difficult and high cost proposition that places tremendous demands on corporate time and resources. Many ERP implementations have been classified as failures because they did not achieve predetermined corporate goals. This article identifies success factors, software selection steps, and implementation procedures critical to a successful implementation. A case study of a largely successful ERP implementation is presented and discussed in terms of these key factors.","n":0.11}}},{"i":940,"$":{"0":{"v":"Competing Risk Regression Models for Epidemiologic Data","n":0.378},"1":{"v":"Competing events can preclude the event of interest from occurring in epidemiologic data and can be analyzed by using extensions of survival analysis methods. In this paper, the authors outline 3 regression approaches for estimating 2 key quantities in competing risks analysis: the cause-specific relative hazard (csRH) and the subdistribution relative hazard (sdRH). They compare and contrast the structure of the risk sets and the interpretation of parameters obtained with these methods. They also demonstrate the use of these methods with data from the Women’s Interagency HIV Study established in 1993, treating time to initiation of highly active antiretroviral therapy or to clinical disease progression as competing events. In our example, women with an injection drug use history were less likely than those without a history of injection drug use to initiate therapy prior to progression to acquired immunodeficiency syndrome or death by both measures of association (csRH ¼ 0.67, 95% confidence interval: 0.57, 0.80 and sdRH ¼ 0.60, 95% confidence interval: 0.50, 0.71). Moreover, the relative hazards for disease progression prior to treatment were elevated (csRH ¼ 1.71, 95% confidence interval: 1.37, 2.13 and sdRH ¼ 2.01, 95% confidence interval: 1.62, 2.51). Methods for competing risks should be used by epidemiologists, with the choice of method guided by the scientific question.","n":0.069}}},{"i":941,"$":{"0":{"v":"Critical factors for successful implementation of enterprise systems","n":0.354},"1":{"v":"Enterprise resource planning (ERP) systems have emerged as the core of successful information management and the enterprise backbone of organizations. The difficulties of ERP implementations have been widely cited in the literature but research on the critical factors for initial and ongoing ERP implementation success is rare and fragmented. Through a comprehensive review of the literature, 11 factors were found to be critical to ERP implementation success – ERP teamwork and composition; change management program and culture; top management support; business plan and vision; business process reengineering with minimum customization; project management; monitoring and evaluation of performance; effective communication; software development, testing and troubleshooting; project champion; appropriate business and IT legacy systems. The classification of these factors into the respective phases (chartering, project, shakedown, onward and upward) in Markus and Tanis’ ERP life cycle model is presented and the importance of each factor is discussed.","n":0.083}}},{"i":942,"$":{"0":{"v":"End user development","n":0.577},"1":{"v":"End-User Development: An Emerging Paradigm.- Psychological Issues in End-User Programming.- More Natural Programming Languages and Environments.- What Makes End-User Development Tick? 13 Design Guidelines.- An Integrated Software Engineering Approach for End-User Programmers.- Component-Based Approaches to Tailorable Systems.- Natural Development of Nomadic Interfaces Based on Conceptual Descriptions.- End User Development of Web Applications.- End-User Development: The Software Shaping Workshop Approach.- Participatory Programming: Developing Programmable Bioinformatics Tools for End-Users.- Challenges for End-User Development in Intelligent Environments.- Fuzzy Rewriting.- Breaking It Up: An Industrial Case Study of Component-Based Tailorable Software Design.- End-User Development as Adaptive Maintenance.- Supporting Collaborative Tailoring.- EUD as Integration of Components Off-The-Shelf: The Role of Software Professionals Knowledge Artifacts.- Organizational View of End-User Development.- A Semiotic Framing for End-User Development.- Meta-design: A Framework for the Future of End-User Development.- Feasibility Studies for Programming in Natural Language.- Future Perspectives in End-User Development.","n":0.084}}},{"i":943,"$":{"0":{"v":"Shifting Innovation to Users via Toolkits","n":0.408},"1":{"v":"In the traditional new product development process, manufacturers first explore user needs and then develop responsive products. Developing an accurate understanding of a user need is not simple or fast or cheap, however. As a result, the traditional approach is coming under increasing strain as user needs change more rapidly, and as firms increasingly seek to serve \"markets of one.\"Toolkits for user innovation is an emerging alternative approach in which manufacturers actuallyabandon the attempt to understand user needs in detail in favor of transferringneed-related aspects of product and service development to users. Experience in fields where the toolkit approach has been pioneered show custom products being developed much more quickly and at a lower cost. In this paper we explore toolkits for user innovation and explain why and how they work.","n":0.087}}},{"i":944,"$":{"0":{"v":"A critical success factors model for ERP implementation","n":0.354},"1":{"v":"An effective IT infrastructure can support a business vision and strategy; a poor, decentralized one can break a company. More and more companies are turning to off-the-shelf ERP (enterprise resource planning) solutions for IT planning and legacy systems management. The authors have developed a framework to help managers successfully plan and implement an ERP project.","n":0.135}}},{"i":945,"$":{"0":{"v":"Antecedents of knowledge transfer from consultants to clients in enterprise system implementations","n":0.289},"1":{"v":"Enterprise resource planning (ERP) systems and other complex information systems represent critical organizational resources. For such systems, firms typically use consultants to aid in the implementation process. Client firms expect consultants to transfer their implementation knowledge to their employees so that they can contribute to successful implementations and learn to maintain the systems independent of the consultants. This study examines the antecedents of knowledge transfer in the context of such an interfirm complex information systems implementation environment. Drawing from the knowledge transfer, information systems, and communication literatures, an integrated theoretical model is developed that posits that knowledge transfer is influenced by knowledge-related, motivational, and communication-related factors. Data were collected from consultant-and-client matched-pair samples from 96 ERP implementation projects. Unlike most prior studies, a behavioral measure of knowledge transfer that incorporates the application of knowledge was used. The analysis suggests that all three groups of factors influence knowledge transfer, and provides support for 9 of the 13 hypotheses. The analysis also confirms two mediating relationships. These results (1) adapt prior research, primarily done in non-IS contexts, to the ERP implementation context, (2) enhance prior findings by confirming the significance of an antecedent that has previously shown mixed results, and (3) incorporate new IS-related constructs and measures in developing an integrated model that should be broadly applicable to the interfirm IS implementation context and other IS situations. Managerial and research implications are discussed.","n":0.066}}},{"i":946,"$":{"0":{"v":"Performance Impacts of Information Technology: Is Actual Usage the Missing Link?","n":0.302},"1":{"v":"The relationship between investment in information technology (IT) and its effect on organizational performance continues to interest academics and practitioners. In many cases, due to the nature of the research design employed, this stream of research has been unable to identify the impact of individual technologies on organizational performance. This study posits that the driver of IT impact is not the investment in the technology, but the actual usage of the technology. This proposition is tested in a longitudinal setting of a healthcare system comprising eight hospitals.Monthly data for a three-year period on various financial and nonfinancial measures of hospital performance and technology usage were analyzed. The data analysis provides evidence for the technology usage-performance link after controlling for various external factors. Technology usage was positively and significantly associated with measures of hospital revenue and quality, and this effect occurred after time lags. The analysis was triangulated using three measures of technology usage. The general support for the principal proposition of this paper that \"actual usage\" may be a key variable in explaining the impact of technology on performance suggests that omission of this variable may be a missing link in IT payoff analyses.","n":0.072}}},{"i":947,"$":{"0":{"v":"ERP implementation: a compilation and analysis of critical success factors","n":0.316},"1":{"v":"Purpose – To explore the current literature base of critical success factors (CSFs) of ERP implementations, prepare a compilation, and identify any gaps that might exist.Design/methodology/approach – Hundreds of journals were searched using key terms identified in a preliminary literature review. Successive rounds of article abstract reviews resulted in 45 articles being selected for the compilation. CSF constructs were then identified using content analysis methodology and an inductive coding technique. A subsequent critical analysis identified gaps in the literature base.Findings – The most significant finding is the lack of research that has focused on the identification of CSFs from the perspectives of key stakeholders. Additionally, there appears to be much variance with respect to what exactly is encompassed by change management, one of the most widely cited CSFs, and little detail of specific implementation tactics.Research limitations/implications – There is a need to focus future research efforts...","n":0.083}}},{"i":948,"$":{"0":{"v":"Investment in Enterprise Resource Planning: Business Impact and Productivity Measures","n":0.316},"1":{"v":"Enterprise Resource Planning (ERP)software systems integrate key business and management processes within and beyond a firm's boundary.Although the business value of ERP implementations has been extensively debated in trade periodicals in the form of qualitative discussion or detailed case studies, there is little large-sample statistical evidence on whether the benefits of ERP implementation exceed the costs and risks. With multiyear multi-firm ERP implementation and financial data, we find that firms that invest in ERP tend to show higher performance across a wide variety of financial metrics. Even though there is a slowdown in business performance and productivity shortly after the implementation, financial markets consistently reward the adopters with higher market valuation (as measured by Tobin's q). Due to the lack of mid- and long-term post-implementation data, future research on the long-run impact of ERP is proposed.","n":0.086}}},{"i":949,"$":{"0":{"v":"How User Innovations Become Commercial Products: A Theoretical Investigation and Case Study","n":0.289},"1":{"v":"In this paper we model the pathways commonly traversed as user innovations are transformed into commercial products. First, one or more users recognize a new set of design possibilities and begin to innovate. They then join into communities, motivated by the increased efficiency of collective innovation. User-manufacturers then emerge, using high variable cost/low-capital production methods. Finally, as user innovation slows, the market stabilizes enough for high-capital, low variable cost manufacturing to enter. We test the model against the history of the rodeo kayak industry and find it supported. We discuss implications for \"dominant design\" theory and for innovation practice.","n":0.101}}},{"i":950,"$":{"0":{"v":"Vicious and virtuous cycles in ERP implementation : a case study of interrelations between critical success factors","n":0.243},"1":{"v":"ERP implementations are complex undertakings. Recent research has provided us with plausible critical success factors (CSFs) for such implementations. This article describes how one list of CSFs (Somers & Nelson, 2001) was used to analyse and explain project performance in one ERP implementation in the aviation industry. In this particular case, poor project performance led to a serious project crisis but this situation was turned around into a success. The list of CSFs employed was found to be helpful and appropriate in explaining both the initial failure and the eventual success of the implementation. CSFs in this case appeared to be highly correlated, ie changes in any one of them would influence most of the others as well. The reversal in performance after the project crisis was caused by substantial changes in attitudes with most of the stakeholders involved, such as top management, project management, project champion and software vendor.","n":0.082}}},{"i":951,"$":{"0":{"v":"Satisfying Heterogeneous User Needs via Innovation Toolkits: The Case of Apache Security Software","n":0.277},"1":{"v":"User needs for a given product type can be quite heterogeneous. Segmenting the market and providing solutions for average user needs in each segment is a partial answer that will typically leave many dissatisfied ? some seriously so. We hypothesize that providing users with ?toolkits for user innovation? to enable them to more easily design customized products for themselves will increase user satisfaction under these conditions. We test this hypothesis via an empirical study of Apache security software ? ?open source? software that is designed to be modifiable by skilled users. We find that heterogeneity of need is high, and that many Apache users are dissatisfied with standard security functionality on offer. We also find that users creating their own software modifications are significantly more satisfied than are non-innovating users. We conclude by suggesting that the ?toolkits for user innovation? approach to enhancing user satisfaction might be generally applicable to markets characterized by heterogeneous user needs.","n":0.08}}},{"i":952,"$":{"0":{"v":"Value Creation by Toolkits for User Innovation and Design: The Case of the Watch Market","n":0.258},"1":{"v":"This study analyzes the value created by so-called ‘‘toolkits for user innovation and design,’’ a new method of integrating customers into new product development and design. Toolkits allow customers to create their own product, which in turn is produced by the manufacturer. In the present study, questions asked were (1) if customers actually make use of the solution space offered by toolkits, and, if so, (2) how much value the self-design actually creates. In this study, a relatively simple, design-focused toolkit was used for a set of four experiments with a total of 717 participants, 267 of whom actually created their own watches. The heterogeneity of the resulting design solutions was calculated using the entropy concept, and willingness to pay (WTP) was measured by the contingent valuation method and Vickrey auctions. Entropy coefficients showed that self-designed watches vary quite widely. On the other hand, significant patterns still are visible despite this high level of entropy, meaning that customer preferences are highly heterogeneous and diverse in style but not completely random. It also was found that consumers are willing to pay a considerable price premium. Their WTP for a self-designed watch exceeds the WTP for standard watches by far, even for the best-selling standard watches of the same technical quality. On average, a 100% value increment was found for watches designed by users with the help of the toolkit. Taken together, these findings suggest that the toolkit’s ability to allow customers to customize products to suit their individual preferences creates value for them in a business-to-consumer (B2C) setting even when only a simple toolkit is employed. Alternative explanations, implications, and necessary future research are discussed.","n":0.06}}},{"i":953,"$":{"0":{"v":"Cocreation of value in a platform ecosystem: the case of enterprise software","n":0.289},"1":{"v":"It has been argued that platform technology owners cocreate business value with other firms in their platform ecosystems by encouraging complementary invention and exploiting indirect network effects. In this study, we examine whether participation in an ecosystem partnership improves the business performance of small independent software vendors (ISVs) in the enterprise software industry and how appropriability mechanisms influence the benefits of partnership. By analyzing the partnering activities and performance indicators of a sample of 1,210 small ISVs over the period 1996-2004, we find that joining a major platform owner's platform ecosystem is associated with an increase in sales and a greater likelihood of issuing an initial public offering (IPO). In addition, we show that these impacts are greater when ISVs have greater intellectual property rights or stronger downstream capabilities. This research highlights the value of interoperability between software products, and stresses that value cocreation and appropriation are not mutually exclusive strategies in interfirm collaboration.","n":0.081}}},{"i":954,"$":{"0":{"v":"Product derivation in software product families: a case study","n":0.333},"1":{"v":"From our experience with several organizations that employ software product families, we have learned that, contrary to popular belief, deriving individual products from shared software assets is a time-consuming and expensive activity. In this paper we therefore present a study that investigated the source of those problems. We provide the reader with a framework of terminology and concepts regarding product derivation. In addition, we present several problems and issues we identified during a case study at two large industrial organizations that are relevant to other, for example, comparable or less mature organizations.","n":0.104}}},{"i":955,"$":{"0":{"v":"Economics of Product Development by Users: the Impact of Sticky Local Information","n":0.289},"1":{"v":"Those who solve more of a given type of problem tend to get better at it-which suggests that problems of any given type should be brought to specialists for a solution. However, in this paper we argue that agency-related costs and information transfer costs (\"sticky\" local information) will tend drive the locus of problem-solving in the opposite direction-away from problem-solving by specialist suppliers, and towards those who directly benefit from a solution and who have difficult-to-transfer local information about a particular application being solved, such as the direct users of a product or service. We examine the actual location of design activities in two fields in which custom products are produced by \"mass-customization\" methods: application-specific integrated circuits (ASICs) and computer telephony integration (CTI) systems. In both, we find that users rather than suppliers are the actual designers of the application-specific portion of the product types examined. We offer anecdotal evidence that the pattern of user-based customization we have documented in these two fields is in fact quite general, and we discuss implications for research and practice.","n":0.075}}},{"i":956,"$":{"0":{"v":"A Unified Approach for Developing Software Reliability Growth Models in the Presence of Imperfect Debugging and Error Generation","n":0.236},"1":{"v":"In this paper, we propose two general frameworks for deriving several software reliability growth models based on a non-homogeneous Poisson process (NHPP) in the presence of imperfect debugging and error generation. The proposed models are initially formulated for the case when there is no differentiation between failure observation and fault removal testing processes, and then extended for the case when there is a clear differentiation between failure observation and fault removal testing processes. During the last three decades, many software reliability growth models (SRGM) have been developed to describe software failures as a random process, and can be used to evaluate development status during testing. With SRGM, software engineers can easily measure (or forecast) the software reliability (or quality), and plot software reliability growth charts. It is not easy to select the best model from a plethora of models available. There are few SRGM in the literature of software engineering that differentiates between failure observation and fault removal processes. In real software development environments, the number of failures observed need not be the same as the number of faults removed. Due to the complexity of software systems, and an incomplete understanding of software, the testing team may not be able to remove the fault perfectly on observation of a failure, and the original fault may remain, resulting in a phenomenon known as imperfect debugging, or get replaced by another fault causing error generation. In the case of imperfect debugging, the fault content of the software remains the same; while in the case of error generation, the fault content increases as the testing progresses. Removal of observed faults may result in the introduction of new faults.","n":0.06}}},{"i":957,"$":{"0":{"v":"COTS-based systems top 10 list","n":0.447},"1":{"v":"Presents a COTS-based system (CBS) software defect-reduction list as hypotheses, rather than results, that also serve as software challenges for enhancing our empirical understanding of CBSs. The hypotheses are: (1) more than 99% of all executing computer instructions come from COTS products (each instruction passed a market test for value); (2) more than half the features in large COTS software products go unused; (3) the average COTS software product undergoes a new release every 8-9 months, with active vendor support for only its latest three releases; (4) CBS development and post-deployment efforts can scale as high as the square of the number of independently developed COTS products targeted for integration; (5) CBS post-deployment costs exceed CBS development costs; (6) although glue-code development usually accounts for less than half the total CBS software development effort, the effort per line of glue code averages about three times the effort per line of developed applications code; (7) non-development costs, such as licensing fees, are significant, and projects must plan for and optimize them; (8) CBS assessment and tailoring efforts vary significantly by COTS product class (operating system, database management system, user interface, device driver, etc.); (9) personnel capability and experience remain the dominant factors influencing CBS development productivity; and (10) CBS is currently a high-risk activity, with effort and schedule overruns exceeding non-CBS software overruns, yet many systems have used COTS successfully for cost reduction and early delivery.","n":0.065}}},{"i":958,"$":{"0":{"v":"An empirical approach to studying software evolution","n":0.378},"1":{"v":"With the approach of the new millennium, a primary focus in software engineering involves issues relating to upgrading, migrating, and evolving existing software systems. In this environment, the role of careful empirical studies as the basis for improving software maintenance processes, methods, and tools is highlighted. One of the most important processes that merits empirical evaluation is software evolution. Software evolution refers to the dynamic behaviour of software systems as they are maintained and enhanced over their lifetimes. Software evolution is particularly important as systems in organizations become longer-lived. However, evolution is challenging to study due to the longitudinal nature of the phenomenon in addition to the usual difficulties in collecting empirical data. We describe a set of methods and techniques that we have developed and adapted to empirically study software evolution. Our longitudinal empirical study involves collecting, coding, and analyzing more than 25000 change events to 23 commercial software systems over a 20-year period. Using data from two of the systems, we illustrate the efficacy of flexible phase mapping and gamma sequence analytic methods, originally developed in social psychology to examine group problem solving processes. We have adapted these techniques in the context of our study to identify and understand the phases through which a software system travels as it evolves over time. We contrast this approach with time series analysis. Our work demonstrates the advantages of applying methods and techniques from other domains to software engineering and illustrates how, despite difficulties, software evolution can be empirically studied.","n":0.063}}},{"i":959,"$":{"0":{"v":"Demand Heterogeneity and Technology Evolution: Implications for Product and Process Innovation","n":0.302},"1":{"v":"The evolution of technology has been a central issue in the strategy and organizations literature. However, the focus of much of this work has been on what is essentially the \"supply side\" of technical change--the evolution of firm capabilities. We present a demand-based view of technology evolution that is focused on the interaction between technology development and the demand environment in which the technology is ultimately evaluated. We develop a formal computer simulation model that explicitly considers the influence of heterogeneity in market demand--the presence of consumers with different needs and requirements--on firms' innovation choices. The model is used to examine the dynamics of product and process innovation (Utterback and Abernathy 1975). The analysis reveals that demand heterogeneity offers an alternative to supply-side explanations of the technology life cycle. Further, by considering the implications of decreasing marginal utility from performance improvements, the model highlights the role of \"technologically satisfied\" consumers in shaping innovation incentives, and suggests a rationale for a new stage in the technology life cycle characterized by increasing performance at a stable price. The stage has not yet been treated formally in the literature, but is widely observed, most prominently in digital and information-based technologies.","n":0.071}}},{"i":960,"$":{"0":{"v":"Modularity and Innovation in Complex Systems","n":0.408},"1":{"v":"The problem of designing, coordinating, and managing complex systems has been central to the management and organization literature. Recent writings have tended to offer modularity as at least a partial solution to this design problem. However, little attention has been paid to the problem of identifying what constitutes an appropriate modularization of a complex system. We develop a formal simulation model that allows us to carefully examine the dynamics of innovation and performance in complex systems. The model points to the trade-off between the destabilizing effects of overly refined modularization and the modest levels of search and a premature fixation on inferior designs that can result from excessive levels of integration. The analysis highlights an asymmetry in this trade-off, with excessively refined modules leading to cycling behavior and a lack of performance improvement. We discuss the implications of these arguments for product and organization design.","n":0.083}}},{"i":961,"$":{"0":{"v":"Direct and indirect effects in a survival context.","n":0.354},"1":{"v":"A cornerstone of epidemiologic research is to understand the causal pathways from an exposure to an outcome. Mediation analysis based on counterfactuals is an important tool when addressing such questions. However, none of the existing techniques for formal mediation analysis can be applied to survival data. This is a severe shortcoming, as many epidemiologic questions can be addressed only with censored survival data. A solution has been to use a number of Cox models (with and without the potential mediator), but this approach does not allow a causal interpretation and is not mathematically consistent. In this paper, we propose a simple measure of mediation in a survival setting. The measure is based on counterfactuals, and measures the natural direct and indirect effects. The method allows a causal interpretation of the mediated effect (in terms of additional cases per unit of time) and is mathematically consistent. The technique is illustrated by analyzing socioeconomic status, work environment, and long-term sickness absence. A detailed implementation guide is included in an online eAppendix (http://links.lww.com/EDE/A476).","n":0.077}}},{"i":962,"$":{"0":{"v":"Effects of Process Maturity on Quality, Cycle Time, and Effort in Software Product Development","n":0.267},"1":{"v":"The information technology (IT) industry is characterized by rapid innovation and intense competition. To survive, IT firms must develop high quality software products on time and at low cost. A key issue is whether high levels of quality can be achieved without adversely impacting cycle time and effort. Conventional beliefs hold that processes to improve software quality can be implemented only at the expense of longer cycle times and greater development effort. However, an alternate view is that quality improvement, faster cycle time, and effort reduction can be simultaneously attained by reducing defects and rework. In this study, we empirically investigate the relationship between process maturity, quality, cycle time, and effort for the development of 30 software products by a major IT firm. We find that higher levels of process maturity as assessed by the Software Engineering Institute's Capability Maturity Modelâ¢ are associated with higher product quality, but also with increases in development effort. However, our findings indicate that the reductions in cycle time and effort due to improved quality outweigh the increases from achieving higher levels of process maturity. Thus, the net effect of process maturity is reduced cycle time and development effort.","n":0.072}}},{"i":963,"$":{"0":{"v":"Software Effort, Quality, and Cycle Time: A Study of CMM Level 5 Projects","n":0.277},"1":{"v":"The Capability Maturity Model (CMM) has become a popular methodology for improving software development processes with the goal of developing high-quality software within budget and planned cycle time. Prior research literature, while not exclusively focusing on CMM level 5 projects, has identified a host of factors as determinants of software development effort, quality, and cycle time. In this study, we focus exclusively on CMM level 5 projects from multiple organizations to study the impacts of highly mature processes on effort, quality, and cycle time. Using a linear regression model based on data collected from 37 CMM level 5 projects of four organizations, we find that high levels of process maturity, as indicated by CMM level 5 rating, reduce the effects of most factors that were previously believed to impact software development effort, quality, and cycle time. The only factor found to be significant in determining effort, cycle time, and quality was software size. On the average, the developed models predicted effort and cycle time around 12 percent and defects to about 49 percent of the actuals, across organizations. Overall, the results in this paper indicate that some of the biggest rewards from high levels of process maturity come from the reduction in variance of software development outcomes that were caused by factors other than software size","n":0.068}}},{"i":964,"$":{"0":{"v":"Team familiarity, role experience, and performance: evidence from indian software services","n":0.302},"1":{"v":"Much of the literature on team learning views experience as a unidimensional concept captured by the cumulative production volume of, or the number of projects completed by, a team. Implicit in this approach is the assumption that teams are stable in their membership and internal organization. In practice, however, such stability is rare, as the composition and structure of teams often changes over time or between projects. In this paper, we use detailed data from an Indian software services firm to examine how such changes may affect the accumulation of experience within, and the performance of, teams. We find that the level of team familiarity (i.e., the average number of times that each member has worked with every other member of the team) has a significant positive effect on performance, but we observe that conventional measures of the experience of individual team members (e.g., years at the firm) are not consistently related to performance. We do find, however, that the role experience of individuals in a team (i.e., years in a given role within a team) is associated with better team performance. Our results offer an approach for capturing the experience held by fluid teams and highlight the need to study context-specific measures of experience, including role experience. In addition, our findings provide insight into how the interactions of team members may contribute to the development of broader firm capabilities.","n":0.066}}},{"i":965,"$":{"0":{"v":"Analysis of preventive maintenance in transactions based software systems","n":0.333},"1":{"v":"Preventive maintenance of operational software systems, a novel technique for software fault tolerance, is used specifically to counteract the phenomenon of software \"aging\". However, it incurs some overhead. The necessity to do preventive maintenance, not only in general purpose software systems of mass use, but also in safety-critical and highly available systems, clearly indicates the need to follow an analysis based approach to determine the optimal times to perform preventive maintenance. In this paper, we present an analytical model of a software system which serves transactions. Due to aging, not only the service rate of the software decreases with time, but also the software itself experiences crash/hang failures which result in its unavailability. Two policies for preventive maintenance are modeled and expressions for resulting steady state availability, probability that an arriving transaction is lost and an upper bound on the expected response time of a transition are derived. Numerical examples are presented to illustrate the applicability of the models.","n":0.079}}},{"i":966,"$":{"0":{"v":"Enterprise resource planning: multisite ERP implementations","n":0.408}}},{"i":967,"$":{"0":{"v":"Impact of Performance-Based Contracting on Product Reliability: An Empirical Analysis","n":0.316},"1":{"v":"Using a proprietary data set provided by a major manufacturer of aircraft engines, we empirically investigate how product reliability is impacted by the use of two different types of after-sales maintenance support contracts: time and material contracts (T&MC) and performance-based contracts (PBC). We offer a number of competing arguments based on the theory of incentives that establish why product reliability may increase or decrease under PBC. We build a two-stage econometric model that explicitly accounts for the endogeneity of contract choices, and find evidence of a positive and significant effect of PBC on product reliability. The estimation of our model indicates that product reliability is higher by 25%--40% under PBC compared to under T&MC, once the endogeneity of contract choice is taken into account. Our results are consistent with two mechanisms for reliability improvement under PBC: more frequent scheduled maintenance and better care performed in each maintenance event.\r\n\r\nThis paper was accepted by Martin Lariviere, operations management.","n":0.08}}},{"i":968,"$":{"0":{"v":"Learning from Experience in Software Development: A Multilevel Analysis","n":0.333},"1":{"v":"This study examines whether individuals, groups, and organizational units learn from experience in software development and whether this learning improves productivity. Although prior research has found the existence of learning curves in manufacturing and service industries, it is not clear whether learning curves also apply to knowledge work like software development. We evaluate the relative productivity impacts from accumulating specialized experience in a system, diversified experience in related and unrelated systems, and experience from working with others on modification requests (MRs) in a telecommunications firm, which uses an incremental software development methodology. Using multilevel modeling, we analyze extensive data archives covering more than 14 years of systems development work on a major telecommunications product dating from the beginning of its development process. Our findings reveal that the relative importance of the different types of experience differs across levels of analysis. Specialized experience has the greatest impact on productivity for MRs completed by individual developers, whereas diverse experience in related systems plays a larger role in improving productivity for MRs and system releases completed by groups and organizational units. Diverse experience in unrelated systems has the least influence on productivity at all three levels of analysis. Our findings support the existence of learning curves in software development and provide insights into when specialized or diverse experience may be more valuable.","n":0.068}}},{"i":969,"$":{"0":{"v":"The Impact of Design and Code Reviews on Software Quality: An Empirical Study Based on PSP Data","n":0.243},"1":{"v":"This research investigates the effect of review rate on defect removal effectiveness and the quality of software products, while controlling for a number of potential confounding factors. Two data sets of 371 and 246 programs, respectively, from a personal software process (PSP) approach were analyzed using both regression and mixed models. Review activities in the PSP process are those steps performed by the developer in a traditional inspection process. The results show that the PSP review rate is a significant factor affecting defect removal effectiveness, even after accounting for developer ability and other significant process variables. The recommended review rate of 200 LOC/hour or less was found to be an effective rate for individual reviews, identifying nearly two-thirds of the defects in design reviews and more than half of the defects in code reviews.","n":0.086}}},{"i":970,"$":{"0":{"v":"Analysing and interpreting competing risk data.","n":0.408},"1":{"v":"When competing risks are present, two types of analysis can be performed: modelling the cause specific hazard and modelling the hazard of the subdistribution. This paper contrasts these two methods and presents the benefits of each. The interpretation is specific to the analysis performed. When modelling the cause specific hazard, one performs the analysis under the assumption that the competing risks do not exist. This could be beneficial when, for example, the main interest is whether the treatment works in general. In modelling the hazard of the subdistribution, one incorporates the competing risks in the analysis. This analysis compares the observed incidence of the event of interest between groups. The latter analysis is specific to the structure of the observed data and it can be generalized only to another population with similar competing risks.","n":0.086}}},{"i":971,"$":{"0":{"v":"Attention Shaping and Software Risk-A Categorical Analysis of Four Classical Risk Management Approaches","n":0.277},"1":{"v":"This paper examines software risk management in a novel way, emphasizing the ways in which managers address software risks through sequential attention shaping and intervention. Software risks are interpreted as incongruent states within a socio-technical model of organizational change that includes task, structure, technology, and actors. Such incongruence can lead to failures in developing or implementing the system and thus to major losses. Based on this model we synthesize a set of software risk factors and risk resolution techniques, which cover the socio-technical components and their interactions. We use the model to analyze how four classical risk management approaches-McFarlan's portfolio approach, Davis' contingency approach, Boehm's software risk approach, and Alter's and Ginzberg's implementation approach-shape managerial attention. This analysis shows that the four approaches differ significantly in their view of the manager's role and possible actions. We advise managers to be aware of the limitations of each approach and to combine them to orchestrate comprehensive risk management practices in a context. Overall, the paper provides a new interpretation of software risk management which goes beyond a narrow system rationalism by suggesting a contingent, contextual, and multivariate view of software development.","n":0.073}}},{"i":972,"$":{"0":{"v":"The structural complexity of software an experimental test","n":0.354},"1":{"v":"This research examines the structural complexity of software and, specifically, the potential interaction of the two dominant dimensions of structural complexity, coupling and cohesion. Analysis based on an information processing view of developer cognition results in a theoretically driven model with cohesion as a moderator for a main effect of coupling on effort. An empirical test of the model was devised in a software maintenance context utilizing both procedural and object-oriented tasks, with professional software engineers as participants. The results support the model in that there was a significant interaction effect between coupling and cohesion on effort, even though there was no main effect for either coupling or cohesion. The implication of this result is that, when designing, implementing, and maintaining software to control complexity, both coupling and cohesion should be considered jointly, instead of independently. By providing guidance on structuring software for software professionals and researchers, these results enable software to continue as the solution of choice for a wider range of richer, more complex problems.","n":0.077}}},{"i":973,"$":{"0":{"v":"Evaluating the cost of software quality","n":0.408}}},{"i":974,"$":{"0":{"v":"ERP Investments and the Market Value of Firms: Toward an Understanding of Influential ERP Project Variables","n":0.25},"1":{"v":"This study contributes to the growing body of literature on the value of enterprise resource planning (ERP) investments at the firm level. Using an organization integration lens that takes into account investments in complementary resources as well as an options thinking logic about the value of an ERP platform, we argue that not all ERP purchases have the same potential impact at the firm level due to ERP project decisions made at the time of purchase. Based on a sample of 116 investment announcements in United Statesbased firms between 1997 and 2001, we find support for our hypotheses that ERP projects with greater functional scope (two or more value-chain modules) or greater physical scope (multiple sites) result in positive, higher shareholder returns. Furthermore, the highest increases in returns (3.29) are found for ERP purchases with greater functional scope and greater physical scope; negative returns are found for projects with lesser functional scope and lesser physical scope. These findings provide empirical support for prior theory about the organizational integration benefits of ERP systems, the contribution of complementary resource investments to the business value of IT investments, and the growth options associated with IT platform investments. The article concludes with implications of our firm-level findings for this first wave of enterprise systems.","n":0.069}}},{"i":975,"$":{"0":{"v":"Design capital and design moves: the logic of digital business strategy","n":0.302},"1":{"v":"As information technology becomes integral to the products and services in a growing range of industries, there has been a corresponding surge of interest in understanding how firms can effectively formulate and execute digital business strategies. This fusion of IT within the business environment gives rise to a strategic tension between investing in digital artifacts for long-term value creation and exploiting them for short-term value appropriation. Further, relentless innovation and competitive pressures dictate that firms continually adapt these artifacts to changing market and technological conditions, but sustained profitability requires scalable architectures that can serve a large customer base and stable interfaces that support integration across a diverse ecosystem of complementary offerings. The study of digital business strategy needs new concepts and methods to examine how these forces are managed in pursuit of competitive advantage. We conceptualize the logic of digital business strategy in terms of two constructs: design capital (i.e., the cumulative stock of designs owned or controlled by a firm) and design moves (i.e., the discrete strategic actions that enlarge, reduce, or modify a firm's stock of designs). We also identify two salient dimensions of design capital, namely, option value and technical debt. Using embedded case studies of four firms, we develop a rich conceptual model and testable propositions to lay out a design-based logic of digital business strategy. This logic highlights the interplay between design moves and design capital in the context of digital business strategy and contributes to a growing body of insights that link the design of digital artifacts to competitive strategy and firm-level performance.","n":0.062}}},{"i":976,"$":{"0":{"v":"Design complexity measurement and testing","n":0.447},"1":{"v":"System designers can quantify the complexity of a software design by using a trio of finely tuned design metrics.","n":0.229}}},{"i":977,"$":{"0":{"v":"Model selection in competing risks regression.","n":0.408},"1":{"v":"In the analysis of time-to-event data, the problem of competing risks occurs when an individual may experience one, and only one, of m different types of events. The presence of competing risks complicates the analysis of time-to-event data, and standard survival analysis techniques such as Kaplan–Meier estimation, log-rank test and Cox modeling are not always appropriate and should be applied with caution. Fine and Gray developed a method for regression analysis that models the hazard that corresponds to the cumulative incidence function. This model is becoming widely used by clinical researchers and is now available in all the major software environments. Although model selection methods for Cox proportional hazards models have been developed, few methods exist for competing risks data. We have developed stepwise regression procedures, both forward and backward, based on AIC, BIC, and BICcr (a newly proposed criteria that is a modified BIC for competing risks data subject to right censoring) as selection criteria for the Fine and Gray model. We evaluated the performance of these model selection procedures in a large simulation study and found them to perform well. We also applied our procedures to assess the importance of bone mineral density in predicting the absolute risk of hip fracture in the Women's Health Initiative–Observational Study, where mortality was the competing risk. We have implemented our method as a freely available R package called crrstep. Copyright © 2013 John Wiley & Sons, Ltd.","n":0.065}}},{"i":978,"$":{"0":{"v":"Using a case study to test the role of three key social enablers in ERP implementation","n":0.25},"1":{"v":"The literature indicates that three key social enablers--strong and committed leadership, open and honest communication, and a balanced and empowered implementation team are necessary conditions/precursors for successful enterprise resource planning (ERP) implementation. In a longitudinal positivist case study, we find that, while all three enablers may contribute to ERP implementation success, only strong and committed leadership can be empirically established as a necessary condition. This presents a challenge to future ERP researchers for resolving apparent contradictions between the existing literature and the results of our analysis. One possible direction for future research would be to undertake an interpretive re-examination of the rationalisitic assumptions that underlie much of the existing literature on ERP systems implementation.","n":0.094}}},{"i":979,"$":{"0":{"v":"Competing risks regression for clustered data","n":0.408},"1":{"v":"A population average regression model is proposed to assess the marginal effects of covariates\non the cumulative incidence function when there is dependence across individuals\nwithin a cluster in the competing risks setting. This method extends the Fine?Gray model proportional hazards\n model for the subdistribution to situations where individualswithin a cluster may be correlated due to unobserved shared factors. \nEstimators of the regression parameters in the marginal model are developed under an independence\nworking assumption, where the correlation across individuals within a cluster is\ncompletely unspecified. The estimators are consistent and asymptotically normal, and\nvariance estimation may be achieved without specifying the form of the dependence\nacross individuals. A simulation study evidences that the inferential procedures perform\nwell with realistic sample sizes. The practical utility of the methods is illustrated with\ndata from the European Bone Marrow Transplant Registry.","n":0.087}}},{"i":980,"$":{"0":{"v":"Metrics for Managing Research and Development in the Context of the Product Family","n":0.277},"1":{"v":"The paper proposes methods to measure the performance of research and development in new product development. We frame these measures in the context of evolving product families in the technology-based firm. Our goal is to more clearly understand the dynamics of platform renewal and derivative product generation and their consequences for long-term success. We explore the utility of the proposed methods with data gathered from a large measurement systems manufacturer. We find that the methods and measures can help management assess the technological and market leverage achieved from the firm's present and past product platforms. This provides a foundation for transforming single-product, single-period planning processes into a multi-product, multi-period form that embraces the product family and the renewal of product architecture. The research also shows the need to integrate data from engineering, manufacturing, and sales organizations to produce information for managing the growth of the firm's product families.","n":0.082}}},{"i":981,"$":{"0":{"v":"On the regression analysis of multivariate failure time data","n":0.333},"1":{"v":"SUMMARY The paper is concerned with the analysis of regression effects when individual study subjects may experience multiple failures. The proposed methods are most likely to be useful when there are a fairly large number of study subjects. Two general classes of regression models are considered in order to relate the hazard or intensity function to covariates and to preceding failure time history. Both models are of a stratified proportional hazards type. One model includes baseline intensity functions that are arbitrary as a function of time from the beginning of study, while the other includes baseline intensity functions that are arbitrary as a function of time from the study subject's immediately preceding failure. Partial likelihood functions are derived for the regression coefficients. Generalizations and illustrations are given.","n":0.089}}},{"i":982,"$":{"0":{"v":"An Updated ERP Systems Annotated Bibliography: 2001-2005","n":0.378},"1":{"v":"This study provides an updated annotated bibliography of ERP publications published in the main IS conferences and journals during the period 2001-2005, categorizing them through an ERP lifecycle-based framework that is structured in phases. The first version of this bibliography was published in 2001 (Esteves and Pastor, 2001c). However, so far, we have extended the bibliography with a significant number of new publications in all the categories used in this paper. We also reviewed the categories and some incongruities were eliminated.","n":0.111}}},{"i":983,"$":{"0":{"v":"A model to evaluate variables impacting the productivity of software maintenance projects","n":0.289},"1":{"v":"The cost of maintaining application software has been rapidly escalating, and is currently estimated to comprise from 50-80% of corporate information systems department budgets. In this research we develop an estimable production frontier model of software maintenance, using a new methodology that allows the simultaneous estimation of both the production frontier and the effects of several productivity factors. Our model allows deviations on both sides of the estimated frontier to reflect the impact of both production inefficiencies and random effects such as measurement errors.\r\n\r\nThe model is then estimated using an empirical dataset of 65 software maintenance projects from a large commercial bank. The insights obtained from the estimation results are found to be quite consistent for reasonable variations in the specification of the model. Estimates of the marginal impacts of all of the included productivity factors are obtained to aid managers in improving productivity in software maintenance.","n":0.082}}},{"i":984,"$":{"0":{"v":"Managing the Sources of Uncertainty: Matching Process and Context in Software Development","n":0.289},"1":{"v":"There is increasing interest in the literature about the notion of a contingent approach to product development process design. This interest stems from the realization that different types of projects carried out in different environments are likely to require quite different development processes if they are to be successful. Stated more formally, a contingent view implies that the performance impact of different development practices is likely to be mediated by the context in which those practices operate. This article provides evidence to support such a view.\r\n\r\n\r\n\r\nOur work examines whether projects in which the development process matches the context achieve superior performance. We focus on two sources of uncertainty that generate challenges for project teams: platform uncertainty, reflecting the uncertainty generated by the amount of new design work that must be undertaken in a project; and market uncertainty, reflecting the uncertainty faced in determining customer requirements for the product under development. We develop hypotheses for how these sources of uncertainty are likely to influence the relationships between a number of specific development practices and performance. We then test these hypotheses using data from a sample of 29 Internet software development projects.\r\n\r\n\r\n\r\nOur results provide evidence to support a contingent view of development process design. We show that in projects facing greater uncertainty, investments in architectural design, early technical feedback, and early market feedback have a stronger association with performance. The latter relationships are influenced by the specific sources from which this uncertainty stems: platform uncertainty mediating the impact of early technical feedback and market uncertainty mediating the impact of early market feedback. Our results also indicate that while greater uncertainty is associated with making later changes to a product's design, this practice is not associated with performance.\r\n\r\n\r\n\r\nOur findings suggest that managers carefully must evaluate both the levels and sources of uncertainty facing a project before designing the most appropriate process for its execution. In particular, they should explore the use of specific development practices based upon their usefulness in resolving the specific types of uncertainty faced. Importantly, these decisions must be made at the start of a project, with purposeful investments to create a process that best matches the context. Reacting to uncertainty ex-post, without such investments in place, is unlikely to prove a successful strategy.","n":0.052}}},{"i":985,"$":{"0":{"v":"Scale Economies in New Software Development","n":0.408},"1":{"v":"Abslmet-ln this paper we reconcile two opposing views regarding the presence of economies or diseconomies of scale In new software development. Our general approach hypothesizes a production function model of software development thot &lows for both increasing and drsmsing returns to sede, and argues that local scale economies or diseconomies depend upon the size of projects. Using eight different data sets, including several reported In previous rese8rch on the subject, we provide tmpirkd evidence in support of our hypothesis. Through the use of the nonparametric DEA technique we also show how to identify the mmt productive scale size that may vary across organizations. Index Tcftns-Dat. envelopment analysis, function points, productivity measurement, sepk economies. software development, source lines of code.","n":0.092}}},{"i":986,"$":{"0":{"v":"Appropriability Mechanisms and the Platform Partnership Decision: Evidence from Enterprise Software","n":0.302},"1":{"v":"We examine whether ownership of intellectual property rights IPR or downstream capabilities is effective in encouraging entry into markets complementary to a proprietary platform by preventing the platform owner from expropriating rents from start-ups. We study this question in the context of the software industry, an environment where evidence of the efficacy of IPR as a mechanism to appropriate the returns from innovation has been mixed. Entry, in our context, is measured by an independent software vendor's ISV's decision to become certified by a platform owner and produce applications compatible with the platform. We find that ISVs with a greater stock of formal IPR such as patents and copyrights, and those with stronger downstream capabilities as measured by trademarks and consulting services are more likely to join the platform, suggesting that these mechanisms are effective in protecting ISVs from the threat of expropriation. We also find that the effects of IPR on the likelihood of partnership are greater when an ISV has weak downstream capabilities or when the threat of imitation is greater, such as when the markets served by the ISV are growing quickly.\r\n\r\nThis paper was accepted by Gerard P. Cachon, information systems.","n":0.072}}},{"i":987,"$":{"0":{"v":"Analyzing the Evolution of Large-Scale Software Systems Using Design Structure Matrices and Design Rule Theory: Two Exploratory Cases","n":0.236},"1":{"v":"Designers have long recognized the value of modularity, but important software modularity principles have remained informal. According to Baldwin and Clark's (2000) design rule theory (DRT) , modular architectures add value to system designs by creating options to improve the system by substituting or experimenting on individual modules. In this paper, we examine the design evolution of two software product platforms through the modeling lens of DRT and design structure matrices (DSMs). We show that DSM models and DRT precisely explain how real- world modularization activities in one case allowed for different rates of evolution in different software modules and in another case conferred distinct strategic advantages on a firm by permitting substitution of an at-risk software module without substantial change to the rest of the system. Our results provide positive evidence that DSM and DRT can inform important aspects of large-scale software structure and evolution, having the potential to guide software architecture design activities.","n":0.08}}},{"i":988,"$":{"0":{"v":"Software Development Practices, Software Complexity, and Software Maintenance Performance: a Field Study","n":0.289},"1":{"v":"Software maintenance claims a large proportion of organizational resources. It is thought that many maintenance problems derive from inadequate software design and development practices. Poor design choices can result in complex software that is costly to support and difficult to change. However, it is difficult to assess the actual maintenance performance effects of software development practices because their impact is realized over the software life cycle. To estimate the impact of development activities in a more practical time frame, this research develops a two-stage model in which software complexity is a key intermediate variable that links design and development decisions to their downstream effects on software maintenance. The research analyzes data collected from a national mass merchandising retailer on 29 software enhancement projects and 23 software applications in a large IBM COBOL environment. Results indicate that the use of a code generator in development is associated with increased software complexity and software enhancement project effort. The use of packaged software is associated with decreased software complexity and software enhancement effort. These results suggest an important link between software development practices and maintenance performance.","n":0.074}}},{"i":989,"$":{"0":{"v":"Duration Models for Repeated Events","n":0.447},"1":{"v":"An important feature of most political events is their repeatability: nearly all political events reoccur, and theories of learning, path dependence, and institutional change all suggest that later events will differ from earlier ones. Yet, most models for event history analysis fail to account for repeated events, a fact that can yield misleading results in practice. We present a class of duration models for analyzing repeated events, discuss their properties and implementation, and offer recommendations for their use by applied researchers. We illustrate these methods through an application to widely used data on international conflict.","n":0.103}}},{"i":990,"$":{"0":{"v":"A Field Study of Scale Economies in Software Maintenance","n":0.333},"1":{"v":"Software maintenance is a major concern for organizations. Productivity gains in software maintenance can enable redeployment of Information Systems resources to other activities. Thus, it is important to understand how software maintenance productivity can be improved. In this study, we investigate the relationship between project size and software maintenance productivity. We explore scale economies in software maintenance by examining a number of software enhancement projects at a large financial services organization. We use Data Envelopment Analysis DEA to estimate the functional relationship between maintenance inputs and outputs and employ DEA-based statistical tests to evaluate returns to scale for the projects. Our results indicate the presence of significant scale economies in software maintenance, and are robust to a number of sensitivity checks. For our sample of projects, there is the potential to reduce software maintenance costs 36% by batching smaller modification projects into larger planned releases. We conclude by rationalizing why the software managers at our research site do not take advantage of scale economies in software maintenance. Our analysis considers the opportunity costs of delaying projects to batch them into larger size projects as a potential explanation for the managers' behavior.","n":0.072}}},{"i":991,"$":{"0":{"v":"The Effects of Time Pressure on Quality in Software Development: An Agency Model","n":0.277},"1":{"v":"An agency framework is used to model the behavior of software developers as they weigh concerns about product quality against concerns about missing individual task deadlines. Developers who care about quality but fear the career impact of missed deadlines may take \"shortcuts.\" Managers sometimes attempt to reduce this risk via their deadline-setting policies; a common method involves adding slack to best estimates when setting deadlines to partially alleviate the time pressures believed to encourage shortcut-taking. This paper derives a formal relationship between deadline-setting policies and software product quality. It shows that: (1) adding slack does not always preserve quality, thus, systematically adding slack is an incomplete policy for minimizing costs; (2) costs can be minimized by adopting policies that permit estimates of completion dates and deadlines that are different and; (3) contrary to casual intuition, shortcut-taking can be eliminated by setting deadlines aggressively, thereby maintaining or even increasing the time pressures under which developers work.","n":0.08}}},{"i":992,"$":{"0":{"v":"Why Do Software Firms Fail? Capabilities, Competitive Actions, and Firm Survival in the Software Industry from 1995 to 2007","n":0.229},"1":{"v":"This study examines why firms fail or survive in the volatile software industry. We provide a novel perspective by considering how software firms' capabilities and their competitive actions affect their ultimate survival. Drawing on the resource-based view (RBV), we conceptualize capabilities as a firm's ability to efficiently transform input resources into outputs, relative to its peers. We define three critical capabilities of software-producing firms---research and development (RD), marketing (MK), and operations (OP)---and hypothesize that in the dynamic, high-technology software industry, RD and MK capabilities are most important for firm survival. We then draw on the competitive dynamics literature to theorize that competitive actions distinguished by a greater emphasis on innovation-related moves will increase firm survival more than actions emphasizing resource-related moves. Finally, we postulate that firms' capabilities will complement their competitive actions in affecting firm survival. Our empirical evaluation examines a cross-sectional, time series panel of 5,827 observations on 870 software companies from 1995 to 2007. We use a stochastic frontier production function to measure the capability for each software firm in each time period. We then use the Cox proportional hazard regression technique to relate capabilities and competitive actions to software firms' failure rates. Unexpectedly, our results reveal that higher OP capability increases software firm survival more than higher MK and RD capabilities. Further, firms with a greater emphasis on innovation-related than resource-related competitive actions have a greater likelihood of survival, and this likelihood increases even further when these firms have higher MK and OP capabilities. Additional analyses of subsectors within the software industry reveal that firms producing visual applications (e.g., graphical and video game software) have the highest MK capability but the lowest OP and RD capabilities and make twice as many innovation-related as resource-related moves. These firms have the highest market values but the worst Altman Z scores, suggesting that they are valued highly but also are at high risk for failure, and indeed the firms in this sector fail at a greater rate than expected. In contrast, firms producing traditional decision-support applications and infrastructure software have different capabilities and make different competitive moves. Our findings suggest that the firms that persist and survive over the long term in the dynamic software industry are able to capitalize on their competitive actions because of their greater capabilities, and particularly OP capabilities.","n":0.051}}},{"i":993,"$":{"0":{"v":"Making Room for the Call Center","n":0.408},"1":{"v":"Abstract A call center can dramatically improve an organization's ability to serve its customers. in this article, three important questions are addressed for senior managers and executives who are establishing or expanding a call-center operation. First, as employees focus on responding to customer inquiries, will their skills become narrower and ultimately less beneficial to the organization? This article argues that skills for employees in call centers can and should be upgraded. Second, will the decentralized decision-making of the call center lead to a loss of managerial control? the article suggests ways of ensuring that the ideal configuration of decentralized decision making and centralized control takes hold in the organization after the call center is established. Third, how can an organization's teamwork and corporate spirit be maintained and enhanced after the establishment of a call center? the article offers practical advice for enhancing an organization's culture even as employees begin to ...","n":0.081}}},{"i":994,"$":{"0":{"v":"Mutual Development: A Case Study in Customer-Initiated Software Product Development","n":0.316},"1":{"v":"The paper is a case study of customer-initiated software product development. We have observed and participated in system development activities in a commercial software house (company) over a period of two years. The company produces project-planning tools for the oil and gas industry, and relies on interaction with customers for further development of its products. Our main research question is how customers and professional developers engage in mutual development mediated by shared software tools (products and support systems). We have used interviews with developers and customers as our main source of data, and identified the activities (from use to development) where customers have contributed to development. We analyze our findings in terms of co-configuration, meta-design and modding in order to name and compare the various stages of development (adaptation, generalization, improvement request, specialization, and tailoring).","n":0.086}}},{"i":995,"$":{"0":{"v":"Software Effort, Quality, and Cycle Time","n":0.408},"1":{"v":"The Capability Maturity Model (CMM) has become a popular methodology for improving software development processes with the goal of developing high-quality software within budget and planned cycle t...","n":0.189}}},{"i":996,"$":{"0":{"v":"OO metrics in practice","n":0.5},"1":{"v":"While it has long been recognized that software process improvement requires measuring both the process and its performance, experience has also shown that few universal metrics exist. The most effective measurement tools are specialized to some aspect of the task or domain being measured. The metrics as measures of code have often been related to external factors, such as software quality in the sense of defects. Software metrics studies often use single snapshots of a software project. Examining a project over a longer time frame allows consideration of other software quality facets, such as reuse and maintainability.","n":0.102}}},{"i":997,"$":{"0":{"v":"How the Incumbent Can Win: Managing Technological Transitions in the Semiconductor Industry","n":0.289},"1":{"v":"The paper reports on an empirical study of the management of technological transitions. It focuses on project-level mechanisms for the generation of knowledge through experimentation and for its accumulation through individual experience. It proposes a model that links these mechanisms to effectiveness in the management of revolutionary and evolutionary development approaches. This argument is tested with data describing projects conducted by all major competitors in the semiconductor industry. Each project was aimed at a technological transition, defined as the introduction of a major new generation of process technology. The analysis shows substantial differences among competitors in the approach taken i.e., evolutionary vs. revolutionary and results achieved. Additionally, it shows that individual organizations can migrate, over time, from evolution to revolution and vice versa. The analysis further indicates that accumulating experience and generating knowledge through experimentation are significantly associated with project performance. While product performance improvement through revolution is associated with research experience and with parallel experimentation capacity, improvement through evolution is associated with project experience and minimum experimental iteration time.","n":0.077}}},{"i":998,"$":{"0":{"v":"Event Dependence and Heterogeneity in Duration Models: The Conditional Frailty Model","n":0.302},"1":{"v":"We introduce the conditional frailty model, an event history model that separates and accounts for both event dependence and heterogeneity in repeated events processes. Event dependence and heterogeneity create within-subject correlation in event times thereby violating the assumptions of standard event history models. Simulations show the advantage of the conditional frailty model. Specifically they demonstrate the model's ability to disentangle the sources of within-subject correlation as well as the gains in both efficiency and bias of the model when compared to the widely used alternatives, which often produce conflicting conclusions. Two substantive political science problems illustrate the usefulness and interpretation of the model: state policy adoption and terrorist attacks.","n":0.096}}},{"i":999,"$":{"0":{"v":"Incremental Maintenance of Software Artifacts","n":0.447},"1":{"v":"Software is multidimensional, but the tools that support it are not. This lack of tool support causes the software artifacts representing different dimensions to evolve independently and to become inconsistent over time. In order to properly support the evolution of software, one must ensure that the different dimensions evolve concurrently. We have built a software development tool, CLIME that uses constraints implemented as database queries to ensure just this. Our approach makes the tool responsible for detecting inconsistencies between software design, specifications, documentation, source code, test cases, and other artifacts without requiring any of these to be a primary representation. The tool works incrementally as the software evolves, without imposing a particular methodology or process. It includes a front end that lets the user explore and fix current inconsistencies. This paper describes the basis for CLIME, the techniques underlying the tool, the interface provided to the programmer, the incremental maintenance of constraints between these artifacts, and our experiences","n":0.08}}},{"i":1000,"$":{"0":{"v":"Empirical Evidence on Swanson's Tri-Core Model of Information Systems Innovation","n":0.316},"1":{"v":"Research in IS innovation has been isolated and fragmented. These studies typically examine single innovations and do not effectively integrate notions of IS innovation with organizational innovation. Swanson Swanson, E. B. 1994. Information systems innovation among organizations. Management Sci.409 1069--1088. extends the prior dual-core model of innovation into a tri-core model specifically for the unique nature of IS innovation. This model provides a useful typology of IS innovation that can form the foundation for innovation theory in this important area. In this paper we present Swanson's tri-core model of IS innovation along with preliminary data to test aspects of the model proposed by Swanson. Adoption of ten IS innovations is studied using two analyses, one based only on adopter sub-samples and the other using a more rigorous treatment of nonadopters based on survival analysis. The objective of this study is simple---to test theory and encourage continued focused inquiry in IS innovation. The results of this study provide partial support for the proposed hypotheses, leading us to conclude on an optimistic note regarding the viability of this model as an integrating frame-work for IS innovation.","n":0.074}}},{"i":1001,"$":{"0":{"v":"In Search of Efficient Flexibility: Effects of Software Component Granularity on Development Effort, Defects, and Customization Effort","n":0.243},"1":{"v":"Simultaneously achieving efficiency and flexibility in enterprise software production has been a considerable challenge for firms. Newer software development paradigms such as component-based and model-driven development attempt to overcome this challenge by emphasizing modular design of complex systems. However, there is a paucity of rigorous empirical research on the use of such software methodologies and the associated extent to which trade-offs between efficiency and flexibility can be influenced. Addressing this gap, we investigate the performance outcomes of a model-driven, component-based software development methodology using data collected from an enterprise software development firm that deployed such a methodology for its product development processes. Examining the design, development, and implementation of 92 business software components of the firm's enterprise resource planning product, we discuss how the design of software components, specifically component granularity, affects development...","n":0.087}}},{"i":1002,"$":{"0":{"v":"IT Implementation Contract Design: Analytical and Experimental Investigation of IT Value, Learning, and Contract Structure","n":0.258},"1":{"v":"This article analytically and experimentally investigates how firms can best capture the business value of information technology (IT) investments through IT contract design. Using a small sample of outsourcing contracts for enterprise information technology (EIT) projects in several industries—coupled with reviews of contracts used by a major enterprise software maker—the authors determine the common provisions and structural characteristics of EIT contracts. The authors use these characteristics to develop an analytical model of optimal contract design with principal–agent techniques. The model captures a set of key characteristics of EIT contracts, including a staged, multiperiod project structure; learning; probabilistic binary outcomes; variable fee structures; possibly risk-averse agents; and implementation risks. The model characterizes conditions under which multistage contracts enable clients to create and capture greater project value than single-stage projects, and how project staging enables f...","n":0.086}}},{"i":1003,"$":{"0":{"v":"The Effectiveness of Knowledge Transfer Portfolios in Software Process Improvement: A Field Study","n":0.277},"1":{"v":"Because of challenges often experienced when deploying software, many firms have embarked on software process improvement (SPI) initiatives. Critical to the success of these initiatives is the transfer of knowledge across individuals who occupy a range of roles in various organizational units involved in software production. Prior research suggests that a portfolio of different mechanisms, employed frequently, can be required for effective knowledge transfer. However, little research exists that examines under what situations differing portfolios of mechanisms are selected. Further, it is not clear how effective different portfolio designs are. In this study, we conceptualize knowledge transfer portfolios in terms of their composition (the types of mechanisms used) and their intensity (the frequency with which the mechanisms are utilized). We hypothesize the influence of organizational design decisions on the composition and intensity of knowledge transfer portfolios for SPI. We then posit how the composition and intensity of knowledge transfer portfolios affect performance improvement. Our findings indicate that a more intense portfolio of knowledge transfer mechanisms is used when the source and recipient are proximate, when they are in a hierarchical relationship, or when they work in different units. Further, a source and recipient select direction-based portfolios when they are farther apart, in a hierarchical relationship, or work in different units. In terms of performance, our results reveal that the fit between the composition and intensity of the knowledge transfer portfolio influences the recipient's performance improvement. At lower levels of intensity direction-based portfolios are more effective, while at higher levels of intensity routine-based portfolios yield the highest performance improvement. We discuss the implications of our findings for researchers and for managers who want to promote knowledge transfer to improve software processes in their organizations.","n":0.059}}},{"i":1004,"$":{"0":{"v":"High tech, high touch: The effect of employee skills and customer heterogeneity on customer satisfaction with enterprise system support services","n":0.224},"1":{"v":"Although firms have invested significant resources in implementing enterprise software systems (ESS) to modernize and integrate their business process infrastructure, customer satisfaction with ESS has remained an understudied phenomenon. In this exploratory research study, we investigate customer satisfaction for support services of ESS and focus on employee skills and customer heterogeneity. We analyze archival customer satisfaction data from 170 real-world customer service encounters of a leading ESS vendor. Our analysis indicates that the technical and behavioral skills of customer support representatives play a major role in influencing overall customer satisfaction with ESS support services. We find that the effect of technical skills on customer satisfaction is moderated by behavioral skills. We also find that the technical skills of the support personnel are valued more by repeat customers than by new customers. We discuss the implications of these findings for managing customer heterogeneity in ESS support services and for the allocation and training of ESS support personnel.","n":0.08}}},{"i":1005,"$":{"0":{"v":"Software complexity and software maintenance: A survey of empirical research","n":0.316},"1":{"v":"A number of empirical studies have pointed to a link between software complexity and software maintenance performance. The primary purpose of this paper is to document “what is known” about this relationship, and to suggest some possible future avenues of research. In particular, a survey of the empirical literature in this area shows two broad areas of study: complexity metrics and comprehension. Much of the complexity metrics research has focused on modularity and structure metrics. The articles surveyed are summarized as to major differences and similarities in a set of detailed tables. The text is used to highlight major findings and differences, and a concluding remarks section provides a series of recommendations for future research.","n":0.093}}},{"i":1006,"$":{"0":{"v":"Does Complexity Deter Customer-Focus?","n":0.5},"1":{"v":"Economic models suggest that firms use a simple cost-benefit calculation to evaluate customer requests for new product features, but an extensive organizational literature shows the decision to implement innovation is more nuanced. We address this theoretical tension by studying how firms respond to customer requests for incremental product innovations, and how these responses change when the requested innovation is complex. Using large-sample empirical analyses combined with detailed qualitative data drawn from interviews, we find considerable variance in the relationship between customer demands, complexity, and investments in incremental innovations. The qualitative study revealed the importance of organization structures, competitive pressures, and incentives for resource allocation processes.","n":0.098}}},{"i":1007,"$":{"0":{"v":"A multidimensional model of client success when engaging external consultants","n":0.316},"1":{"v":"Too often the relationship between clients and external consultants is perceived as one of protagonist versus antagonist. Stories on dramatic, failed consultancies abound, as do related anecdotal quips. A contributing factor to many \"apparently\" failed consultancies is a poor appreciation by both the client and consultant of the client's true goals for the project and how to assess progress toward these goals. This paper presents and analyses a measurement model for assessing client success when engaging an external consultant. Three main areas of assessment are identified: 1 the consultant's recommendations, 2 client learning, and 3 consultant performance. Engagement success is empirically measured along these dimensions through a series of case studies and a subsequent survey of clients and consultants involved in 85 computer-based information system selection projects. Validation of the model constructs suggests the existence of six distinct and individually important dimensions of engagement success. Both clients and consultants are encouraged to attend to these dimensions in pre-engagement proposal and selection processes, and post-engagement evaluation of outcomes.","n":0.077}}},{"i":1008,"$":{"0":{"v":"Deriving tests from software architectures","n":0.447},"1":{"v":"Software architectures are intended to describe essential high level structural and behavioral characteristics of a system. Architecture Description Languages (ADLs) describe these characteristics in ways that can be analyzed and manipulated algorithmically. This provides a unique opportunity for deriving tests at the system level. The paper defines formal testing criteria based on architecture relations, which are paths that architectural components use to communicate. The criteria have been applied to a specific ADL. Results from a comparative empirical study on industrial software are presented.","n":0.11}}},{"i":1009,"$":{"0":{"v":"The Product Life Cycle in the Commercial Mainframe Computer Market, 1968-1982","n":0.302},"1":{"v":"We investigate product life cycles in the commercial mainframe computer market. We use hazard models with time-varying covariates to estimate the probability of product exit and Poisson models to estimate the probability of introduction. We measure the importance of different aspects of market structure, such as the degree of competitiveness, cannibalization, vintage, product niche, and firm effects. We find evidence of a relationship between the determinants of product exit and product entry.","n":0.118}}},{"i":1010,"$":{"0":{"v":"Intractable ERP: a comprehensive analysis of failed enterprise-resource-planning projects","n":0.333},"1":{"v":"An enterprise-resource-planning system --- or ERP system, for short --- is by definition \"any software system designed to support and automate the business processes of medium and large businesses.\" [16] Integrated ERP systems became popular in the early 1990's. Single monolithic pieces of software, ERP systems promised to do away with inconsistent data, incompatible formats, and uncooperative applications.Still, ERP systems come with their own, unexpected difficulties. Their tremendous generality and enormous complexity make them prone to glitches and low performance, difficult to maintain, and nightmarish to implement.This study takes a close look at four ERP-implementation failures, all of which occurred recently in American industry. It analyses possible causes that led to the disasters, and suggests software-engineering processes that help avoiding such outcomes. The Model-Based Architecting and Software Engineering (MBASE) guidelines, developed by Professors Barry Boehm and Daniel Port at the Center for Software Engineering (CSE) at USC, provide a base for these elaborations.The original workout is actually far more extensive than the abridgement published here. It can be downloaded from the CSE web page. Alternatively, it may be ordered from the author directly through email.","n":0.074}}},{"i":1011,"$":{"0":{"v":"Competing in Crowded Markets: Multimarket Contact and the Nature of Competition in the Enterprise Systems Software Industry","n":0.243},"1":{"v":"As more and more firms seek to digitize their business processes and develop new digital capabilities, the enterprise systems software (ESS) has emerged as a significant industry. ESS firms offer software components (e.g., ERP, CRM, Marketing analytics) to shape their clients' digitization strategies. With rapid rates of technological and market innovation, the ESS industry consists of several horizontal markets that form around these components. As numerous vendors compete with each other within and across these markets, many of these horizontal markets appear to be crowded with rivals. In fact, multimarket contact and presence in crowded markets appear to be the pathways through which a majority of the ESS firms compete. Though the strategy literature has demonstrated the virtues of multimarket contact, paradoxically, the same literature argues that operating in crowded markets is not wise. In particular, crowded markets increase a firm's exposure to the whirlwinds of intense competition and have deleterious consequences for financial performance. Thus, the behavior of ESS firms raises an interesting anomaly and research question: Why do ESS firms continue to compete in crowded markets if they are deemed to be bad for financial performance? We argue that the effects of rivalry in crowded markets are counteracted by a different force, in the form of the economics of demand externalities. Demand externalities occur because the customers of ESS firms expect that software components from one market will be easily integrated with those that they buy from other markets. However, with rapid rates of technological innovation and market formation and dissolution, customers experience significant ambiguity in deciding which markets and components suit their needs. Therefore, they look at crowded markets as an important signal about the legitimacy and viability of specific components for their needs. Through their presence in crowded markets, ESS firms can signal their commitment to many of the components that customers might need for their digital platforms. Customers might find that such firms are attractive because their commitments to crowded markets can mitigate concerns about compatibilities between the components purchased across several markets. This unique potential for demand externality across markets suggests that ESS vendors might, in fact, benefit from competing in many crowded markets. We test our explanations through data across three time periods from a set of ESS firms that account for more than 95% of the revenue in this market. We find that ESS firms do reap performance benefits by competing in crowded markets. More importantly, we find that they can enhance their benefits from crowded markets if they face the same competitors in multiple markets, thereby increasing their multimarket contact with rivals. These results have interesting implications not just for understanding competitive conduct in the ESS industry but also in many of the emerging digital goods industries where the markets have similar competitive characteristics to the ESS industry. Our ideas complement emerging ideas about platform models of competition in the digital goods industry and provide important directions for future research.","n":0.045}}},{"i":1012,"$":{"0":{"v":"Optimal Enhancement and Lifetime of Software Systems: A Control Theoretic Analysis","n":0.302},"1":{"v":"We develop an optimal control model to maximize the net value provided by a software system over its useful life. The model determines the initial number of features in the system, the level of dynamic enhancement effort, and the lifetime of the system. The various factors affecting these optimal choices are systems characteristics (e.g., complexity, age, quality), user learning, and process maturity. We also consider that there is a time lag between the addition of a feature and the realization of its benefit to users. The basic model is extended to consider the decision of replacing the existing system by a new one.","n":0.099}}},{"i":1013,"$":{"0":{"v":"Moving beyond the single site implementations study: How (and why) we should study the biography of packaged enterprise solutions","n":0.229}}},{"i":1014,"$":{"0":{"v":"New Product Innovation with Multiple Features and Technology Constraints","n":0.333},"1":{"v":"We model a firm's decisions about product innovation, focusing on the extent to which features should be improved or changed in the succession of models that comprise a life cycle. We show that the structure of the internal and external environment in which a firm operates suggests when to innovate to the technology frontier. The criterion is maximization of the expected present value of profits during the life cycle. Computational studies complement the theoretical results and lead to insights about when to bundle innovations across features. The formalization was influenced by extensive interviews with managers in a high-technology firm that dominates its industry.","n":0.099}}},{"i":1015,"$":{"0":{"v":"Does Component Sharing Help or Hurt Reliability? An Empirical Study in the Automotive Industry","n":0.267},"1":{"v":"Component sharing---the use of a component on multiple products within a firm's product line---is widely practiced as a means of offering high variety at low cost. Although many researchers have examined trade-offs involved in component sharing, little research has focused on the impact of component sharing on quality. In this paper, we examine how component sharing impacts one dimension of quality---reliability---defined as mean time to failure. Design considerations suggest that a component designed uniquely for a product will result in higher reliability due to the better fit of the component within the architecture of the product. On the other hand, the learning curve literature suggests that greater experience with a component can improve conformance quality, and can increase reliability via learning from end-user feedback. The engineering literature suggests that improved conformance in turn increases reliability. Sharing a component across multiple products increases experience, and hence, should increase reliability. Using data from the automotive industry, we find support for the hypothesis that higher component reliability is associated with higher cumulative experience with a component. Further, we find support for the hypothesis that higher component reliability is associated with a component that has been designed uniquely for a product. This finding suggests that the popular design strategy of component sharing can in some cases compromise product quality, via reduced reliability.","n":0.068}}},{"i":1016,"$":{"0":{"v":"Structural Complexity and Programmer Team Strategy: An Experimental Test","n":0.333},"1":{"v":"This study develops and empirically tests the idea that the impact of structural complexity on perfective maintenance of object-oriented software is significantly determined by the team strategy of programmers (independent or collaborative). We analyzed two key dimensions of software structure, coupling and cohesion, with respect to the maintenance effort and the perceived ease-of-maintenance by pairs of programmers. Hypotheses based on the distributed cognition and task interdependence theoretical frameworks were tested using data collected from a controlled lab experiment employing professional programmers. The results show a significant interaction effect between coupling, cohesion, and programmer team strategy on both maintenance effort and perceived ease-of-maintenance. Highly cohesive and low-coupled programs required lower maintenance effort and were perceived to be easier to maintain than the low-cohesive programs and high-coupled programs. Further, our results would predict that managers who strategically allocate maintenance tasks to either independent or collaborative programming teams depending on the structural complexity of software could lower their team's maintenance effort by as much as 70 percent over managers who use simple uniform resource allocation policies. These results highlight the importance of achieving congruence between team strategies employed by collaborating programmers and the structural complexity of software.","n":0.072}}},{"i":1017,"$":{"0":{"v":"Demand Heterogeneity and Technology Evolution","n":0.447},"1":{"v":"The evolution of technology has been a central issue in the strategy and organizations literature. However, the focus of much of this work has been on what is essentially the \"supply side\" of techn...","n":0.171}}},{"i":1018,"$":{"0":{"v":"How User Innovations become Commercial Products","n":0.408}}},{"i":1019,"$":{"0":{"v":"Modularity and Innovation","n":0.577},"1":{"v":"This chapter contains sections titled: Murky Clouds and the Systems Integration Metaphor, An Alternative Metaphor: The Fashion Industry, Conclusions","n":0.229}}},{"i":1020,"$":{"0":{"v":"Exploring the Structure of Complex Software Designs","n":0.378},"1":{"v":"This paper reports data from a study that seeks to characterize the differences in design structure between complex software products. We use design structure matrices (DSMs) to map dependencies be...","n":0.183}}},{"i":1021,"$":{"0":{"v":"Nonparametric estimation with recurrent competing risks data.","n":0.378},"1":{"v":"Nonparametric estimators of component and system life distributions are developed and presented for situations where recurrent competing risks data from series systems are available. The use of recurrences of components’ failures leads to improved efficiencies in statistical inference, thereby leading to resource-efficient experimental or study designs or improved inferences about the distributions governing the event times. Finite and asymptotic properties of the estimators are obtained through simulation studies and analytically. The detrimental impact of parametric model misspecification is also vividly demonstrated, lending credence to the virtue of adopting nonparametric or semiparametric models, especially in biomedical settings. The estimators are illustrated by applying them to a data set pertaining to car repairs for vehicles that were under warranty.","n":0.092}}},{"i":1022,"$":{"0":{"v":"ERP Competence-Building Mechanisms: An Exploratory Investigation of Configurations of ERP Adopters in the European and U.S. Manufacturing Sectors","n":0.236},"1":{"v":"This paper contributes to the literature on enterprise resource planning (ERP) by pursuing two objectives. First, it identifies configurations of ERP adopters that have similar needs and develop similar competencies. Second, it tests the hypothesis that, to maximize benefits from their ERP projects, organizations should align their ERP competence-building mechanisms with the ERP needs that arise from their operational environment. The analysis of a sample of manufacturing companies that implemented ERP between 1995 and 2001 uncovers four distinct configurations representing different degrees of fit between needs and competence-building mechanisms: the frugal ERP, the extensive business process reengineering (BPR), the adaptive ERP, and the straitjacket. The results support our hypothesis and suggest that the consequences of a misfit between needs and competence-building mechanisms are more severe for companies that operate in complex and dynamic environments and have informal organizational structures than for firms with rigid structures that operate in simple and stable environments.","n":0.081}}},{"i":1023,"$":{"0":{"v":"Non‐parametric Tests for Recurrent Events under Competing Risks","n":0.354},"1":{"v":"We consider a data set on nosocomial infections of patients hospitalized in a French intensive care facility. Patients may suffer from recurrent infections of different types and they also have a high risk of death. To deal with such situations, a model of recurrent events with competing risks and a terminal event is introduced. Our aim was to compare the occurrence rates of two types of events. For this purpose, we propose two tests: one to detect if the occurrence rate of a given type of event increases with time; a second to detect if the instantaneous probability of experiencing an event of a given type is always greater than the one of another type. The asymptotic properties of the test statistics are derived and Monte Carlo methods are used to study the power of the tests. Finally, the procedures developed are applied to the French nosocomial infections data set. Copyright (c) 2009 Board of the Foundation of the Scandinavian Journal of Statistics.","n":0.078}}},{"i":1024,"$":{"0":{"v":"Metrics for Managing Research and Development","n":0.408}}},{"i":1025,"$":{"0":{"v":"Ex post tests and diagnostics for a proportional hazards model","n":0.316}}},{"i":1026,"$":{"0":{"v":"A relational model of data for large shared data banks","n":0.316},"1":{"v":"Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation). A prompting service which supplies such information is not a satisfactory solution. Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed. Changes in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information.Existing noninferential, formatted data systems provide users with tree-structured files or slightly more general network models of the data. In Section 1, inadequacies of these models are discussed. A model based on n-ary relations, a normal form for data base relations, and the concept of a universal data sublanguage are introduced. In Section 2, certain operations on relations (other than logical inference) are discussed and applied to the problems of redundancy and consistency in the user's model.","n":0.076}}},{"i":1027,"$":{"0":{"v":"Refactoring Databases: Evolutionary Database Design","n":0.447},"1":{"v":"Refactoring has proven its value in a wide range of development projectshelping software professionals improve system designs, maintainability, extensibility, and performance. Now, for the first time, leading agile methodologist Scott Ambler and renowned consultant Pramodkumar Sadalage introduce powerful refactoring techniques specifically designed for database systems. Ambler and Sadalage demonstrate how small changes to table structures, data, stored procedures, and triggers can significantly enhance virtually any database designwithout changing semantics. Youll learn how to evolve database schemas in step with source codeand become far more effective in projects relying on iterative, agile methodologies. This comprehensive guide and reference helps you overcome the practical obstacles to refactoring real-world databases by covering every fundamental concept underlying database refactoring. Using start-to-finish examples, the authors walk you through refactoring simple standalone database applications as well as sophisticated multi-application scenarios. Youll master every task involved in refactoring database schemas, and discover best practices for deploying refactorings in even the most complex production environments. The second half of this book systematically covers five major categories of database refactorings. Youll learn how to use refactoring to enhance database structure, data quality, and referential integrity; and how to refactor both architectures and methods. This book provides an extensive set of examples built with Oracle and Java and easily adaptable for other languages, such as C#, C++, or VB.NET, and other databases, such as DB2, SQL Server, MySQL, and Sybase. Using this books techniques and examples, you can reduce waste, rework, risk, and costand build database systems capable of evolving smoothly, far into the future.","n":0.063}}},{"i":1028,"$":{"0":{"v":"Database systems:models, languages dwesign, and application programming","n":0.378}}},{"i":1029,"$":{"0":{"v":"Database Design and Relational Theory: Normal Forms and All That Jazz","n":0.302},"1":{"v":"What makes this book different from others on database design? Many resources on design practice do little to explain the underlying theory, and books on design theory are aimed primarily at theoreticians. In this book, renowned expert Chris Date bridges the gap by introducing design theory in ways practitioners can understanddrawing on lessons learned over four decades of experience to demonstrate why proper database design is so critical in the first place.Every chapter includes a set of exercises that show how to apply the theoretical ideas in practice, provide additional information, or ask you to prove some simple theoretical result. If youre a database professional familiar with the relational model, and have more than a passing interest in database design, this book is for you.Questions this book answers include:Why is Heaths Theorem so important? What is The Principle of Orthogonal Design? What makes some JDs reducible and others irreducible? Why does dependency preservation matter? Should data redundancy always be avoided? Can it be? Databases often stay in production for decades, and careful design is critical for avoiding subtle errors and processing problems over time. If theyre badly designed, the negative impacts can be incredibly widespread. This gentle introduction shows you how to use important theoretical results to create good database designs.","n":0.069}}},{"i":1030,"$":{"0":{"v":"RDBNorma: - A semi-automated tool for relational database schema normalization up to third normal form.","n":0.258},"1":{"v":"In this paper a tool called RDBNorma is proposed, that uses a novel approach to represent a relational database schema and its functional dependencies in computer memory using only one linked list and used for semi-automating the process of relational database schema normalization up to third normal form. This paper addresses all the issues of representing a relational schema along with its functional dependencies using one linked list along with the algorithms to convert a relation into second and third normal form by using above representation. We have compared performance of RDBNorma with existing tool called Micro using standard relational schemas collected from various resources. It is observed that proposed tool is at least 2.89 times faster than the Micro and requires around half of the space than Micro to represent a relation. Comparison is done by entering all the attributes and functional dependencies holds on a relation in the same order and implementing both the tools in same language and on same machine.","n":0.078}}},{"i":1031,"$":{"0":{"v":"Database Design Debts through Examining Schema Evolution","n":0.378},"1":{"v":"Causes of the database debt can stem from ill-conceptual, logical, and/or physical database design decisions, violations to key design databases principles, use of anti-patterns etc. In this paper, we explore the problem of relational database design debt and define the problem. We develop a taxonomy, which classifies various types of debts that can relate to conceptual, logical and physical design of a database. We define the concept of Database Design Debt, discuss their origin, causes and preventive mechanisms. We draw on MediaWiki case study and examine its database schema evolution to support our work. The contribution hopes to make database designers and application developers aware of these debts so they can minimize/avoid their consequences on a given system.","n":0.092}}},{"i":1032,"$":{"0":{"v":"DAHLIA: A visual analyzer of database schema evolution","n":0.354},"1":{"v":"In a continuously changing environment, software evolution becomes an unavoidable activity. The mining software repositories (MSR) field studies the valuable data available in software repositories such as source code version-control systems, issue/bug-tracking systems, or communication archives. In recent years, many researchers have used MSR techniques as a way to support software understanding and evolution. While many software systems are data-intensive, i.e., their central artefact is a database, little attention has been devoted to the analysis of this important system component in the context of software evolution. The goal of our work is to reduce this gap by considering the database evolution history as an additional information source to aid software evolution. We present DAHLIA (Database ScHema EvoLutIon Analysis), a visual analyzer of database schema evolution. Our tool mines the database schema evolution history from the software repository and allows its interactive, visual analysis. We describe DAHLIA and present our novel approach supporting data-intensive software evolution.","n":0.08}}},{"i":1033,"$":{"0":{"v":"Algorithm for Relational Database Normalization Up to 3NF","n":0.354},"1":{"v":"When an attempt is made to modify tables that have not been sufficiently normalized undesirable sideeffects may follow. This can be further specified as an update, insertion or deletion anomaly depending on whether the action that causes the error is a row update, insertion or deletion respectively. If a relation R has more than one key, each key is referred to as a candidate key of R. Most of the practical recent works on database normalization use a restricted definition of normal forms where only the primary key (an arbitrary chosen key) is taken into account and ignoring the rest of candidate keys. In this paper, we propose an algorithmic approach for database normalization up to third normal form by taking into account all candidate keys, including the primary key. The effectiveness of the proposed approach is evaluated on many real world examples.","n":0.084}}},{"i":1034,"$":{"0":{"v":"Model driven development of data warehouses","n":0.408},"1":{"v":"Long lasting and heavyweight data warehouse development processes may be far more automated using set of standard schema and data transformations based on relevant conceptual models. Model-driven method for development of data warehouses is proposed based on primary concept model and warehouse concept model. The primary concept model treats object types and their properties as individual concepts and enables direct conversion to warehouse schema; it may be easily extracted from normalized relational schema of database. The concept model enables to get warehouse schema in multi-dimensional normal form that ensures avoiding of aggregation anomalies. It suggests extracting of operational data to warehouse in their original fine-grained form thus allowing flexibility of queries. Transformations between primary concept model and warehouse concept model enable automatic or semi-automatic development of warehouse schema. Important issue from this research is in the fact that normalization of operational databases not only secures from update anomalies but also makes it possible to easy using of operational data in warehousing","n":0.079}}},{"i":1035,"$":{"0":{"v":"Performance criteria for relational databases in different normal forms","n":0.333},"1":{"v":"With the increase in client-server and other applications that are dependent on telecommunications possibilities and the use of dispersed and distributed processing, desire for access to organizational databases will increase. The increasing demands on databases make efficient storage space and access time important issues. In this article we illustrate criteria for measuring efficiency of database access as applications expressed in different normal forms. The efficiency measures are (1) actual memory space use, (2) time use, and (3) average number of operations performed. These are tested based on an optimal data search simulation of 25 different database cases for four distinct queries in both third and fourth normal form, and actual space and time use and average number of operations required by 208 subjects who performed the same queries in third and fourth normal form. We find that in almost all studied cases where there are multivalued dependencies, the fourth normal form is more efficient. These techniques, particularly the simulation and the experimental method, may be used as efficiency measures when establishing the ideal form for a new database.","n":0.075}}},{"i":1036,"$":{"0":{"v":"The practical need for fourth normal form","n":0.378},"1":{"v":"Many practitioners and academicians believe that data violating fourth normal form is rarely encountered. We report upon a study of forty organizational databases; nine of them contained data violating fourth normal form. Consequently, the need to understand and user fourth normal form is more important than previously believed.","n":0.144}}},{"i":1037,"$":{"0":{"v":"Microservices tenets","n":0.707},"1":{"v":"Some microservices proponents claim that microservices form a new architectural style; in contrast, advocates of service-oriented architecture (SOA) argue that microservices merely are an implementation approach to SOA. This overview and vision paper first reviews popular introductions to microservices to identify microservices tenets. It then compares two microservices definitions and contrasts them with SOA principles and patterns. This analysis confirms that microservices indeed can be seen as a development- and deployment-level variant of SOA; such microservices implementations have the potential to overcome the deficiencies of earlier approaches to SOA realizations by employing modern software engineering paradigms and Web technologies such as domain-driven design, RESTful HTTP, IDEAL cloud application architectures, polyglot persistence, lightweight containers, a continuous DevOps approach to service delivery, and comprehensive but lean fault management. However, these paradigms and technologies also cause a number of additional design choices to be made and create new options for many \"distribution classics\" type of architectural decisions. As a result, the cognitive load for (micro-)services architects increases, as well as the design, testing and maintenance efforts that are required to benefit from an adoption of microservices. To initiate and frame the buildup of architectural knowledge supporting microservices projects, this paper compiles related practitioner questions; it also derives research topics from these questions. The paper concludes with a summarizing position statement: microservices constitute one particular implementation approach to SOA (service development and deployment).","n":0.066}}},{"i":1038,"$":{"0":{"v":"Object-Oriented Design","n":0.707},"1":{"v":"Introduction. 1. Improving Design. 2. Developing the Multi-Layer, Multi-Component Model. 3. Designing the Problem Domain Component. 4. Designing the Human Interaction Component. 5. Designing the Data Management Component. 6. Applying OOD with OOPLs (or less than an OOPL). 7. Applying OOD Criteria. 8. OOD and CASE. 9. Getting Started with OOD. Appendix A. OOA Strategy Summary Appendix B. OOD Strategy Summary. Appendix C. An Example: OOA to OOD to OOP. Bibliography.","n":0.119}}},{"i":1039,"$":{"0":{"v":"Measuring object-oriented design principles: The results of focus group-based research","n":0.316},"1":{"v":"Abstract   Object-oriented design principles are fundamental concepts that carry important design knowledge and foster the development of software-intensive systems with a focus on good design quality. They emerged after the first steps in the field of object-oriented programming and the recognition of best practices in using this programming paradigm to build maintainable software. Although design principles are known by software developers, it is difficult to apply them in practice without concrete rules to follow. We recognized this gap and systematically derived design best practices for a number of design principles and provide tool support for automatic measurement of these practices. The aim of this paper is to examine the relationship between design best practices and 10 selected design principles. This should provide evidence whether the key design aspects of the design principles are covered. We conducted focus group research with six focus groups and 31 participants in total. In parallel, each group discussed five design principles and assessed the coverage by using the Delphi method. Despite suggestions of additional design practices that were added by the participants, the result reveals the impact of each design best practice to the design principle and shows that the main design aspects of the design principles are covered by our approach and is therefore feasible to derive concrete design improvement actions.","n":0.068}}},{"i":1040,"$":{"0":{"v":"Metrics Functions for Kanban Guards","n":0.447},"1":{"v":"Agile and lean approaches favor self-organizing teams that use low-tech solutions for communicating and negotiating project content and scope in software projects. We consider this approach to have many benefits, but we also recognize that there is information in software projects that does not readily lend itself to low-tech types of visualization. Different characteristics of the code base is one such example. In this paper, we outline metrics functions that can take advantage of more advanced information of the development artifacts and provide the lean or agile team with partially automated decision support for quality assurance actions.","n":0.102}}},{"i":1041,"$":{"0":{"v":"The Consortium for IT Software Quality (CISQ)","n":0.378},"1":{"v":"Standards and guidelines are the backbone to support efficient and effective projects in software engineering with respect to quality aspects. The Consortium for IT Software Quality (CISQ) represents a consortium of end-user and vendor organizations working together under the Object Management Group (OMG) process to produce standards for measuring and reporting software quality independently of the process used to produce that software. This paper introduces to CISQ and presents the main contributions to software quality for end-users and vendors.","n":0.113}}},{"i":1042,"$":{"0":{"v":"The Illusory Diffusion of Innovation: An Examination of Assimilation Gaps","n":0.316},"1":{"v":"Innovation researchers have known for sometime that a new information technology maybe widely acquired, but then only sparsely deployed among acquiring firms. When this happens, the observed pattern of cumulative adoptions will vary depending on which eventin the assimilation process (i.e., acquisition or deployment) is treated as the adoption event. Instead of mirroring one another, a widening gap-termed here an assimilation gap-will existbetween the cumulative adoption curves associated with the alternatively conceived adoption events. When a pronounced assimilation gap exists, the common practice of using cumulative purchases or acquisitions as the basis for diffusion modeling can present an illusory picture of the diffusion process-leading to potentially erroneous judgments about the robustness ofthe diffusion process already observed, and of the technology's future prospects. Researchers may draw inappropriate theoretical inferences about the forces driving diffusion. Practitioners may commit to a technology based on a belief that pervasive adoption is inevitable, when it is not. This study introduces the assimilation gap concept, and develops a general operational measure derived from the difference between the cumulative acquisition and deployment patterns. It describes how two characteristics-increasing returns to adoption and knowledge barriers impeding adoption-separately and in combination may serve to predispose a technology to exhibit a pronounced gap. It develops techniques for measuring assimilation gaps, for establishing whether two gaps are significantly different from each other, and for establishing whether a particular gap is absolutely large enough to be of substantive interest. Finally, it demonstrates these techniques in an analysis of adoption data for three prominent innovations in software process technology-relational database management systems (RDBs), general purpose fourth generation languages (4GLs), and computer aided software engineering tools (CASE). The analysis confirmed that assimilation gaps can be sensibly measured, and that their measured size is largely consistent with a priori expectations and recent research results. A very pronounced gap was found for CASE, while more moderate-though still significant-gaps were found for RDBs and 4GLs. These results have the immediate implication that, where the possibility of a substantial assimilation gap exists, the time of deployment should be captured instead of, or in addition to, time of acquisition as the basis for diffusion modeling. More generally, the results suggest that observers be guarded about concluding, based on sales data, that an innovation is destined to become widely used. In addition, by providing the ability to analyze and compare assimilation gaps, this study provides an analytic foundation for future research on why assimilation gaps occur, and what might be done to reduce them.","n":0.049}}},{"i":1043,"$":{"0":{"v":"Diffusion of New Products: Empirical Generalizations and Managerial Uses","n":0.333},"1":{"v":"The diffusion model developed by Bass Bass, F. M. 1969. A new product growth model for consumer durables. Management Sci.15January 215--227. constitutes an empirical generalization. It represents a pattern or regularity that has been shown to repeat over many new products and services in many countries and over a variety circumstances. Numerous and various applications of the model have lead to further generalizations. Modifications and extensions of the model have lead to further generalizations. In addition to the empirical generalizations that stem from the model, we discuss here some of the managerial applications of the model.","n":0.102}}},{"i":1044,"$":{"0":{"v":"An extraction method to collect data on defects and effort evolution in a constantly modified system","n":0.25},"1":{"v":"This paper describes a data extraction method that was carried out on a set of historical development documentation, related to a commercial software system for mobile platform. This method is part of a major project, which aims to identify evidences of technical debt via the analysis of the defect evolution and effort estimation deviation, verifying if there are relations between these concepts and project decisions during the cycles of development. We intend that a future analysis of such data supports the identification of patterns regarding specific decisions and variations of the defect number/frequency and effort deviation. Thus, such patterns could assist project managers during future project decisions, mainly regarding the maintenance and evolution stages.","n":0.094}}},{"i":1045,"$":{"0":{"v":"Finding Technical Debt in Platform and Network Architectures","n":0.354}}},{"i":1046,"$":{"0":{"v":"Design Patterns: Elements of Reusable Object-Oriented Software","n":0.378},"1":{"v":"The book is an introduction to the idea of design patterns in software engineering, and a catalog of twenty-three common patterns. The nice thing is, most experienced OOP designers will find out they've known about patterns all along. It's just that they've never considered them as such, or tried to centralize the idea behind a given pattern so that it will be easily reusable.","n":0.125}}},{"i":1047,"$":{"0":{"v":"Testing Consequences of Grime Buildup in Object Oriented Design Patterns","n":0.316},"1":{"v":"Evidence suggests that as software ages the original realizations of design patterns remain in place, and participants in design pattern realizations accumulate \"grime\" - non-pattern-related code. This research examines the consequences that grime buildup has on the testability of general purpose design patterns. Test cases put in place during the design phase and initial implementation of a project can become ineffective as the system matures. The evolution of a design due to added functionality or defect fixing increases the coupling and dependencies between many classes that must be tested. We show that as systems age, the growth of grime and the appearance of anti-patterns increase testing requirements. Early recognition and removal of grime and anti-patterns can potentially improve system testability.","n":0.091}}},{"i":1048,"$":{"0":{"v":"A Multiple Case Study of Continuous Architecting in Large Agile Companies: Current Gaps and the CAFFEA Framework","n":0.243},"1":{"v":"In order to continuously support the value delivery both in short-term and long-term, a key goal for large software companies is to continuously develop and manage software architecture. In order to understand how architecture management is employed in large Agile software companies, we have conducted interviews involving several roles at 5 firms. Through a combination of structured inductive and deductive analysis proper of Grounded Theory, we have identified current architect roles and gaps in the architecture practices in the studied organizations. From such investigation, we have developed an organizational framework, CAFFEA, for Agile architecting, including roles, (virtual) teams and practices. The framework has been evaluated through a cross-company workshop including participants from 5 large software companies, discussion groups and a final survey. Finally, we have evaluated the framework in practice after one year of its application at one of the companies. We found that some necessary architectural practices are overlooked in Large Agile Software Development. The evaluation of CAFFEA framework showed that the included roles and teams are needed in order to mitigate the gaps in the architectural practices. The responsibilities and the activities have been mapped to key architect roles compatible with the Scrum setting employed at the companies. The evaluation of CAFFEA shows key benefits.","n":0.07}}},{"i":1049,"$":{"0":{"v":"Qualitative evaluation and research methods","n":0.447},"1":{"v":"PART ONE: CONCEPTUAL ISSUES IN THE USE OF QUALITATIVE METHODS The Nature of Qualitative Inquiry Strategic Themes in Qualitative Methods Variety in Qualitative Inquiry Theoretical Orientations Particularly Appropriate Qualitative Applications PART TWO: QUALITATIVE DESIGNS AND DATA COLLECTION Designing Qualitative Studies Fieldwork Strategies and Observation Methods Qualitative Interviewing PART THREE: ANALYSIS, INTERPRETATION, AND REPORTING Qualitative Analysis and Interpretation Enhancing the Quality and Credibility of Qualitative Analysis","n":0.124}}},{"i":1050,"$":{"0":{"v":"Diversity in software engineering research","n":0.447},"1":{"v":"One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space).","n":0.07}}},{"i":1051,"$":{"0":{"v":"Feature subset selection and feature ranking for multivariate time series","n":0.316},"1":{"v":"Feature subset selection (FSS) is a known technique to preprocess the data before performing any data mining tasks, e.g., classification and clustering. FSS provides both cost-effective predictors and a better understanding of the underlying process that generated the data. We propose a family of novel unsupervised methods for feature subset selection from multivariate time series (MTS) based on common principal component analysis, termed CLeVer. Traditional FSS techniques, such as recursive feature elimination (RFE) and Fisher criterion (FC), have been applied to MTS data sets, e.g., brain computer interface (BCI) data sets. However, these techniques may lose the correlation information among features, while our proposed techniques utilize the properties of the principal component analysis to retain that information. In order to evaluate the effectiveness of our selected subset of features, we employ classification as the target data mining task. Our exhaustive experiments show that CLeVer outperforms RFE, FC, and random selection by up to a factor of two in terms of the classification accuracy, while taking up to 2 orders of magnitude less processing time than RFE and FC.","n":0.075}}},{"i":1052,"$":{"0":{"v":"An empirical study of the bad smells and class error probability in the post-release object-oriented system evolution","n":0.243},"1":{"v":"Bad smells are used as a means to identify problematic classes in object-oriented systems for refactoring. The belief that the bad smells are linked with problematic classes is largely based on previous metric research results. Although there is a plethora of empirical studies linking software metrics to errors and error proneness of classes in object-oriented systems, the link between the bad smells and class error probability in the evolution of object-oriented systems after the systems are released has not been explored. There has been no empirical evidence linking the bad smells with class error probability so far. This paper presents the results from an empirical study that investigated the relationship between the bad smells and class error probability in three error-severity levels in an industrial-strength open source system. Our research, which was conducted in the context of the post-release system evolution process, showed that some bad smells were positively associated with the class error probability in the three error-severity levels. This finding supports the use of bad smells as a systematic method to identify and refactor problematic classes in this specific context.","n":0.074}}},{"i":1053,"$":{"0":{"v":"Context is king: The developer perspective on the usage of static analysis tools","n":0.277},"1":{"v":"Automatic static analysis tools (ASATs) are tools that support automatic code quality evaluation of software systems with the aim of (i) avoiding and/or removing bugs and (ii) spotting design issues. Hindering their wide-spread acceptance are their (i) high false positive rates and (ii) low comprehensibility of the generated warnings. Researchers and ASATs vendors have proposed solutions to prioritize such warnings with the aim of guiding developers toward the most severe ones. However, none of the proposed solutions considers the development context in which an ASAT is being used to further improve the selection of relevant warnings. To shed light on the impact of such contexts on the warnings configuration, usage and adopted prioritization strategies, we surveyed 42 developers (69% in industry and 31% in open source projects) and interviewed 11 industrial experts that integrate ASATs in their workflow. While we can confirm previous findings on the reluctance of developers to configure ASATs, our study highlights that (i) 71% of developers do pay attention to different warning categories depending on the development context, and (ii) 63% of our respondents rely on specific factors (e.g., team policies and composition) when prioritizing warnings to fix during their programming. Our results clearly indicate ways to better assist developers by improving existing warning selection and prioritization strategies.","n":0.069}}},{"i":1054,"$":{"0":{"v":"Detecting code smells using machine learning techniques: Are we there yet?","n":0.302},"1":{"v":"Code smells are symptoms of poor design and implementation choices weighing heavily on the quality of produced source code. During the last decades several code smell detection tools have been proposed. However, the literature shows that the results of these tools can be subjective and are intrinsically tied to the nature and approach of the detection. In a recent work the use of Machine-Learning (ML) techniques for code smell detection has been proposed, possibly solving the issue of tool subjectivity giving to a learner the ability to discern between smelly and non-smelly source code elements. While this work opened a new perspective for code smell detection, it only considered the case where instances affected by a single type smell are contained in each dataset used to train and test the machine learners. In this work we replicate the study with a different dataset configuration containing instances of more than one type of smell. The results reveal that with this configuration the machine learning techniques reveal critical limitations in the state of the art which deserve further research.","n":0.075}}},{"i":1055,"$":{"0":{"v":"How do developers fix issues and pay back technical debt in the Apache ecosystem","n":0.267},"1":{"v":"During software evolution technical debt (TD) follows a constant ebb and flow, being incurred and paid back, sometimes in the same day and sometimes ten years later. There have been several studies in the literature investigating how technical debt in source code accumulates during time and the consequences of this accumulation for software maintenance. However, to the best of our knowledge there are no large scale studies that focus on the types of issues that are fixed and the amount of TD that is paid back during software evolution. In this paper we present the results of a case study, in which we analyzed the evolution of fifty-seven Java open-source software projects by the Apache Software Foundation at the temporal granularity level of weekly snapshots. In particular, we focus on the amount of technical debt that is paid back and the types of issues that are fixed. The findings reveal that a small subset of all issue types is responsible for the largest percentage of TD repayment and thus, targeting particular violations the development team can achieve higher benefits.","n":0.075}}},{"i":1056,"$":{"0":{"v":"How developers perceive smells in source code: A replicated study","n":0.316},"1":{"v":"Abstract   Context. In recent years, smells, also referred to as bad smells, have gained popularity among developers. However, it is still not clear how harmful they are perceived from the developers’ point of view. Many developers talk about them, but only few know what they really are, and even fewer really take care of them in their source code.  Objective. The goal of this work is to understand the perceived criticality of code smells both in theory, when reading their description, and in practice.  Method. We executed an empirical study as a differentiated external replication of two previous studies. The studies were conducted as surveys involving only highly experienced developers (63 in the first study and 41 in the second one). First the perceived criticality was analyzed by proposing the description of the smells, then different pieces of code infected by the smells were proposed, and finally their ability to identify the smells in the analyzed code was tested.  Results. According to our knowledge, this is the largest study so far investigating the perception of code smells with professional software developers. The results show that developers are very concerned about code smells in theory, nearly always considering them as harmful or very harmful (17 out of 23 smells). However, when they were asked to analyze an infected piece of code, only few infected classes were considered harmful and even fewer were considered harmful because of the smell.  Conclusions. The results confirm our initial hypotheses that code smells are perceived as more critical in theory but not as critical in practice.","n":0.062}}},{"i":1057,"$":{"0":{"v":"Analyzing forty years of software maintenance models","n":0.378},"1":{"v":"Software maintenance has dramatically evolved in the last four decades, to cope with the continuously changing software development models, and programming languages and adopting increasingly advanced prediction models. In this work, we present the initial results of a Systematic Literature Review (SLR), highlighting the evolution of the metrics and models adopted in the last forty years.","n":0.134}}},{"i":1058,"$":{"0":{"v":"A Survey on Code Analysis Tools for Software Maintenance Prediction","n":0.316},"1":{"v":"Software maintenance is a widely studied area of software engineering that it is particularly important in safety-critical and mission-critical applications where defects may have huge impact and code needs to be checked carefully through the analysis of data collected using a number of tools developed to investigate specific aspects. However, such tools are often not available to practitioners preventing them from applying the most recent and advanced approaches to industrial projects. This paper is an initial investigation about code analysis tools used to perform research studies on software maintenance prediction. We focus on the identification of tools that are available and can be used by practitioners to apply the same maintenance approaches described in published academic papers.","n":0.092}}},{"i":1059,"$":{"0":{"v":"The Technical Debt Dataset","n":0.5},"1":{"v":"Technical Debt analysis is increasing in popularity as nowadays researchers and industry are adopting various tools for static code analysis to evaluate the quality of their code. Despite this, empirical studies on software projects are expensive because of the time needed to analyze the projects. In addition, the results are difficult to compare as studies commonly consider different projects. In this work, we propose the Technical Debt Dataset, a curated set of project measurement data from 33 Java projects from the Apache Software Foundation. In the Technical Debt Dataset, we analyzed all commits from separately defined time frames with SonarQube to collect Technical Debt information and with Ptidej to detect code smells. Moreover, we extracted all available commit information from the git logs, the refactoring applied with Refactoring Miner, and fault information reported in the issue trackers (Jira). Using this information, we executed the SZZ algorithm to identify the fault-inducing and -fixing commits. We analyzed 78K commits from the selected 33 projects, detecting 1.8M SonarQube issues, 62K code smells, 28K faults and 57K refactorings. The project analysis took more than 200 days. In this paper, we describe the data retrieval pipeline together with the tools used for the analysis. The dataset is made available through CSV files and an SQLite database to facilitate queries on the data. The Technical Debt Dataset aims to open up diverse opportunities for Technical Debt research, enabling researchers to compare results on common projects.","n":0.065}}},{"i":1060,"$":{"0":{"v":"Embracing Technical Debt, from a Startup Company Perspective","n":0.354},"1":{"v":"Software startups are typically under extreme pressure to get to market quickly with limited resources and high uncertainty. This pressure and uncertainty is likely to cause startups to accumulate technical debt as they make decisions that are more focused on the short-term than the long-term health of the codebase. However, most research on technical debt has been focused on more mature software teams, who may have less pressure and, therefore, reason about technical debt very differently than software startups. In this study, we seek to understand the organizational factors that lead to and the benefits and challenges associated with the intentional accumulation of technical debt in software startups. We interviewed 16 professionals involved in seven different software startups. We find that the startup phase, the experience of the developers, software knowledge of the founders, and level of employee growth are some of the organizational factors that influence the intentional accumulation of technical debt. In addition, we find the software startups are typically driven to achieve a \"good enough level,\" and this guides the amount of technical debt that they intentionally accumulate to balance the benefits of speed to market and reduced resources with the challenges of later addressing technical debt.","n":0.071}}},{"i":1061,"$":{"0":{"v":"What if i had no smells","n":0.408},"1":{"v":"What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.","n":0.085}}},{"i":1062,"$":{"0":{"v":"On the diffuseness of code technical debt in Java projects of the apache ecosystem","n":0.267},"1":{"v":"Background. Companies commonly invest majorBackground. Companies commonly invest major effort into removing, respectively not introducing, technical debt issues detected by static analysis tools such as SonarQube, Cast, or Coverity. These tools classify technical debt issues into categories according to severity, and developers commonly pay attention to not introducing issues with a high level of severity that could generate bugs or make software maintenance more difficult. Objective. In this work, we aim to understand the diffuseness of Technical Debt (TD) issues and the speed with which developers remove them from the code if they introduced such an issue. The goal is to understand which type of TD is more diffused and how much attention is paid by the developers, as well as to investigate whether TD issues with a higher level of severity are resolved faster than those with a lower level of severity. We conducted a case study across 78K commits of 33 Java projects from the Apache Software Foundation Ecosystem to investigate the distribution of 1.4M TD items. Results. TD items introduced into the code are mostly related to code smells (issues that can increase the maintenance effort). Moreover, developers commonly remove the most severe issues faster than less severe ones. However, the time needed to resolve issues increases when the level of severity increases (minor issues are removed faster that blocker ones). Conclusion. One possible answer to the unexpected issue of resolution time might be that severity is not correctly defined by the tools. Another possible answer is that the rules at an intermediate severity level could be the ones that technically require more time to be removed. The classification of TD items, including their severity and type, require thorough investigation from a research point of view.effort into removing, respectively not introducing, technical debtissues detected by static analysis tools such as SonarQube, Cast, or Coverity. These tools classify technical debt issues intocategories according to severity, and developers commonly payattention to not introducing issues with a high level of severitythat could generate bugs or make software maintenance moredifficult. Objective. In this work, we aim to understand the diffuseness ofTechnical Debt (TD) issues and the speed with which developersremove them from the code if they introduced such an issue. The goal is to understand which type of TD is more diffusedand how much attention is paid by the developers, as well asto investigate whether TD issues with a higher level of severityare resolved faster than those with a lower level of severity. Weconducted a case study across 78K commits of 33 Java projectsfrom the Apache Software Foundation Ecosystem to investigatethe distribution of 1.4M TD items. Results. TD items introduced into the code are mostly relatedto code smells (issues that can increase the maintenance effort). Moreover, developers commonly remove the most severe issuesfaster than less severe ones. However, the time needed to resolveissues increases when the level of severity increases (minor issuesare removed faster that blocker ones). Conclusion. One possible answer to the unexpected issue ofresolution time might be that severity is not correctly definedby the tools. Another possible answer is that the rules at anintermediate severity level could be the ones that technicallyrequire more time to be removed. The classification of TD items, including their severity and type, require thorough investigationfrom a research point of view.","n":0.043}}},{"i":1063,"$":{"0":{"v":"Change Prediction through Coding Rules Violations","n":0.408},"1":{"v":"Static source code analysis is an increasingly important activity to manage software project quality, and is often found as a part of the development process. A widely adopted way of checking code quality is through the detection of violations to specific sets of rules addressing good programming practices. SonarQube is a platform able to detect these violations, called Issues. In this paper we described an empirical study performend on two industrial projects, where we used Issues extracted on different versions of the projects to predict changes in code through a set of machine learning models. We achieved good detection performances, especially when predicting changes in the next version. This result paves the way for future investigations of the interest in an industrial setting towards the prioritization of Issues management according to their impact on change-proneness.","n":0.086}}},{"i":1064,"$":{"0":{"v":"On the Fault Proneness of SonarQube Technical Debt Violations: A comparison of eight Machine Learning Techniques.","n":0.25},"1":{"v":"Background. The popularity of tools for analyzing Technical Debt, and particularly that of SonarQube, is increasing rapidly. SonarQube proposes a set of coding rules, which represent something wrong in the code that will soon be reflected in a fault or will increase maintenance effort. However, while the management of some companies is encouraging developers not to violate these rules in the first place and to produce code below a certain technical debt threshold, developers are skeptical of their importance. Objective. In order to understand which SonarQube violations are actually fault-prone and to analyze the accuracy of the fault-prediction model, we designed and conducted an empirical study on 21 well-known mature open-source projects. Method. We applied the SZZ algorithm to label the fault-inducing commits. We compared the classification power of eight Machine Learning models (Logistic Regression, Decision Tree, Random Forest, Extremely Randomized Trees, AdaBoost, Gradient Boosting, XGBoost) to obtain a set of violations that are correlated with fault-inducing commits. Finally, we calculated the percentage of violations introduced in the fault-inducing commit and removed in the fault-fixing commit, so as to reduce the risk of spurious correlations. Result. Among the 202 violations defined for Java by SonarQube, only 26 have a relatively low fault-proneness. Moreover, violations classified as ''bugs'' by SonarQube hardly never become a failure. Consequently, the accuracy of the fault-prediction power proposed by SonarQube is extremely low. Conclusion. The rules applied by SonarQube for calculating technical debt should be thoroughly investigated and their harmfulness needs to be further confirmed. Therefore, companies should carefully consider which rules they really need to apply, especially if their goal is to reduce fault-proneness.","n":0.061}}},{"i":1065,"$":{"0":{"v":"Are SonarQube Rules Inducing Bugs","n":0.447},"1":{"v":"The popularity of tools for analyzing Technical Debt, and particularly the popularity of SonarQube, is increasing rapidly. SonarQube proposes a set of coding rules, which represent something wrong in the code that will soon be reflected in a fault or will increase maintenance effort. However, our local companies were not confident in the usefulness of the rules proposed by SonarQube and contracted us to investigate the fault-proneness of these rules. In this work we aim at understanding which SonarQube rules are actually fault-prone and to understand which machine learning models can be adopted to accurately identify fault-prone rules. We designed and conducted an empirical study on 21 well-known mature open-source projects. We applied the SZZ algorithm to label the fault-inducing commits. We analyzed the fault-proneness by comparing the classification power of seven machine learning models. Among the 202 rules defined for Java by SonarQube, only 25 can be considered to have relatively low fault-proneness. Moreover, violations considered as “bugs” by SonarQube were generally not fault-prone and, consequently, the fault-prediction power of the model proposed by SonarQube is extremely low. The rules applied by SonarQube for calculating technical debt should be thoroughly investigated and their harmfulness needs to be further confirmed. Therefore, companies should carefully consider which rules they really need to apply, especially if their goal is to reduce fault-proneness.","n":0.067}}},{"i":1066,"$":{"0":{"v":"Work practices and challenges in pull-based development: the contributor's perspective","n":0.316},"1":{"v":"In the pull-based development model, the integrator has the crucial role of managing and integrating contributions. This work focuses on the role of the integrator and investigates working habits and challenges alike. We set up an exploratory qualitative study involving a large-scale survey of 749 integrators, to which we add quantitative data from the integrator's project. Our results provide insights into the factors they consider in their decision making process to accept or reject a contribution. Our key findings are that integrators struggle to maintain the quality of their projects and have difficulties with prioritizing contributions that are to be merged. Our insights have implications for practitioners who wish to use or improve their pull-based development process, as well as for researchers striving to understand the theoretical implications of the pull-based model in software development.","n":0.086}}},{"i":1067,"$":{"0":{"v":"An empirical analysis of the docker container ecosystem on GitHub","n":0.316},"1":{"v":"Docker allows packaging an application with its dependencies into a standardized, self-contained unit (a so-called container), which can be used for software development and to run the application on any system. Dockerfiles are declarative definitions of an environment that aim to enable reproducible builds of the container. They can often be found in source code repositories and enable the hosted software to come to life in its execution environment. We conduct an exploratory empirical study with the goal of characterizing the Docker ecosystem, prevalent quality issues, and the evolution of Dockerfiles. We base our study on a data set of over 70000 Dockerfiles, and contrast this general population with samplings that contain the Top-100 and Top-1000 most popular Docker-using projects. We find that most quality issues (28.6%) arise from missing version pinning (i.e., specifying a concrete version for dependencies). Further, we were not able to build 34% of Dockerfiles from a representative sample of 560 projects. Integrating quality checks, e.g., to issue version pinning warnings, into the container build process could result into more reproducible builds. The most popular projects change more often than the rest of the Docker population, with 5.81 revisions per year and 5 lines of code changed on average. Most changes deal with dependencies, that are currently stored in a rather unstructured manner. We propose to introduce an abstraction that, for instance, could deal with the intricacies of different package managers and could improve migration to more light-weight images.","n":0.064}}},{"i":1068,"$":{"0":{"v":"Modern Release Engineering in a Nutshell -- Why Researchers Should Care","n":0.302},"1":{"v":"The release engineering process is the process that brings high quality code changes from a developer's workspace to the end user, encompassing code change integration, continuous integration, build system specifications, infrastructure-as-code, deployment and release. Recent practices of continuous delivery, which bring new content to the end user in days or hours rather than months or years, have generated a surge of industry-driven interest in the release engineering pipeline. This paper argues that the involvement of researchers is essential, by providing a brief introduction to the six major phases of the release engineering pipeline, a roadmap of future research, and a checklist of three major ways that the release engineering process of a system under study can invalidate the findings of software engineering studies. The main take-home message is that, while release engineering technology has flourished tremendously due to industry, empirical validation of best practices and the impact of the release engineering process on (amongst others) software quality is largely missing and provides major research opportunities.","n":0.078}}},{"i":1069,"$":{"0":{"v":"Does your configuration code smell","n":0.447},"1":{"v":"Infrastructure as Code (IaC) is the practice of specifying computing system configurations through code, and managing them through traditional software engineering methods. The wide adoption of configuration management and increasing size and complexity of the associated code, prompt for assessing, maintaining, and improving the configuration code's quality. In this context, traditional software engineering knowledge and best practices associated with code quality management can be leveraged to assess and manage configuration code quality. We propose a catalog of 13 implementation and 11 design configuration smells, where each smell violates recommended best practices for configuration code. We analyzed 4,621 Puppet repositories containing 8.9 million lines of code and detected the cataloged implementation and design configuration smells. Our analysis reveals that the design configuration smells show 9% higher average co-occurrence among themselves than the implementation configuration smells. We also observed that configuration smells belonging to a smell category tend to co-occur with configuration smells belonging to another smell category when correlation is computed by volume of identified smells. Finally, design configuration smell density shows negative correlation whereas implementation configuration smell density exhibits no correlation with the size of a configuration management system.","n":0.073}}},{"i":1070,"$":{"0":{"v":"Systems Approaches to Tackling Configuration Errors: A Survey","n":0.354},"1":{"v":"In recent years, configuration errors (i.e., misconfigurations) have become one of the dominant causes of system failures, resulting in many severe service outages and downtime. Unfortunately, it is notoriously difficult for system users (e.g., administrators and operators) to prevent, detect, and troubleshoot configuration errors due to the complexity of the configurations as well as the systems under configuration. As a result, the cost of resolving configuration errors is often tremendous from the aspects of both compensating the service disruptions and diagnosing, recovering from the failures. The prevalence, severity, and cost have made configuration errors one of the most thorny system problems that desire to be addressed.   This survey article provides a holistic and structured overview of the systems approaches that tackle configuration errors. To understand the problem fundamentally, we first discuss the characteristics of configuration errors and the challenges of tackling such errors. Then, we discuss the state-of-the-art systems approaches that address different types of configuration errors in different scenarios. Our primary goal is to equip the stakeholder with a better understanding of configuration errors and the potential solutions for resolving configuration errors in the spectrum of system development and management. To inspire follow-up research, we further discuss the open problems with regard to system configuration. To the best of our knowledge, this is the first survey on the topic of tackling configuration errors.","n":0.067}}},{"i":1071,"$":{"0":{"v":"Co-evolution of infrastructure and source code: an empirical study","n":0.333},"1":{"v":"Infrastructure-as-code automates the process of configuring and setting up the environment (e.g., servers, VMs and databases) in which a software system will be tested and/or deployed, through textual specification files in a language like Puppet or Chef. Since the environment is instantiated automatically by the infrastructure languages' tools, no manual intervention is necessary apart from maintaining the infrastructure specification files. The amount of work involved with such maintenance, as well as the size and complexity of infrastructure specification files, have not yet been studied empirically. Through an empirical study of the version control system of 265 Open Stack projects, we find that infrastructure files are large and churn frequently, which could indicate a potential of introducing bugs. Furthermore, we found that the infrastructure code files are coupled tightly with the other files in a project, especially test files, which implies that testers often need to change infrastructure specifications when making changes to the test framework and tests.","n":0.08}}},{"i":1072,"$":{"0":{"v":"Measuring dependency freshness in software systems","n":0.408},"1":{"v":"Modern software systems often make use of third-party components to speed-up development and reduce maintenance costs. In return, developers need to update to new releases of these dependencies to avoid, for example, security and compatibility risks. In practice, prioritizing these updates is difficult because the use of outdated dependencies is often opaque. In this paper we aim to make this concept more transparent by introducing metrics to quantify the use of recent versions of dependencies, i.e. the system's \"dependency freshness\".   We propose and investigate a system-level metric based on an industry benchmark. We validate the usefulness of the metric using interviews, analyze the variance of the metric through time, and investigate the relationship between outdated dependencies and security vulnerabilities. The results show that the measurements are considered useful, and that systems using outdated dependencies four times as likely to have security issues as opposed to systems that are up-to-date.","n":0.082}}},{"i":1073,"$":{"0":{"v":"Benchmark-Based Aggregation of Metrics to Ratings","n":0.408},"1":{"v":"Software metrics have been proposed as instruments, not only to guide individual developers in their coding tasks, but also to obtain high-level quality indicators for entire software systems. Such system-level indicators are intended to enable meaningful comparisons among systems or to serve as triggers for a deeper analysis.Common methods for aggregation range from simple mathematical operations (e.g. addition and central tendency) to more complex methodologies such as distribution fitting, wealth inequality metrics (e.g. Gini coefficient and Theil Index) and custom formulae.However, these methodologies provide little guidance for interpreting the aggregated results or to trace back to individual measurements.To resolve such limitations, a two-stage rating approach has been proposed where (i) measurement values are compared to thresholds to summarize them into risk profiles, and (ii) risk profiles are mapped to ratings.In this paper, we extend our approach for deriving metric thresholds from benchmark data into a methodology for benchmark-based calibration of two-stage aggregation of metrics into ratings.We explain the core algorithm of the methodology and we demonstrate its application to various metrics of the SIG quality model, using a benchmark of 100 software systems.We present an evaluation of the sensitivity of the algorithm to the underlying data.","n":0.071}}},{"i":1074,"$":{"0":{"v":"Observer reliability of arteriovenous malformations grading scales using current imaging modalities.","n":0.302},"1":{"v":"Object The aim of this study was to examine observer reliability of frequently used arteriovenous malformation (AVM) grading scales, including the 5-tier Spetzler-Martin scale, the 3-tier Spetzler-Ponce scale, and the Pollock-Flickinger radiosurgery-based scale, using current imaging modalities in a setting closely resembling routine clinical practice. Methods Five experienced raters, including 1 vascular neurosurgeon, 2 neuroradiologists, and 2 senior neurosurgical residents independently reviewed 15 MRI studies, 15 CT angiograms, and 15 digital subtraction angiograms obtained at the time of initial diagnosis. Assessments of 5 scans of each imaging modality were repeated for measurement of intrarater reliability. Three months after the initial assessment, raters reassessed those scans where there was disagreement. In this second assessment, raters were asked to justify their rating with comments and illustrations. Generalized kappa (κ) analysis for multiple raters, Kendall's coefficient of concordance (W...","n":0.086}}},{"i":1075,"$":{"0":{"v":"Defining and measuring Puppet code quality","n":0.408}}},{"i":1076,"$":{"0":{"v":"Investigating Community, Reliability and Usability of CFEngine, Chef and Puppet","n":0.316},"1":{"v":"An investigative study on community, reliability and usability of CFEngine, Chef and Puppet is represented in this paper. This research study attempts to quantify software qualities like community, reliability and usability of these products and analyses the result to figure out if any product stands out in any of these qualities. Comprehending software characteristics like community , usability and reliability is complex operation often making it challenging to make a quantifiable measurement on them. Research is made in this paper to explore and make these qualities measurable and quantifiable through the application of different statistical and mathematical model. Product popularity trend, resources available for product, community structure as well as it’s field support is studied utilizing different sources like Google, Hackers news and users mailing list. Reliability growth in latest three version of these product is examined by application of Weibull distribution on data obtained from individual bug repository. Finally the usability test is conducted to cover both subjective and objective aspect of user experience on these product to measure each product’s usability and study the difference in usability offered by each. This research hopes to pave the way for future research into this area and help people to comprehend community ,reliability and usability of these products.","n":0.07}}},{"i":1077,"$":{"0":{"v":"Using thematic analysis in psychology","n":0.447},"1":{"v":"Thematic analysis is a poorly demarcated, rarely acknowledged, yet widely used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.","n":0.09}}},{"i":1078,"$":{"0":{"v":"Empirical studies of agile software development: A systematic review","n":0.333},"1":{"v":"Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations.","n":0.084}}},{"i":1079,"$":{"0":{"v":"Guidelines for snowballing in systematic literature studies and a replication in software engineering","n":0.277},"1":{"v":"Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.   Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.   Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.   Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.   Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.","n":0.093}}},{"i":1080,"$":{"0":{"v":"A systematic review of systematic review process research in software engineering","n":0.302},"1":{"v":"Context: Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research. Objective: To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process. Method: We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools. Results: We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult. Conclusion: We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem.","n":0.059}}},{"i":1081,"$":{"0":{"v":"A tertiary study on technical debt: Types, management strategies, research trends, and base information for practitioners","n":0.25},"1":{"v":"Abstract   Context  The concept of technical debt (TD) contextualizes problems faced during software evolution considering the tasks that are not carried out adequately during its development. Currently, it is common to associate any impediment related to the software product and its development process to the definition of TD. This can bring confusion and ambiguity in the use of the term. Besides, due to the increasing amount of work in the area, it is difficult to have a comprehensive view of the plethora of proposals on TD management.    Objective  This paper intends to investigate the current state of research on TD by identifying what research topics have been considered, organizing research directions and practical knowledge that has already been defined, identifying the known types of TD, and organizing what activities, strategies and tools have been proposed to support the management of TD.    Method  A tertiary study was performed based on a set of five research questions. In total, 13 secondary studies, dated from 2012 to March 2018, were evaluated.    Results  The results of this tertiary study are beneficial for both practitioners and researchers. We evolved a taxonomy of TD types, identified a list of situations in which debt items can be found in software projects, and organized a map representing the state of the art of activities, strategies and tools to support TD management. Besides, we also summarized some research directions and practical knowledge, and identified the research topics that have been more considered in secondary studies.    Conclusion  This tertiary study revisited the TD landscape. Its results can help to identify points that still require further investigation in TD research.","n":0.061}}},{"i":1082,"$":{"0":{"v":"Analyzing the concept of technical debt in the context of agile software development: A systematic literature review","n":0.243},"1":{"v":"Abstract   Context  Technical debt (TD) is a metaphor that is used to communicate the consequences of poor software development practices to non-technical stakeholders. In recent years, it has gained significant attention in agile software development (ASD).    Objective  The purpose of this study is to analyze and synthesize the state of the art of TD, and its causes, consequences, and management strategies in the context of ASD.    Research Method  Using a systematic literature review (SLR), 38 primary studies, out of 346 studies, were identified and analyzed.    Results  We found five research areas of interest related to the literature of TD in ASD. Among those areas, “managing TD in ASD” received the highest attention, followed by “architecture in ASD and its relationship with TD”. In addition, eight categories regarding the causes and five categories regarding the consequences of incurring TD in ASD were identified. “Focus on quick delivery” and “architectural and design issues” were the most popular causes of incurring TD in ASD. “Reduced productivity”, “system degradation” and “increased maintenance cost” were identified as significant consequences of incurring TD in ASD. Additionally, we found 12 strategies for managing TD in the context of ASD, out of which “refactoring” and “enhancing the visibility of TD” were the most significant.    Conclusion  The results of this study provide a structured synthesis of TD and its management in the context of ASD as well as potential research areas for further investigation.","n":0.065}}},{"i":1083,"$":{"0":{"v":"A systematic literature review of technical debt prioritization","n":0.354}}},{"i":1084,"$":{"0":{"v":"Technical debt triage in backlog management","n":0.408},"1":{"v":"Remediation of technical debt through regular refactoring initiatives is considered vital for the software system's long and healthy life. However, since today's software companies face increasing pressure to deliver customer value continuously, the balance between spending developer time, effort, and resources on implementing new features or spending it on refactoring of technical debt becomes vital. The goal of this study is to explore how the prioritization of technical debt is carried out by practitioners within today's software industry. This study also investigates what factors influence the prioritization process and its related challenges. This paper reports the results of surveying 17 software practitioners, together with follow-up interviews with them. Our results show that there is no uniform way of prioritizing technical debt and that it is commonly done reactively without applying any explicit strategies. Often, technical debt issues are managed and prioritized in a shadow backlog, separate from the official sprint backlog. This study was also able to identify several different challenges related to prioritizing technical debt, such as the lack of quantitative information about the technical debt items and that the refactoring of technical debt issues competes with the implementation of customer requirements.","n":0.072}}},{"i":1085,"$":{"0":{"v":"Technical debt cripples software developer productivity: a longitudinal study on developers' daily software development work","n":0.258},"1":{"v":"Software companies need to continuously deliver customer value, both from a short- and long-term perspective. However, software development can be impeded by what has been described as Technical Debt (TD). The aim of this study is to explore the negative consequences of TD in terms of wasted software development time. This study also investigates on which additional activities this wasted time is spent and whether different types of TD impact the wasted time differently. This study also sets out to examine the benefits of tracking and communicating the amount of wasted time, both from a developer's and manager's perspective. This paper reports the results of a longitudinal study, surveying 43 software developers, together with follow-up interviews with 16 industrial software practitioners. The analysis of the reported wasted time revealed that developers waste, on average, 23% of their development time due to TD and that they are frequently forced to introduce new TD due to already existing TD. The most common activity on which additional time is spent is performing additional testing.","n":0.076}}},{"i":1086,"$":{"0":{"v":"An Overview and Comparison of Technical Debt Measurement Tools","n":0.333},"1":{"v":"There are numerous commercial tools and research prototypes that offer support for measuring  technical debt. However, different tools adopt different terms, metrics, and ways to identify and  measure technical debt. These tools offer diverse features, and their popularity / community  support varies significantly. Therefore, (a) practitioners face difficulties when trying to select a tool  matching their needs; and (b) the concept of technical debt and its role in software development  is blurred. We attempt to clarify the situation by comparing the features and popularity of technical  debt measurement tools, and analyzing the existing empirical evidence on their validity. Our  findings can help practitioners to find the most suitable tool for their purposes, and researchers  by highlighting the current tool shortcomings.","n":0.091}}},{"i":1087,"$":{"0":{"v":"Automated Measurement of Technical Debt: A Systematic Literature Review","n":0.333},"1":{"v":"Background: Technical Debt (TD) is a quite complex concept that includes several aspect of software development. Often, people talk about TD as the amount of postponed work but this is just a basic approximation of the concept that includes many aspects that are technical and managerial. If TD is managed properly, it can provide a huge advantage but it can also make projects unmaintainable, if not. Therefore, being able of measuring TD is a very important aspect for a proper management of the development process. However, due to the complexity of the concept and the different aspects that are involved, such measurement it not easy and there are several different approaches in literature. Goals: This work aims at investigating the existing approaches to the measurement and the analysis of TD focusing on quantitative methods that could also be automated. Method: The Systematic Literature Review (SLR) approach was applied to 331 studies obtained from the three largest digital libraries and databases. Results: After applying all filtering stages, 21 papers out of 331 were selected and deeply analyzed. The majority of them suggested new approaches to measure TD using different criteria not built on top of existing ones. Conclusions: Existing studies related to the measurement of TD were observed and analyzed. The findings have shown that the field is not mature and there are several models that have almost no independent validation. Moreover few tools for helping to automate the evaluation process exist.","n":0.064}}},{"i":1088,"$":{"0":{"v":"The measurement of observer agreement for categorical data","n":0.354},"1":{"v":"This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.","n":0.105}}},{"i":1089,"$":{"0":{"v":"Statistical Power Analysis for the Behavioral Sciences","n":0.378},"1":{"v":"Contents: Prefaces. The Concepts of Power Analysis. The t-Test for Means. The Significance of a Product Moment rs (subscript s). Differences Between Correlation Coefficients. The Test That a Proportion is .50 and the Sign Test. Differences Between Proportions. Chi-Square Tests for Goodness of Fit and Contingency Tables. The Analysis of Variance and Covariance. Multiple Regression and Correlation Analysis. Set Correlation and Multivariate Methods. Some Issues in Power Analysis. Computational Procedures.","n":0.12}}},{"i":1090,"$":{"0":{"v":"Finding bugs is easy","n":0.5},"1":{"v":"Many techniques have been developed over the years to automatically find bugs in software. Often, these techniques rely on formal methods and sophisticated program analysis. While these techniques are valuable, they can be difficult to apply, and they aren't always effective in finding real bugs.   Bug patterns are code idioms that are often errors. We have implemented automatic detectors for a variety of bug patterns found in Java programs. In this extended abstract1, we describe how we have used bug pattern detectors to find serious bugs in several widely used Java applications and libraries. We have found that the effort required to implement a bug pattern detector tends to be low, and that even extremely simple detectors find bugs in real applications.   From our experience applying bug pattern detectors to real programs, we have drawn several interesting conclusions. First, we have found that even well tested code written by experts contains a surprising number of obvious bugs. Second, Java (and similar languages) have many language features and APIs which are prone to misuse. Finally, that simple automatic techniques can be effective at countering the impact of both ordinary mistakes and misunderstood language features.","n":0.072}}},{"i":1091,"$":{"0":{"v":"A Methodology for Collecting Valid Software Engineering Data","n":0.354},"1":{"v":"An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.","n":0.073}}},{"i":1092,"$":{"0":{"v":"Predicting software defects in varying development lifecycles using Bayesian nets","n":0.316},"1":{"v":"An important decision in software projects is when to stop testing. Decision support tools for this have been built using causal models represented by Bayesian Networks (BNs), incorporating empirical data and expert judgement. Previously, this required a custom BN for each development lifecycle. We describe a more general approach that allows causal models to be applied to any lifecycle. The approach evolved through collaborative projects and captures significant commercial input. For projects within the range of the models, defect predictions are very accurate. This approach enables decision-makers to reason in a way that is not possible with regression-based models.","n":0.101}}},{"i":1093,"$":{"0":{"v":"Static analysis tools as early indicators of pre-release defect density","n":0.316},"1":{"v":"During software development it is helpful to obtain early estimates of the defect density of software components. Such estimates identify fault-prone areas of code requiring further testing. We present an empirical approach for the early prediction of pre-release defect density based on the defects found using static analysis tools. The defects identified by two different static analysis tools are used to fit and predict the actual pre-release defect density for Windows Server 2003. We show that there exists a strong positive correlation between the static analysis defect density and the pre-release defect density determined by testing. Further, the predicted pre-release defect density and the actual pre-release defect density are strongly correlated at a high degree of statistical significance. Discriminant analysis shows that the results of static analysis tools can be used to separate high and low quality components with an overall classification rate of 82.91%.","n":0.083}}},{"i":1094,"$":{"0":{"v":"Which warnings should I fix first","n":0.408},"1":{"v":"Automatic bug-finding tools have a high false positive rate: most warnings do not indicate real bugs. Usually bug-finding tools assign important warnings high priority. However, the prioritization of tools tends to be ineffective. We observed the warnings output by three bug-finding tools, FindBugs, JLint, and PMD, for three subject programs, Columba, Lucene, and Scarab. Only 6%, 9%, and 9% of warnings are removed by bug fix changes during 1 to 4 years of the software development. About 90% of warnings remain in the program or are removed during non-fix changes -- likely false positive warnings. The tools' warning prioritization is little help in focusing on important warnings: the maximum possible precision by selecting high-priority warning instances is only 3%, 12%, and 8% respectively.   In this paper, we propose a history-based warning prioritization algorithm by mining warning fix experience that is recorded in the software change history. The underlying intuition is that if warnings from a category are eliminated by fix-changes, the warnings are important. Our prioritization algorithm improves warning precision to 17%, 25%, and 67% respectively.","n":0.075}}},{"i":1095,"$":{"0":{"v":"The Google FindBugs fixit","n":0.5},"1":{"v":"In May 2009, Google conducted a company wide FindBugs \"fixit\". Hundreds of engineers reviewed thousands of FindBugs warnings, and fixed or filed reports against many of them. In this paper, we discuss the lessons learned from this exercise, and analyze the resulting dataset, which contains data about how warnings in each bug pattern were classified. Significantly, we observed that even though most issues were flagged for fixing, few appeared to be causing any serious problems in production. This suggests that most interesting software quality problems were eventually found and fixed without FindBugs, but FindBugs could have found these problems early, when they are cheap to remediate. We compared this observation to bug trends observed in code snapshots from student projects.   The full dataset from the Google fixit, with confidential details encrypted, will be published along with this paper.","n":0.085}}},{"i":1096,"$":{"0":{"v":"Straightforward Statistics for the Behavioral Sciences","n":0.408},"1":{"v":"Getting started: why study satistics? basic concepts and ideas. Descriptive statistics: frequency distributions and graphs summary measures relative measures and the normal curve linear correlation linear regression. Concepts of inferential statistics: sampling distributions logic of hypothesis testing. Methods of inferential statistics: one-sample t statistic - when a t ratio is not practical two-sample t tests analysis of variance two-way analysis of variance repeated-measures analysis of variance nonparametric tests bringing it all together. Appendices: statistical tables answers to selected review questions computer applications of statistics.","n":0.109}}},{"i":1097,"$":{"0":{"v":"Comparing bug finding tools with reviews and tests","n":0.354},"1":{"v":"Bug finding tools can find defects in software source code using an automated static analysis. This automation may be able to reduce the time spent for other testing and review activities. For this we need to have a clear understanding of how the defects found by bug finding tools relate to the defects found by other techniques. This paper describes a case study using several projects mainly from an industrial environment that were used to analyse the interrelationships. The main finding is that the bug finding tools predominantly find different defects than testing but a subset of defects found by reviews. However, the types that can be detected are analysed more thoroughly. Therefore, a combination is most advisable if the high number of false positives of the tools can be tolerated.","n":0.087}}},{"i":1098,"$":{"0":{"v":"Prioritizing Warning Categories by Analyzing Software History","n":0.378},"1":{"v":"Automatic bug finding tools tend to have high false positive rates: most warnings do not indicate real bugs. Usually bug finding tools prioritize each warning category. For example, the priority of \"overflow \" is 1 and the priority of \"jumbled incremental\" is 3, but the tools 'prioritization is not very effective. In this paper, we prioritize warning categories by analyzing the software change history. The underlying intuition is that if warnings from a category are resolved quickly by developers, the warnings in the category are important. Experiments with three bug finding tools (FindBugs, JLint, and PMD) and two open source projects (Columba and jEdit) indicate that different warning categories have very different lifetimes. Based on that observation, we propose a preliminary algorithm for warning category prioritizing.","n":0.089}}},{"i":1099,"$":{"0":{"v":"On the Impact of Design Flaws on Software Defects","n":0.333},"1":{"v":"The presence of design flaws in a software system has a negative impact on the quality of the software, as they indicate violations of design practices and principles, which make a software system harder to understand, maintain, and evolve. Software defects are tangible effects of poor software quality. In this paper we study the relationship between software defects and a number of design flaws. We found that, while some design flaws are more frequent, none of them can be considered more harmful with respect to software defects. We also analyzed the correlation between the introduction of new flaws and the generation of defects.","n":0.099}}},{"i":1100,"$":{"0":{"v":"Design patterns and change proneness: an examination of five evolving systems","n":0.302},"1":{"v":"Design patterns are recognized, named solutions to common design problems. The use of the most commonly referenced design patterns should promote adaptable and reusable program code. When a system evolves, changes to code involving a design pattern should, in theory, consist of creating new concrete classes that are extensions or subclasses of previously existing classes. Changes should not, in theory, involve direct modifications to the classes in prior versions that play roles in a design patterns. We studied five systems, three proprietary systems and two open source systems, to identify the observable effects of the use of design patterns in early versions on changes that occur as the systems evolve. In four of the five systems, pattern classes are more rather than less change prone. Pattern classes in one of the systems were less change prone. These results held up after normalizing for the effect of class size - larger classes are more change prone in two of the five systems. These results provide insight into how design patterns are actually used, and should help us to learn to develop software designs that are more easily adapted.","n":0.073}}},{"i":1101,"$":{"0":{"v":"Evaluating the relation between coding standard violations and faultswithin and across software versions","n":0.277},"1":{"v":"In spite of the widespread use of coding standards and tools enforcing their rules, there is little empirical evidence supporting the intuition that they prevent the introduction of faults in software. In previous work, we performed a pilot study to assess the relation between rule violations and actual faults, using the MISRA C 2004 standard on an industrial case. In this paper, we investigate three different aspects of the relation between violations and faults on a larger case study, and compare the results across the two projects. We find that 10 rules in the standard are significant predictors of fault location.","n":0.1}}},{"i":1102,"$":{"0":{"v":"The repeatability of code defect classifications","n":0.408},"1":{"v":"Counts of defects found during the various defect defection activities in software projects and their classification provide a basis for product quality evaluation and process improvement. However, since defect classifications are subjective, it is necessary to ensure that they are repeatable (i.e., that the classification is not dependent on the individual). We evaluate a slight adaptation of a commonly used defect classification scheme that has been applied in IBM's Orthogonal Defect Classification work, and in the SEI's Personal Software Process. The evaluation utilizes the Kappa statistic. We use defect data from code inspections conducted during a development project. Our results indicate that the classification scheme is in general repeatable. We further evaluate classes of defects to find out if confusion between some categories is more common, and suggest a potential improvement to the scheme.","n":0.086}}},{"i":1103,"$":{"0":{"v":"A maintainability model for industrial software systems using design level metrics","n":0.302},"1":{"v":"Software maintenance is a time consuming and expensive phase of a software product's life-cycle. The paper investigates the use of software design metrics to statistically estimate the maintainability of large software systems, and to identify error prone modules. A methodology for assessing, evaluating and, selecting software metrics for predicting software maintainability is presented. In addition, a linear prediction model based on a minimal set of design level software metrics is proposed. The model is evaluated by applying it to industrial software systems.","n":0.11}}},{"i":1104,"$":{"0":{"v":"An empirical validation of FindBugs issues related to defects","n":0.333},"1":{"v":"Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with our previous experiment and then to related work: results are consistent with both of them. Conclusions: Since our and other empirical studies demonstrated that few issues are related to real defects with high precision, developers could enable only them (or prioritize), reducing the information overload of FindBugs and having the possibility to discover defects earlier. Furthermore, the technique presented in the paper can be adopted to other tools on a code base with tests to find issues with high precision that can be checked on code in production to find defects earlier.","n":0.065}}},{"i":1105,"$":{"0":{"v":"From assessment to reduction: how cutter consortium helps rein in millions of dollars in technical debt","n":0.25},"1":{"v":"Technical debt assessments often follow a similar pattern across engagements. In contrast, technical debt reduction projects can vary significantly from one company to another. In addition to exposing unexpected technical challenges, technical debt reduction projects bring business issues, organizational consideration, and methodical questions to the fore. This paper outlines how all these aspects are being dealt with in a Cutter Consortium engagement with a client that struggles to wrestle technical debt to the ground.","n":0.116}}},{"i":1106,"$":{"0":{"v":"Evaluating interrater agreement with intraclass correlation coefficient in SPICE-based software process assessment","n":0.289},"1":{"v":"As software process assessment (SPA) involves a subjective procedure, its reliability is an important issue. Two types of reliability have intensively been investigated in SPA: internal consistency (internal reliability) and interrater agreement (external reliability). This study investigates interrater agreement. Cohen's Kappa coefficient has been a popular measure for estimating interrater agreement. However, the application of Kappa coefficient in certain situations is incorrect due to the \"Kappa Paradoxes\". To cope with the insufficiency of Kappa coefficient, this study applied the intraclass correlation coefficient (ICC) to estimate interrater agreement. The ICC has not been employed in the SPA context. In addition, we examined the stability of the estimated ICC value by using a bootstrap resampling method. Results show that ICC could be applied where the Kappa coefficient could not be applied, but not all cases.","n":0.087}}},{"i":1107,"$":{"0":{"v":"The quamoco product quality modelling and assessment approach","n":0.354},"1":{"v":"Published software quality models either provide abstract quality attributes or concrete quality assessments. There are no models that seamlessly integrate both aspects. In the project Quamoco, we built a comprehensive approach with the aim to close this gap.     For this, we developed in several iterations a meta quality model specifying general concepts, a quality base model covering the most important quality factors and a quality assessment approach. The meta model introduces the new concept of a product factor, which bridges the gap between concrete measurements and abstract quality aspects. Product factors have measures and instruments to operationalise quality by measurements from manual inspection and tool analysis. The base model uses the ISO 25010 quality attributes, which we refine by 200 factors and 600 measures for Java and C# systems.     We found in several empirical validations that the assessment results fit to the expectations of experts for the corresponding systems. The empirical analyses also showed that several of the correlations are statistically significant and that the maintainability part of the base model has the highest correlation, which fits to the fact that this part is the most comprehensive. Although we still see room for extending and improving the base model, it shows a high correspondence with expert opinions and hence is able to form the basis for repeatable and understandable quality assessments in practice.","n":0.067}}},{"i":1108,"$":{"0":{"v":"Evaluating prediction systems in software project estimation","n":0.378},"1":{"v":"Context: Software engineering has a problem in that when we empirically evaluate competing prediction systems we obtain conflicting results. Objective: To reduce the inconsistency amongst validation study results and provide a more formal foundation to interpret results with a particular focus on continuous prediction systems. Method: A new framework is proposed for evaluating competing prediction systems based upon (1) an unbiased statistic, Standardised Accuracy, (2) testing the result likelihood relative to the baseline technique of random 'predictions', that is guessing, and (3) calculation of effect sizes. Results: Previously published empirical evaluations of prediction systems are re-examined and the original conclusions shown to be unsafe. Additionally, even the strongest results are shown to have no more than a medium effect size relative to random guessing. Conclusions: Biased accuracy statistics such as MMRE are deprecated. By contrast this new empirical validation framework leads to meaningful results. Such steps will assist in performing future meta-analyses and in providing more robust and usable recommendations to practitioners.","n":0.079}}},{"i":1109,"$":{"0":{"v":"A simulation study of the model evaluation criterion MMRE","n":0.333},"1":{"v":"The mean magnitude of relative error, MMRE, is probably the most widely used evaluation criterion for assessing the performance of competing software prediction models. One purpose of MMRE is to assist us to select the best model. In this paper, we have performed a simulation study demonstrating that MMRE does not always select the best model. Our findings cast some doubt on the conclusions of any study of competing software prediction models that use MMRE as a basis of model comparison. We therefore recommend not using MMRE to evaluate and compare prediction models. At present, we do not have any universal replacement for MMRE. Meanwhile, we therefore recommend using a combination of theoretical justification of the models that are proposed together with other metrics proposed in this paper.","n":0.088}}},{"i":1110,"$":{"0":{"v":"Reliability and validity in comparative studies of software prediction models","n":0.316},"1":{"v":"Empirical studies on software prediction models do not converge with respect to the question \"which prediction model is best?\" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models.","n":0.079}}},{"i":1111,"$":{"0":{"v":"Experience with the accuracy of software maintenance task effort prediction models","n":0.302},"1":{"v":"The paper reports experience from the development and use of eleven different software maintenance effort prediction models. The models were developed applying regression analysis, neural networks and pattern recognition and the prediction accuracy was measured and compared for each model type. The most accurate predictions were achieved applying models based on multiple regression and on pattern recognition. We suggest the use of prediction models as instruments to support the expert estimates and to analyse the impact of the maintenance variables on the maintenance process and product. We believe that the pattern recognition based models evaluated, i.e., the prediction models based on the Optimized Set Reduction method, show potential for such use. >","n":0.094}}},{"i":1112,"$":{"0":{"v":"Technical Debt Prioritization: State of the Art. A Systematic Literature Review","n":0.302},"1":{"v":"Background. Software companies need to manage and refactor Technical Debt issues. Therefore, it is necessary to understand if and when refactoring of Technical Debt should be prioritized with respect to developing features or fixing bugs.  Objective. The goal of this study is to investigate the existing body of knowledge in software engineering to understand what Technical Debt prioritization approaches have been proposed in research and industry. Method. We conducted a Systematic Literature Review of 557 unique papers published until 2019, following a consolidated methodology applied in software engineering. We included 44 primary studies.  Results. Different approaches have been proposed for Technical Debt prioritization, all having different goals and proposing optimization regarding different criteria. The proposed measures capture only a small part of the plethora of factors used to prioritize Technical Debt qualitatively in practice. We present an impact map of such factors. However, there is a lack of empirical and validated set of tools.  Conclusion. We observed that Technical Debt prioritization research is preliminary and there is no consensus on what the important factors are and how to measure them. Consequently, we cannot consider current research conclusive. In this paper, we therefore outline different directions for necessary future investigations.","n":0.071}}},{"i":1113,"$":{"0":{"v":"A Dynamical Quality Model to Continuously Monitor Software Maintenance","n":0.333}}},{"i":1114,"$":{"0":{"v":"A process for driving the harmonization of models","n":0.354},"1":{"v":"At present, there are several factors that may influence an organization in needing to work with more than one reference model. The following can be highlighted: (i) market niches with specific models, (ii) improvement of practices from legacy process models, (iii) business positioning, (iv) leveraged or merger corporate (v) systematic search of the capability of the processes, (vi) business growth, among others. Currently, however, there is no detailed strategy to address the harmonization of reference models. So, the aim of this paper is to present a process that defines the elements necessary to support the harmonization of multiple reference models. This process allows us to guide the implementation of a harmonization project systematically. It also describes our experience of the application of the proposed process in one organization. These results show that the process and the harmonization techniques used to support the objectives of harmonization of ISO 27001 and ISO 20000 of the company involved were suitable.","n":0.08}}},{"i":1115,"$":{"0":{"v":"Software renewal projects estimation using dynamic calibration","n":0.378},"1":{"v":"Effort estimation is a long faced problem, but, in spite of the amount of research spent in this field, it still remains an open issue in the software engineering community. This is true especially in the case of renewal of legacy systems, where the current and well established approaches also fail. This paper presents an application of the method named dynamic calibration for effort estimation of renewal projects together with its experimental validation. The approach satisfies all the requirements of the estimation models. The results obtained by applying dynamic calibration are compared with those obtained with a competitor method: estimation by analogy. The results are shown to be promising although further experimentation on field is needed.","n":0.093}}},{"i":1116,"$":{"0":{"v":"Comparing ISO/IEC 12207 and CMMI-DEV: Towards a mapping of ISO/IEC 15504-7","n":0.302},"1":{"v":"Software process improvement is a planned, managed and controlled effort which aims to enhance the capability of the software development processes of an organization. In particular, SPI often involves process reference models, process assessment methods and models that guide process improvement though specific standards such as the ISO and CMMI families. Recently, growing interest has been shown towards the need for harmonization of different improvement technologies with the aim of presenting an integrated vision about the standards. In this sense this paper presents a comparison between the process areas of CMMI-DEV and the processes described in the latest version of ISO/IEC 12207:2008. Based on these results we investigate the relationship between the CMMI-DEV and ISO/IEC 15504-7 models with the aim of identifying the degree of coverage of CMMI-DEV maturity levels in relation to ISO/IEC 15504-7.","n":0.086}}},{"i":1117,"$":{"0":{"v":"Empirical studies for innovation dissemination: ten years of experience","n":0.333},"1":{"v":"Context: technology transfer and innovation dissemination are key success factors for an enterprise. The shift to a new software technology determines inevitable changes to ingrained and familiar processes and at the same time leads to changes in practices, requires training and commitment on behalf of technical staff and management. As so, it cannot leave out neither organizational nor technical factors. Objective: our conjecture is that the process of innovation dissemination is facilitated if the new technology is supported by empirical evidence. In this sense, Empirical Software Engineering (ESE) serves as support for transferring an innovation, either it being a process or product, within production processes. Method: this paper investigates the relation between empirical studies and technical/organizational factors in order to identify the most suitable empirical study for disseminating an innovation in an enterprise. The analysis has been carried out with respect to empirical studies carried out during a ten year time span within the Software Engineering Research LABoratory (SERLAB), at the University of Bari. Results: the results point out that a critical factor in designing empirical studies is the quality model, i.e. measurement program defined by researchers during the study and used to collect relevant information on the effectiveness and efficacy of the innovation being transferred, in order to gain commitment on behalf of stakeholders who will finally adopt the technology. Conclusion: the study outcomes provide an empirically founded guideline that can be used when choosing the most appropriate approach for addressing an innovation.","n":0.064}}},{"i":1118,"$":{"0":{"v":"Empirical Investigation of the Efficacy and Efficiency of Tools for Transferring Software Engineering Knowledge","n":0.267},"1":{"v":"Continuous pressure on behalf of enterprises leads to a constant need for innovation. This involves exchanging results of knowledge and innovation among research groups and enterprises in accordance to the Open Innovation paradigm. The technologies that seem to be apparently attractive for exchanging knowledge are the Internet and its search engines. Literature provides many discordant opinions on their efficacy, and no empirical evidence on the topic. This work starts from the definition of a Knowledge Acquisition Process, and presents a rigorous empirical investigation that evaluates the efficacy of the previous technologies within the Exploratory Search of Knowledge and of Relevant Knowledge according to specific knowledge requirements. The investigation has pointed out that these technologies are not effective for Explorative Search. The paper concludes with a brief analysis of other technologies to develop and analyse in order to overcome the weaknesses that this investigation has pointed out within the Knowledge Acquisition Process.","n":0.081}}},{"i":1119,"$":{"0":{"v":"Software Effort Estimation and Productivity.","n":0.447},"1":{"v":"Publisher Summary   This chapter is concerned about effort and cost estimation models which are appropriate for software project development. Project development is meant to include life cycle phases from project design, through system integration, to testing and software delivery. It discusses and evaluates several models for software effort estimation and performance of these models is compared on sets of projects for which some information is available. The models are categorized into (1) historical-experiential models, (2) statistically-based models, (3) theoretically-based models, and (4) composite models. There is some hope that effort and cost models so restricted can be developed which are transportable from one organization to another. The chapter concludes that further experimentation, the gathering of more data, and the combining and enhancing of models will be necessary in order to allow computer scientists to explain and better control the software development process.","n":0.084}}},{"i":1120,"$":{"0":{"v":"Technology Acceptance Model 3 and a Research Agenda on Interventions","n":0.316},"1":{"v":"Prior research has provided valuable insights into how and why employees make a decision about the adoption and use of information technologies (ITs) in the workplace. From an organizational point of view, however, the more important issue is how managers make informed decisions about interventions that can lead to greater acceptance and effective utilization of IT. There is limited research in the IT implementation literature that deals with the role of interventions to aid such managerial decision making. Particularly, there is a need to understand how various interventions can influence the known determinants of IT adoption and use. To address this gap in the literature, we draw from the vast body of research on the technology acceptance model (TAM), particularly the work on the determinants of perceived usefulness and perceived ease of use, and: (i) develop a comprehensive nomological network (integrated model) of the determinants of individual level (IT) adoption and use; (ii) empirically test the proposed integrated model; and (iii) present a research agenda focused on potential pre- and postimplementation interventions that can enhance employees' adoption and use of IT. Our findings and research agenda have important implications for managerial decision making on IT implementation in organizations.","n":0.071}}},{"i":1121,"$":{"0":{"v":"Qualitative data analysis (2nd ed): Mathew B. Miles and A. Michael Huberman. Thousand Oaks, CA: Sage Publications, 1994. Price: $65.00 hardback, $32.00 paperback. 238 pp","n":0.2}}},{"i":1122,"$":{"0":{"v":"Task-technology fit and individual performance","n":0.447},"1":{"v":"A key concern in Information Systems (IS) research has been to better understand the linkage between information systems and individual performance. The research reported in this study has two primary objectives: (1) to propose a comprehensive theoretical model that incorporates valuable insights from two complementary streams of research, and (2) to empirically test the core of the model. At the heart of the new model is the assertion that for an information technology to have a positive impact on individual performance, the technology: (1) must be utilized and (2) must be a good fit with the tasks it supports. This new model is moderately supported by an analysis of data from over 600 individuals in two companies. This research highlights the importance of the fit between technologies and users' tasks in achieving individual performance impacts from information technology. It also suggests that task-technology fit when decomposed into its more detailed components, could be the basis for a strong diagnostic tool to evaluate whether information systems and services in a given organization are meeting user needs.","n":0.076}}},{"i":1123,"$":{"0":{"v":"The Measurement of Web-Customer Satisfaction: An Expectation and Disconfirmation Approach","n":0.316},"1":{"v":"Online shopping provides convenience to Web shoppers, yet its electronic format changes information-gathering methods traditionally used by customers. This change raises questions concerning customer satisfaction with the online purchasing process. Web shopping involves a number of phases, including the information phase, in which customers search for information regarding their intended purchases. The purpose of this paper is to develop theoretically justifiable constructs for measuring Web-customer satisfaction during the information phase.By synthesizing the expectation-disconfirmation paradigm with empirical theories in user satisfaction, we separate Web site quality into information quality (IQ) and system quality (SQ), and propose nine key constructs for Web-customer satisfaction. The measurements for these constructs are developed and tested in a two-phase study. In the first phase, the IQ and SQ dimensions are identified, and instruments for measuring them are developed and tested. In the second phase, using the salient dimensions of Web-IQ and Web-SQ as the basis for formulating first-order factors, we develop and empirically test instruments for measuring IQ and SQ-satisfaction. Moreover, this phase involves the design and test of second-order factors for measuring Web-customer expectations, disconfirmation, and perceived performance regarding IQ and SQ. The analysis of the measurement model indicates that the proposed metrics have a relatively high degree of validity and reliability. The results of the study provide reliable instruments for operationalizing the key constructs in the analysis of Web-customer satisfaction within the expectation-disconfirmation paradigm.","n":0.066}}},{"i":1124,"$":{"0":{"v":"Antecedents of Information and System Quality: An Empirical Examination Within the Context of Data Warehousing","n":0.258},"1":{"v":"Understanding the successful adoption of information technology is largely based upon understanding the linkages among quality, satisfaction, and usage. Although the satisfaction and usage constructs have been well studied in the information systems literature, there has been only limited attention to information and system quality over the past decade. To address this shortcoming, we developed a model consisting of nine fundamental determinants of quality in an information technology context, four under the rubric of information quality (the output of an information system) and five that describe system quality (the information processing system required to produce the output). We then empirically examined the aptness of our model using a sample of 465 data warehouse users from seven different organizations that employed report-based, query-based, and analytical business intelligence tools. The results suggest that our determinants are indeed predictive of overall information and system quality in data warehouse environments, and that our model strikes a balance between comprehensiveness and parsimony. We conclude with a discussion of the implications for both theory and the development and implementation of information technology applications in practice.","n":0.075}}},{"i":1125,"$":{"0":{"v":"Trust in a specific technology: An investigation of its components and measures","n":0.289},"1":{"v":"Trust plays an important role in many Information Systems (IS)-enabled situations. Most IS research employs trust as a measure of interpersonal or person-to-firm relations, such as trust in a Web vendor or a virtual team member. Although trust in other people is important, this article suggests that trust in the Information Technology (IT) itself also plays a role in shaping IT-related beliefs and behavior. To advance trust and technology research, this article presents a set of trust in technology construct definitions and measures. We also empirically examine these construct measures using tests of convergent, discriminant, and nomological validity. This study contributes to the literature by providing: (a) a framework that differentiates trust in technology from trust in people, (b) a theory-based set of definitions necessary for investigating different kinds of trust in technology, and (c) validated trust in technology measures useful to research and practice.","n":0.083}}},{"i":1126,"$":{"0":{"v":"DESIGNING AND CONDUCTING MIXED METHODS RESEARCH (2nd Edition)","n":0.354}}},{"i":1127,"$":{"0":{"v":"Knowing-Why About Data Processes and Data Quality","n":0.378},"1":{"v":"Knowledge about work processes is a prerequisite for performing work. We investigate whether a certain mode of knowledge, knowing-why, affects work performance and whether the knowledge held by different work roles matters for work performance. We operationalize these questions in the specific domain of data production processes and data quality. We analyze responses from three roles within data production processes, data collectors, data custodians, and data consumers, to investigate the effects of different knowledge modes held by different work roles on data quality. We find that work roles and the mode of knowledge do matter. Specifically, data collectors with why-knowledge about the data production process contribute to producing better quality data. Overall, knowledge of data collectors is more critical than that of data custodians.","n":0.09}}},{"i":1128,"$":{"0":{"v":"Operationalised product quality models and assessment","n":0.408},"1":{"v":"ContextSoftware quality models provide either abstract quality characteristics or concrete quality measurements; there is no seamless integration of these two aspects. Quality assessment approaches are, hence, also very specific or remain abstract. Reasons for this include the complexity of quality and the various quality profiles in different domains which make it difficult to build operationalised quality models. ObjectiveIn the project Quamoco, we developed a comprehensive approach aimed at closing this gap. MethodThe project combined constructive research, which involved a broad range of quality experts from academia and industry in workshops, sprint work and reviews, with empirical studies. All deliverables within the project were peer-reviewed by two project members from a different area. Most deliverables were developed in two or three iterations and underwent an evaluation. ResultsWe contribute a comprehensive quality modelling and assessment approach: (1) A meta quality model defines the structure of operationalised quality models. It includes the concept of a product factor, which bridges the gap between concrete measurements and abstract quality aspects, and allows modularisation to create modules for specific domains. (2) A largely technology-independent base quality model reduces the effort and complexity of building quality models for specific domains. For Java and C# systems, we refined it with about 300 concrete product factors and 500 measures. (3) A concrete and comprehensive quality assessment approach makes use of the concepts in the meta-model. (4) An empirical evaluation of the above results using real-world software systems showed: (a) The assessment results using the base model largely match the expectations of experts for the corresponding systems. (b) The approach and models are well understood by practitioners and considered to be both consistent and well suited for getting an overall view on the quality of a software product. The validity of the base quality model could not be shown conclusively, however. (5) The extensive, open-source tool support is in a mature state. (6) The model for embedded software systems is a proof-of-concept for domain-specific quality models. ConclusionWe provide a broad basis for the development and application of quality models in industrial practice as well as a basis for further extension, validation and comparison with other approaches in research.","n":0.053}}},{"i":1129,"$":{"0":{"v":"Wilcoxon Signed-Rank Test","n":0.577},"1":{"v":"This is a nonparametric test procedure for the analysis of matched-pair data, based on differences, or for a single sample. The null hypothesis is that the differences, or individual observations in the single-sample case, have a distribution centered about zero. The absolute values are ranked. The test statistic is the sum of the ranks for either the positive or the negative values. Examples and details, including large-sample properties, are given.\r\n\r\n\r\nKeywords:\r\n\r\nnonparametric;\r\ndistribution-free;\r\nranks;\r\nabsolute differences;\r\nlarge samples;\r\nARE;\r\ncensored","n":0.118}}},{"i":1130,"$":{"0":{"v":"Lotus notes ® and collaboration: plus ça change...","n":0.354},"1":{"v":"Work in organizations is becoming increasingly focused on collaborative work in groups. Groupware is widely touted as the information technology that can support this new mode of work by fostering collaboration. In a study of Lotus Notes®, a popular groupware product, implemented throughout the professional staff of a large American insurance company, we found the impact of groupware to be somewhat different from certain common expectations. While almost everyone was quite pleased with the Notes® implementation and its perceived impact, there was no evidence of a change in the degree of collaboration among organization members. Two key themes are explored as possible explanations for this result: fit of the technology to the organization, and limited training in how best to use this new technology.","n":0.09}}},{"i":1131,"$":{"0":{"v":"From space to place: predicting users' intentions to return to virtual worlds","n":0.289},"1":{"v":"Virtual worlds have received considerable attention as platforms for entertainment, education, and commerce. But organizations are experiencing failures in their early attempts to lure customers, employees, or partners into these worlds. Among the more grievous problems is the inability to attract users back into a virtual environment. In this study, we propose and test a model to predict users' intentions to return to a virtual world. Our model is based on the idea that users intend to return to a virtual world having conceived of it as a \"place\" in which they have had meaningful experiences. We rely on the interactionist theory of place attachment to explain the links among the constructs of our model. Our model is tested via a lab experiment. We find that users' intentions to return to a virtual world is determined by a state of deep involvement (termed cognitive absorption) that users experience as they perform an activity and tend to lose track of time. In turn, cognitive absorption is determined by users' awareness of whom they interact with and how they interact within a virtual world, what they interact about, and where, in a virtual sense, such interaction occurs. Our work contributes to theory in the following ways: it identifies state predictors of cognitive absorption, it conceives of virtual worlds in such a way as to account for users' experiences through the notion of place, and it explains how the properties of a virtual world contribute to users' awareness.","n":0.064}}},{"i":1132,"$":{"0":{"v":"Impact of Knowledge Support on the Performance of Software Process Tailoring","n":0.302},"1":{"v":"The use of a well-defined process is a widely recognized approach to increasing quality and productivity in software development. Building software processes from scratch each time is expensive and risky. Therefore, they are often created by tailoring existing processes and standards. Process tailoring is a knowledge-intensive activity. This research explores the link between knowledge support and software process tailoring performance under different levels of tailoring task complexity. It theoretically develops and tests how the fit between knowledge (generalized and contextualized) and software tailoring task complexity influences process tailoring performance. Process tailoring performance is conceptualized in terms of effectiveness and efficiency. The results from an experiment and a protocol analysis show that contextualized knowledge outperforms generalized knowledge in improving tailoring performance, and that such improvement in performance is greater in complex process tailoring tasks when compared to simple tasks.","n":0.085}}},{"i":1133,"$":{"0":{"v":"Zero-tolerance construction [software development]","n":0.5},"1":{"v":"Software construction is the ultimate embodiment of the software development process. The art of programming lies in that nether region between the hopeful wishes of an elegant architecture and the hard grindstone of technical details. To prevent major catastrophic loss, we must focus on preventing the triggering mechanism from occurring. If we can fix the little problems as they occur, then we shall have fewer large problems with which to contend.","n":0.119}}},{"i":1134,"$":{"0":{"v":"Software Engineering Metrics: What Do They Measure and How Do We Know?","n":0.289},"1":{"v":"Construct validity is about the question, how we know that we're measuring the attribute that we think we're measuring? This is discussed in formal, theoretical ways in the computing literature (in terms of the representational theory of measurement) but rarely in simpler ways that foster application by practitioners. Construct validity starts with a thorough analysis of the construct, the attribute we are attempting to measure. In the IEEE Standard 1061, direct measures need not be validated. \"Direct\" measurement of an attribute involves a metric that depends only on the value of the attribute, but few or no software engineering attributes or tasks are so simple that measures of them can be direct. Thus, all metrics should be validated. The paper continues with a framework for evaluating proposed metrics, and applies it to two uses of bug counts. Bug counts capture only a small part of the meaning of the attributes they are being used to measure. Multidimensional analyses of attributes appear promising as a means of capturing the quality of the attribute in question. Analysis fragments run throughout the paper, illustrating the breakdown of an attribute or task of interest into sub-attributes for grouped study.","n":0.072}}},{"i":1135,"$":{"0":{"v":"A Quantitative Approach to Software Maintainability Prediction","n":0.378},"1":{"v":"Software maintainability is one important aspect in the evaluation of software evolution of a software product. Due to the complexity of tracking maintenance behaviors, it is difficult to accurately predict the cost and risk of maintenance after delivery of software products. In an attempt to address this issue quantitatively, software maintainability is viewed as an inevitable evolution process driven by maintenance behaviors, given a health index at the time when a software product are delivered. A Hidden Markov Model (HMM) is used to simulate the maintenance behaviors shown as their possible occurrence probabilities. And software metrics is the measurement of the quality of a software product and its measurement results of a product being delivered are combined to form the health index of the product. The health index works as a weight on the process of maintenance behavior over time. When the occurrence probabilities of maintenance behaviors reach certain number which is reckoned as the indication of the deterioration status of a software product, the product can be regarded as being obsolete. Longer the time, better the maintainability would be.","n":0.075}}},{"i":1136,"$":{"0":{"v":"Quasi-experimentation: Design & analysis issues for field settings","n":0.354}}},{"i":1137,"$":{"0":{"v":"A probabilistic software quality model","n":0.447},"1":{"v":"In order to take the right decisions in estimating the costs and risks of a software change, it is crucial for the developers and managers to be aware of the quality attributes of their software. Maintainability is an important characteristic defined in the ISO/IEC 9126 standard, owing to its direct impact on development costs. Although the standard provides definitions for the quality characteristics, it does not define how they should be computed. Not being tangible notions, these characteristics are hardly expected to be representable by a single number. Existing quality models do not deal with ambiguity coming from subjective interpretations of characteristics, which depend on experience, knowledge, and even intuition of experts. This research aims at providing a probabilistic approach for computing high-level quality characteristics, which integrate expert knowledge, and deal with ambiguity at the same time. The presented method copes with “goodness” functions, which are continuous generalizations of threshold based approaches, i.e. instead of giving a number for the measure of goodness, it provides a continuous function. Two different systems were evaluated using this approach, and the results were compared to the opinions of experts involved in the development. The results show that the quality model values change in accordance with the maintenance activities, and they are in a good correlation with the experts' expectations.","n":0.068}}},{"i":1138,"$":{"0":{"v":"The squale model — A practice-based industrial quality model","n":0.333},"1":{"v":"ISO 9126 promotes a three-level model of quality (factors, criteria, and metrics) which allows one to assess quality at the top level of factors and criteria. However, it is difficult to use this model as a tool to increase software quality. In the Squale model, we add practices as an intermediate level between metrics and criteria. Practices abstract away from raw information (metrics, tool reports, audits) and provide technical guidelines to be respected. Moreover, practice marks are adjusted using formulae to suit company development habits or exigences: for example bad marks are stressed to point to places which need more attention. The Squale model has been developed and validated over the last couple of years in an industrial setting with Air France-KLM and PSA Peugeot-Citroen.","n":0.089}}},{"i":1139,"$":{"0":{"v":"A NEW MEASURE OF RANK CORRELATION","n":0.408},"1":{"v":"1. In psychological work the problem of comparing two different rankings of the same set of individuals may be divided into two types. In the first type the individuals have a given order A which is objectively defined with reference to some quality, and a characteristic question is: if an observer ranks the individuals in an order B, does a comparison of B with A suggest that he possesses a reliable judgment of the quality, or, alternatively, is it probable that B could have arisen by chance? In the second type no objective order is given. Two observers consider the individuals and rank them in orders A and B. The question now is, are these orders sufficiently alike to indicate similarity of taste in the observers, or, on the other hand, are A and B incompatible within assigned limits of probability? An example of the first type occurs in the familiar experiments wherein an observer has to arrange a known set of weights in ascending order of weight; the second type would arise if two observers had to rank a set of musical compositions in order of preference. The measure of rank correlation proposed in this paper is capable of being applied to both problems, which are, in fact, formally very much the same. For purposes of simplicity in the exposition it has, however, been thought convenient to preserve a distinction between theni.","n":0.066}}},{"i":1140,"$":{"0":{"v":"Toward Data-Driven Requirements Engineering","n":0.5},"1":{"v":"Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. The goal is data-driven requirements engineering by the masses and for the masses.","n":0.098}}},{"i":1141,"$":{"0":{"v":"Naming the pain in requirements engineering","n":0.408},"1":{"v":"Requirements Engineering (RE) has received much attention in research and practice due to its importance to software project success. Its interdisciplinary nature, the dependency to the customer, and its inherent uncertainty still render the discipline difficult to investigate. This results in a lack of empirical data. These are necessary, however, to demonstrate which practically relevant RE problems exist and to what extent they matter. Motivated by this situation, we initiated the Naming the Pain in Requirements Engineering (NaPiRE) initiative which constitutes a globally distributed, bi-yearly replicated family of surveys on the status quo and problems in practical RE. In this article, we report on the qualitative analysis of data obtained from 228 companies working in 10 countries in various domains and we reveal which contemporary problems practitioners encounter. To this end, we analyse 21 problems derived from the literature with respect to their relevance and criticality in dependency to their context, and we complement this picture with a cause-effect analysis showing the causes and effects surrounding the most critical problems. Our results give us a better understanding of which problems exist and how they manifest themselves in practical environments. Thus, we provide a first step to ground contributions to RE on empirical observations which, until now, were dominated by conventional wisdom only.","n":0.069}}},{"i":1142,"$":{"0":{"v":"Rapid quality assurance with Requirements Smells","n":0.408},"1":{"v":"Abstract   Bad requirements quality can cause expensive consequences during the software development lifecycle, especially if iterations are long and feedback comes late. We aim at a light-weight static requirements analysis approach that allows for rapid checks immediately when requirements are written down. We transfer the concept of  code smells  to requirements engineering as  Requirements Smells . To evaluate the benefits and limitations, we define Requirements Smells, realize our concepts for a smell detection in a prototype called  Smella  and apply Smella in a series of cases provided by three industrial and a university context. The automatic detection yields an average precision of 59% at an average recall of 82% with high variation. The evaluation in practical environments indicates benefits such as an increase of the awareness of quality defects. Yet, some smells were not clearly distinguishable. Lightweight smell detection can uncover many practically relevant requirements defects in a reasonably precise way. Although some smells need to be defined more clearly, smell detection provides a helpful means to support quality assurance in requirements engineering, for instance, as a supplement to reviews.","n":0.075}}},{"i":1143,"$":{"0":{"v":"On the diffuseness and the impact on maintainability of code smells: a large scale empirical investigation","n":0.25},"1":{"v":"Code smells are symptoms of poor design and implementation choices that may hinder code comprehensibility and maintainability. Despite the effort devoted by the research community in studying code smells, the extent to which code smells in software systems affect software maintainability remains still unclear. In this paper we present a large scale empirical investigation on the diffuseness of code smells and their impact on code change- and fault-proneness. The study was conducted across a total of 395 releases of 30 open source projects and considering 17,350 manually validated instances of 13 different code smell kinds. The results show that smells characterized by long and/or complex code (e.g., Complex Class) are highly diffused, and that smelly classes have a higher change- and fault-proneness than smell-free classes.","n":0.089}}},{"i":1144,"$":{"0":{"v":"Some Code Smells Have a Significant but Small Effect on Faults","n":0.302},"1":{"v":"We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness.","n":0.066}}},{"i":1145,"$":{"0":{"v":"MVP Explained: A Systematic Mapping Study on the Definitions of Minimal Viable Product","n":0.277},"1":{"v":"Context: One of the most important steps of the Lean Startupmethodology is the definition of Minimum Viable Product (MVP), needed to start the learning process by integrating the early adopters' feedbacks as soon as possible. Objective: This study aims at identifying the common definitions of MVP proposed and the key factors identified to help entrepreneurs efficiently define their MVP, reducing errors due to unconsidered unknown factors. Method: We identified the MVP definitions and key factors by means of a systematic mapping study, defining the research questions and the protocol to be used. We selected the bibliographic sources, the keywords, and the selection criteria for searching the relevant papers. Results: We found 97 articles and, through inclusion and exclusion criteria, removed 75 articles, which reduced the total to 22 at the end of the process. The results are a classification schema for characterizing the definition of Minimum Viable Product in Lean Startups and a set of common key factors identified in the MVP definitions. Conclusion: The identified key factors are related to technical characteristics of the product as well as market and customer aspects. We found a positive improvement of the state of the art of MVP and the definition of Minimum.","n":0.071}}},{"i":1146,"$":{"0":{"v":"App store mining is not enough for app improvement","n":0.333},"1":{"v":"The rise in popularity of mobile devices has led to a parallel growth in the size of the app store market, intriguing several research studies and commercial platforms on mining app stores. App store reviews are used to analyze different aspects of app development and evolution. However, app users’ feedback does not only exist on the app store. In fact, despite the large quantity of posts that are made daily on social media, the importance and value that these discussions provide remain mostly unused in the context of mobile app development. In this paper, we study how Twitter can provide complementary information to support mobile app development. By analyzing a total of 30,793 apps over a period of six weeks, we found strong correlations between the number of reviews and tweets for most apps. Moreover, through applying machine learning classifiers, topic modeling and subsequent crowd-sourcing, we successfully mined 22.4% additional feature requests and 12.89% additional bug reports from Twitter. We also found that 52.1% of all feature requests and bug reports were discussed on both tweets and reviews. In addition to finding common and unique information from Twitter and the app store, sentiment and content analysis were also performed for 70 randomly selected apps. From this, we found that tweets provided more critical and objective views on apps than reviews from the app store. These results show that app store review mining is indeed not enough; other information sources ultimately provide added value and information for app developers.","n":0.064}}},{"i":1147,"$":{"0":{"v":"Comparing requirements decomposition within the Scrum, Scrum with Kanban, XP, and banana development processes","n":0.267},"1":{"v":"Context: Eliciting requirements from customers is a complex task. In Agile processes, the customer talks directly with the development team and often reports requirements in an unstructured way. The requirements elicitation process is up to the developers, who split it into user stories by means of different techniques. Objective: We aim to compare the requirements decomposition process of an unstructured process and three Agile processes, namely XP, Scrum, and Scrum with Kanban. Method: We conducted a multiple case study with a replication design, based on the project idea of an entrepreneur, a designer with no experience in software development. Four teams developed the project independently, using four different development processes. The requirements were elicited by the teams from the entrepreneur, who acted as product owner and was available to talk with the four groups during the project. Results: The teams decomposed the requirements using different techniques, based on the selected development process. Conclusion: Scrum with Kanban and XP resulted in the most effective processes from different points of view. Unexpectedly, decomposition techniques commonly adopted in traditional processes are still used in Agile processes, which may reduce project agility and performance. Therefore, we believe that decomposition techniques need to be addressed to a greater extent, both from the practitioners’ and the research points of view.","n":0.068}}},{"i":1148,"$":{"0":{"v":"Identifying Design and Requirement Self-Admitted Technical Debt Using N-gram IDF","n":0.316},"1":{"v":"In software projects, technical debt takes place when a developer adopting a trivial solution containing quick and easy shortcuts to implement over a suitable solution that can take a longer time to solve a problem. This can cause major additional costs leading to negative impacts for software maintenance since those shortcuts might need to be reworked in the future. Detecting technical debt early can help a team cope with those risks. In this paper, we focus on Self-Admitted Technical Debt (SATD) that is a debt intentionally produced by developers. We propose an automated model to identify two most common types of self-admitted technical debt, requirement and design debt, from source code comments. We combine N-gram IDF and auto-sklearn machine learning to build the model. With the empirical evaluation on ten projects, our approach outperform the baseline method by improving the performance over 20% when identifying requirement self-admitted technical debt and achieving an average F1-score of 64% when identifying design self-admitted technical debt.","n":0.079}}},{"i":1149,"$":{"0":{"v":"UML Model Smells and Model Refactorings in Early Software Development Phases","n":0.302},"1":{"v":"This document describes smells and refactorings found in literature used for quality assurance of UML models. We focus on model smells and model refactorings applicable in early stage of model-based software development, i.e. during requirements analysis and early design phases. Therefore, the presented catalog should be seen as representative. With another focus, the selection of model smells and refactorings would be different. Furthermore, all model smells and refactorings described are related to at least one model quality aspect each affected by this technique. Please note that these assignements are under discussion. Model quality aspects are discussed based on two articles presented by Mohagheghi et al. [15] and Fieber et al. [7]. The main results of these articles are presented in the first chapter whereas Chapters 2 and 3 present 17 UML model smells and 27 UML model refactorings altogether in a structured way.","n":0.084}}},{"i":1150,"$":{"0":{"v":"A survey on UML model smells detection techniques for software refactoring","n":0.302}}}]}